2000-01-01 - AzusA 16-way Itanium server	AzusA is a prototype 16-way, optimized, Itanium-processor-based server suited for enterprise level-computing. The server's architecture and features are discussed to analyzed performance scalability for diverse workloads. The server uses a chip set which supports up to 16 Itanium processors for optimum 16-way performance. In combination with IA-64 Itanium processors, AsuzA provides a platform solution for the backbone of the Internet and enterprise computing. . Keywords: Computer architecture; Computer systems programming; Data reduction; Data structures; Electronic commerce; Microprocessor chips; Response time (computer systems); Enterprise-level computing; Client server computer systems
2000-01-01 - Devices & circuits	In the high-profile electronics industry, a new microprocessor architecture is enough of a rarity to excite great anticipation. This year, industry watchers will have plenty to look forward to, as the first batch of high-end computers built with Intel Corp.'s new microprocessor, Itanium, becomes available. This year could also mark the beginning of the end for microprocessors with clock rates below gigahertz, with IBM's Power4 processor chip and Compaq's Alpha 21364, both slated to run at 1 GHz and higher, at the forefront. . Keywords: Bandwidth; Electronics industry; Parallel processing systems; Personal computers; Micromirrors; Microprocessor chips
2000-01-01 - Formal verification of floating point trigonometric functions	We have formal verified a number of algorithms for evaluating transcendental functions in double-extended precision floating point arithmetic in the Intel® IA-64 architecture. These algorithms are used in the Itanium™ processor to provide compatibility with IA-32 (x86) hard-ware transcendentals, and similar ones are used in mathematical software libraries. In this paper we describe in some depth the formal verification of the sin and cos functions, including the initial range reduction step. This illustrates the different facets of verification in this field, covering both pure mathematics and the detailed analysis of floating point rounding. © Springer-Verlag Berlin Heidelberg 2000. . Keywords: Algorithms; Computer aided design; Digital arithmetic; Formal methods; Floating points; Itanium; Mathematical software; Precision floating point; Range reduction; Transcendental functions; Trigonometric functions; Formal verification
2000-01-01 - Formal verification of VLIW microprocessors with speculative execution	This is a study of the formal verification of a VLIW microprocessor that imitates the Intel Itanium [9][12][17] in features such as predicated execution, register remapping, advanced and speculative loads, and branch prediction. The formal verification is done with the Burch and Dill flushing technique [5] by exploiting the properties of Positive Equality [3][4]. The contributions include an extensive use of conservative approximations in abstracting portions of the processor and a framework for decomposition of the Boolean evaluation of the correctness formula. The conservative approximations are applied automatically when abstracting a memory whose forwarding logic is not affected by stalling conditions that preserve the correctness of the memory semantics for the same memory. These techniques allow a reduction of more than a factor of 4 in the CPU time for the formal verification of the most complex processor model examined relative to the monolithic evaluation of the correctness formula for a version of the same processor where conservative approximations are not applied. © Springer-Verlag Berlin Heidelberg 2000. . Keywords: Abstracting; Computer aided analysis; Semantics; Branch prediction; CPU time; Positive equality; Predicated execution; Processor modeling; Remapping; Speculative execution; Speculative loads; Formal verification
2000-01-01 - High availability and reliability in the Itanium processor	The Itanium processor offers reliability and availability for mission-critical applications. The major features of the processor are presented. The motivation and design decisions for these features are discussed. . Keywords: Buffer storage; Client server computer systems; Codes (symbols); Computer architecture; Computer systems programming; Data reduction; Data structures; Error correction; Error detection; Response time (computer systems); Error correcting codes (ECC); Machine check abort (MCA) architecture; Program processors
2000-01-01 - HP caliper: An architecture for performance analysis tools	HP Caliper is an architecture for software developer tools that deal with executable (binary) programs. It provides a common framework that allows building of a wide variety of tools for doing performance analysis, profiling, coverage analysis, correctness checking, and testing. HP Caliper uses a technology known as dynamic instrumentation, which allows program instructions to be changed on-the-fly with instrumentation probes. Dynamic instrumentation makes HP Caliper easy to use: It requires no special preparation of an application, supports shared libraries, collects data for multiple threads, and has low intrusion and overhead. This paper describes HP Caliper for HP-UX, running on the IA-64 (Itanium) processor. It describes Caliper's architecture, dynamic instrumentation algorithm, and the experiences gathered during its implementation. © Proceedings of the 1st Conference on Industrial Experiences with Systems Software, WIESS 2000. All rights reserved. . Keywords: Computer software; Coverage analysis; Dynamic instrumentation; Multiple threads; On the flies; Performance analysis; Program instructions; Shared libraries; Software developer; Memory architecture
2000-01-01 - Introducing the Itanium Processors	[No abstract available] . Keywords:
2000-01-01 - Itanium processor microarchitecture	The Itanium processor employs the explicitly parallel instruction computing (EPIC) design to exploit instruction level parallelism. Its hardware and software work aims to deliver higher performance through a simple, more efficient design. This processor is the first implementation of the IA-64 instruction set architecture. . Keywords: Bandwidth; Computer systems programming; Data communication systems; Data storage equipment; Digital arithmetic; Flip flop circuits; Microprocessor chips; Parallel processing systems; Pipeline processing systems; Program compilers; Response time (computer systems); Explicitly parallel instruction computing (EPIC); Instruction level parallelism (ILP); Instruction set architecture (ISA); Reduced instruction set computing
2000-01-01 - ItaniumTM processor clock design	The Itanium processor is Intel's first 64-bit microprocessor and features a highly parallel architecture fabricated using the 0.18 um process. This higher integration of features requires a significant silicon real estate and high clock loading. These factors, coupled with more prominent on-die variations because of reduced device geometries, call for special techniques to manage the clock design. The Itanium processor employs very well balanced clock routing along with distributed deskew buffers (DSK) to achieve low skew. The ItaniumTM processor also includes additional features to aid performance tuning and timing debug. This paper highlights the salient features of the Itanium processor clock design and presents clock characterization data from initial silicon. . Keywords: Electric network synthesis; Electric network topology; Microprocessor chips; Parallel processing systems; Semiconducting silicon; Timing circuits; Clock routing; Integrated circuit layout
2000-01-01 - Itanium™ processor system bus design	This paper presents the design of the ItaniumTMProcessor's system bus interface achieving a peak data bandwidth of 2.1GB/s in a glue-less 4-way multiprocessing system. A source-synchronous data bus with differential strobes enables this high bandwidth. Topics covered in this paper include optimisation technique for the system topology, CPU package, signalling protocol and I/O circuits. Highly accurate modelling and validation methodologies enable a good correlation of experimental results with simulation data. © 2000 Non IEEE. . Keywords: Bandwidth; Good correlations; Modelling and validation; Optimisation techniques; Processor systems; Signalling protocols; Simulation data; Source-synchronous; System topology; System buses
2000-01-01 - The 82460GX server/workstation chip set	This article provides an introduction to the memory, I/O, and graphics subsystems of Intel's Itanium processor chip set and discusses several aspects of the processor bus. . Keywords: Buffer storage; Client server computer systems; Computer graphics; Computer workstations; Dynamic random access storage; Input output programs; Integrated circuit layout; Interfaces (computer); Network protocols; Accelerated graphics port; Memory interface unit; Private data bus; System address chip; System data chip; Microprocessor chips
2000-01-01 - The linux/ia64 project: Kernel design and status update	The IA-64 architecture co-developed by HP and Intel is going to reach market in the second half of 2000 with Itanium as its first implementation. All the major industry players have endorsed this architecture and nowadays most of the specifications are public. To provide for early availability of Linux on this platform, the port started over two years ago at HP Labs and grew to become an industry wide effort. A major milestone was reached earlier this year, when the entire source code produced to support this new platform was released to the Open Source community. In this paper, we describe some of the key system architecture features of IA-64 and the major machine dependent kernel subsystems. We also give a brief update on the application level developments including the software development kit for Linux/ia32 recently released. . Keywords: Computer operating systems; Computer simulation; Computer software; Marketing; Multimedia systems; Optimization; Software prototyping; Vectors; HP (CO); IA64; Intel (CO); Itanium; Kernel; Linux; Computer architecture
2000-01-01 - Wavefront scheduling: Path based data representation and scheduling of subgraphs	The IA-64 architecture is rich with features that enable aggressive exploitation of instruction-level parallelism. Features such as speculation, predication, multiway branches and others provide compilers with new opportunities for the extraction of parallelism in programs. Code scheduling is a central component in any compiler for the IA-64 architecture. This paper describes the implementation of the global code scheduler (GCS) in Intel's compiler for the IA-64 architecture. GCS schedules code over acyclic regions of control flow. There is a tight coupling between the formation and scheduling of regions. GCS employs a new path based data dependence representation that combines control now and data dependence information to make data analysis easy and accurate. This paper provides details of this representation. The scheduler uses a novel instruction scheduling technique called Wavefront scheduling. The concepts of wavefront scheduling and deferred compensation are explained to demonstrate the efficient generation of compensation code while scheduling. This paper also presents P-ready code motion, an opportunistic instruction level tail duplication which aims to strike a balance between code expansion and performance potential. Performance results show greater than 30% improvement in speedup for wavefront scheduling over basic block scheduling on the Itanium microarchitecture. . Keywords: Code converters; Computer hardware; Graph theory; Optical phase conjugation; Program processors; Storage allocation (computer); Global code scheduler (GCS); Instruction-level parallelism (ILP); Path based data representation; Wavefront scheduling; Data acquisition
2001-01-01 - A specification methodology by a collection of compact properties as applied to the Intel® Itanium™ processor bus protocol	In practice, formal specifications are often considered too costly for the benefits they promise. Specifically, interface specifications such as standard bus protocol descriptions are still documented informally, and although many admit formal versions would be useful, they are dissuaded by the time and effort needed for development. We champion a formal specification methodology that attacks this costvalue problem from two angles. First, the framework allows formal specifications to be feasible for signal-level bus protocols with minimal effort, lowering costs. And second, a specification written in this style has many different uses, other than as a precise specification document, resulting in increased value over cost. This methodology allows the specification to be easily transformed into an executable checker or an simulation environment, for example. In an earlier paper, we demonstrated the methodology on a widelyused bus protocol. Now, we show that the generalized methodology can be applied to more advanced bus protocols, in particular, the Intel® ItaniumTM Processor bus protocol. In addition, the paper outlines how writing and checking such a specification revealed interesting issues, such as deadlock and missed data phases, during the development of the protocol. © Springer-Verlag Berlin Heidelberg 2001. . Keywords: Formal logic; System buses; Bus protocol; Data phasis; Interface specification; Processor bus; Signal level; Simulation environment; Specification methodologies; Standard bus; Formal specification
2001-01-01 - Calculation of Load Invalidation Rates for Data Speculation	For wide and in-order-issue processors like the Intel Itanium Processor Family (IFF), a memory load operation often appears on a critical execution path. When a load may cause a cache miss, it is even more important to move it early to hide its latency. However, a load is potentially dependent upon stores preceding it in a program. This makes it difficult to schedule the load before the stores. Compiler generated data speculation has been provided as a runtime disambiguation mechanism in IFF, which allows a compiler to move a load before a store even when the two operations are aliased. However, data speculation is effective only if the speculatively executed load is not frequently invalidated by the aliased stores; otherwise, costly runtime recovery code can severely degrade performance. In this paper, we define the concept of "conflict region" and use it to precisely formulate the invalidation rate problem. We solve the problem by decomposing it into two sub-problems: the address invalidation probability problem and the common path probability problem. We provide solutions to both with static analysis for array references inside loops. © (2001) by the International Society for Computers and Their Applications All rights reserved. . Keywords: Probability; Problem solving; Program compilers; Aliased; Common-path; Conflict region; Data speculations; Instruction level parallelism; Invalidation rate and common path probability; Itanium processor family; Memory load; Path probability; Runtimes; Static analysis
2001-01-01 - IA-64 itanium processor cartridge	The Itanium processor cartridge is a packaging optimization for electrical and thermal performance in a server environment. The 3-in. X 5-in cartridge contains the Itanium CPU, an innovative power delivery scheme, and an integrated vapor chamber thermal spreading lid for removing power. An I2C serial connection provides access to system management features. . Keywords: Computer simulation; Data transfer; Digital signal processing; Electric power supplies to apparatus; Interfaces (computer); Multichip modules; Servers; Spurious signal noise; Static random access storage; Ball grid array; Cache devices; Central processing unit; Electrical bus; Itanium processor cartridge; Organic land grid array; Pin grid array; Electronics packaging
2001-01-01 - Itanium processors will gang up to perform 13.6 teraflops	[No abstract available] . Keywords:
2001-01-01 - Itanium™ Processor system bus design	This paper presents the design of the Itanium™ Processors system bus interface achieving a peak data bandwidth of 2.1 GB/s in a glueless four-way multiprocessing System. A source-synchronous data bus with differential strobes enables this high bandwidth. Topics covered in this paper include optimization technique for the system topology, CPU package, signaling protocol, and I/O circuits. Highly accurate modeling and validation methodologies enable a good correlation of experimental results with simulation data. . Keywords: Bandwidth; Computer simulation; Network protocols; Topology; Bus interfaces; Signalling protocols; Multiprocessing systems
2001-01-01 - Memory performance analysis of SPEC2000C for the Intel® Itanium™ processor	We describe our memory performance analysis of SPEC2000C using the newly released Intel® Itanium™ processor (IPF). Memory overhead is very significant for SPEC200OC; on the average 39% cycles are spent in data stalls. Cache misses are significant, but also data translation performance (DTLB) affects many benchmarks. We present a study based on collecting measurements from the hardware performance counters and cache profiling using program instrumentation of loads/stores. We define important loads as the load sites that contribute at least 95% of the cache misses at all levels. Our measurements show that the number of important loads in a program is relatively small. Our analysis show that important loads are most of the time contained in inner loops, and that the trip counts of these loops is significantly high. We present preliminary results on using stride profiling to reduce cache misses of important loads, bringing an average of 6% improvement to SPEC2000C. Finally, we present our study of data translation performance and propose design choices. © 2001 IEEE. . Keywords: Cache Miss; Data translations; Hardware performance counters; Inner loops; Itanium; Memory overheads; Memory performance; Program instrumentations; Benchmarking
2001-01-01 - NEC provided 16 way Itanium server "AzusA" for Hewlett-Packard as an OEM products	[No abstract available] . Keywords:
2001-01-01 - On the importance of points-to analysis and other memory disambiguation methods for C programs	In this paper, we evaluate the benefits achievable from pointer analysis and other memory disambiguation techniques for C/C++ programs, using the framework of the production compiler for the Intel® Itanium™ processor. Most of the prior work on memory disambiguation has primarily focused on pointer analysis, and either presents only static estimates of the accuracy of the analysis (such as average points-to set size), or provides performance data in the context of certain individual optimizations. In contrast, our study is based on a complete memory disambiguation framework that uses a whole set of techniques including pointer analysis. Further, it presents how various compiler analyses and optimizations interact with the memory disambiguator, evaluates how much they benefit from disambiguation, and measures the eventual impact on the performance of the program. The paper also analyzes the types of disambiguation queries that are typically received by the disambiguator, which disambiguation techniques prove most effective in resolving them, and what type of queries prove difficult to be resolved. The study is based on empirical data collected for the SPEC CINT2000 C/C++ programs, running on the Itanium processor. © 2001 ACM. . Keywords:
2001-01-01 - On the importance of points-to analysis and other memory disambiguation methods for C programs	In this paper, we evaluate the benefits achievable from pointer analysis and other memory disambiguation techniques for C/C++ programs, using the framework of the production compiler for the Intel ® Itanium™ processor. Most of the prior work on memory disambiguation has primarily focused on pointer analysis, and either presents only static estimates of the accuracy of the analysis (such as average points-to set size), or provides performance data in the context of certain individual optimizations. In contrast, our study is based on a complete memory disambiguation framework that uses a whole set of techniques including pointer analysis. Further, it presents how various compiler analyses and optimizations interact with the memory disambiguator, evaluates how much they benefit from disambiguation, and measures the eventual impact on the performance of the program. The paper also analyzes the types of disambiguation queries that are typically received by the disambiguator, which disambiguation tec hniques prove most effective in resolving them, and what type of queries prove difficult to be resolved. The study is based on empirical data collected for the SPEC CINT2000 C/C++ programs, running on the Itanium processor. . Keywords: Hierarchical systems; Optimization; Program compilers; Query languages; Scheduling; Storage allocation (computer); Pointer analysis; Redundant load elimination (RLE); C (programming language)
2001-01-01 - Optimizing software data prefetches with rotating registers	Software data prefetching is a well-known technique to improve the performance of programs that suffer many cache misses at several levels of memory hierarchy. However, it has significant overhead in terms of increased code size, additional instructions, and possibly increased memory bus traffic due to redundant prefetches. This paper presents two novel methods to reduce the overhead of software data prefetching and improve the program performance by optimized prefetch scheduling. These methods exploit the availability of rotating registers and predication in architectures sit ch as the Itanium™ architecture. The methods (1) minimize redundant prefetches, (2) reduce the number of issue slots needed for prefetch instructions, and (3) avoid branch mispredict penalties - all with minimal code size increase. Compared to traditional data prefetching techniques, these methods (i) do not require loop unrolling, (ii) do not require predicate computations and (iii) require fewer machine resources. One of these methods has been implemented in the Intel Production Compiler for the Itanium™ processor. This technique is compared with traditional approaches for software prefetching and experimental results are presented based on the floating-point benchmark suite of CPU2000. . Keywords: Buffer storage; Computer architecture; Microprocessor chips; Optimization; Data prefetching; Program processors
2001-01-01 - Performance characteristics of Itanium™ processor on data encryption algorithms	In this paper, performance characteristics of the Itanium™ processor, the first implementation of the Itanium™ Processor Family (IPF), is examined for the task of data encryption using public key and symmetric key encryption algorithms. The Itanium™ processor key performance characteristics: wide issue width, large number of functional units, and large number of registers are examined for the task of performing multi-precision arithmetic dominant in public key algorithms and the computing requirements of symmetric key encryption algorithms. © 2001 IEEE. . Keywords: Algorithms; Data encryption; Data encryption algorithm; Functional units; Multi precision arithmetic; Performance characteristics; Public key algorithms; Public keys; Symmetric key encryption algorithm; Cryptography
2001-01-01 - Register renaming and scheduling for dynamic execution of predicated code	To achieve higher processor performance requires greater synergy between advanced hardware features and innovative compiler techniques. Recent advancement in compilation techniques for predicated execution has provided significant opportunity in exploiting instruction level parallelism. However, little research has been done on how to efficiently execute predicated code in a dynamic microarchitecture. In this paper, we evaluate hardware optimizations for executing predicated code on a dynamically scheduled microarchitecture. We provide two novel ideas to improve the efficiency of executing predicated code. On a generic Intel Itanium processor pipeline model, we demonstrate that, with some microarchitecture enhancements, a dynamic execution processor can achieve about 16% performance improvement over an equivalent static execution processor. . Keywords: Codes (symbols); Computer architecture; Computer hardware; Dynamic execution processors; Register renaming; Static execution processors; Program processors
2001-01-01 - Scientific Computing on the Itanium™ Processor	The 64-bit Intel® Itanium™ architecture is designed for high-performance scientific and enterprise computing, and the Itanium processor is its first silicon implementation. Features such as extensive arithmetic support, predication, speculation, and explicit parallelism can be used to provide a sound infrastructure for supercomputing. A large number of high-performance computer companies are offering Itanium™-based systems, some capable of peak performance exceeding 50 GFLOPS. In this paper we give an overview of the most relevant architectural features and provide illustrations of how these features are used in both low-level and high-level support for scientific and engineering computing, including transcendental functions and linear algebra kernels. © 2001 ACM. . Keywords: Functions; Enterprise computing; EPIC; Fused multiply-add; High performance computers; High performance scientific computing; Itania (TM) processor; Itanium; Itanium processor; Silicon implementation; Transcendental functions; Linear algebra
2001-01-01 - Speculative precomputation: Long-range prefetching of delinquent loads	This paper explores Speculative Precomputation, a technique that uses idle thread contexts in a multithreaded architecture to improve performance of single-threaded applications. It attacks program stalls from data cache misses by pre-computing future memory accesses in available thread contexts, and prefetching these data. This technique is evaluated by simulating the performance of a research processor based on the Itanium™ ISA supporting Simultaneous Multithreading. Two primary forms of Speculative Precomputation are evaluated. If only the non-speculative thread spawns speculative threads, performance gains of up to 30% are achieved when assuming ideal hardware. However, this speedup drops considerably with more realistic hardware assumptions. Permitting speculative threads to directly spawn additional speculative threads reduces the overhead associated with spawning threads and enables significantly more aggressive speculation, overcoming this limitation. Even with realistic costs for spawning threads, speedups as high as 169% are achieved, with an average speedup of 76%. . Keywords: Cache memory; Computer hardware; Computer simulation; Data handling; Multiprogramming; Pipeline processing systems; Program compilers; Delinquent loads; Long range prefetching; Multithreading; Speculative precomputation; Reduced instruction set computing
2001-01-01 - Statistical clock skew modeling with data delay variations	Accurate clock skew budgets are important for microprocessor designers to avoid hold-time failures and to properly allocate resources when optimizing global and local paths. Many published clock skew budgets neglect voltage jitter and process variation, which are becoming dominant factors in otherwise balanced H-trees. However, worst-case process variation assumptions are severely pessimistic. This paper describes the major sources of clock skew in a microprocessor using a modified H-tree and applies the model to a second-generation Itanium-M processor family microprocessor currently under design. Monte Carlo simulation is used to develop statistical clock skew budgets for setup and hold time constraints in a four-level skew hierarchy. Voltage jitter through the phase locked loop (PLL) and clock buffers accounts for the majority of skew budgets. We show that taking into account the number of nearly critical paths between clocked elements at each level of the skew hierarchy and variations in the data delays of these paths reduces the difference between global and local skew budgets by more than a factor of two. Another insight is that data path delay variability limits the potential cycle-time benefits of active deskew circuits because the paths with the worst skew are unlikely to also be the paths with the longest data delays. . Keywords: Buffer storage; Computer simulation; Electric potential; Jitter; Monte Carlo methods; Phase locked loops; Resource allocation; Spurious signal noise; Clock skew modeling; Microprocessor chips
2001-01-01 - The impact of if-conversion and branch prediction on program execution on the Intel® Itanium™ processor	The research community has studied if-conversion for many years. However, due to the lack of existing hardware, studies were conducted by simulating code generated by experimental compilers. This paper presents the first comprehensive study of the use of predication to implement if-conversion on production hardware with a near-production compiler. To better understand trends in the measurements, we generated binaries at three increasing levels of if-conversion aggressiveness. For each level, we gathered data regarding the global runtime effects of if-conversion on overall execution time, register pressure, code size, and branch behavior. Furthermore, we studied the inherent characteristics of program control-flow structure related to branching to help determine fundamental limits of if-conversion. Our results show that on the Itanium™ processor if-conversion could potentially remove 29% of the branch mispredictions in SPEC2000CINT but that this accounts for a substantially smaller overall program speedup than previously reported. . Keywords: Code converters; Codes (symbols); Computer simulation; Program compilers; Shift registers; Program execution; Microprocessor chips
2001-01-01 - The itanium platform lands its first computing systems	[No abstract available] . Keywords:
2001-01-01 - Towards a formal model of shared memory consistency for Intel Itanium™	A formal model of shared memory consistency covering a core set of reverse-engineered instructions for intel itanium was presented. The model uses a few explicit devices to clearly describe the tricky notion of casuality. The model deals with cacheable memory instructions consisting of ordinary stores, release stores, ordinary loads and acquire loads. The operational semantics in terms of five data structures held by various processors was also discussed. . Keywords: Cache memory; Computer program listings; Data structures; Digital libraries; Formal logic; Mathematical models; Program processors; Semantics; Shared memory systems; Data storage equipment
2002-01-01 - A fully bypassed six-issue integer datapath and register file on the itanium-2 microprocessor	The six-issue integer datapath of the second-generation Itanium Microprocessor is described. Pulse techniques enable a high-speed, 20-ported, 128-entry, 65-bit register file with only 12 wordlines per register. A four-stage operand bypass network achieves a fully bypassed design with operands sourced from 34 locations with 16 destinations. To control this network, over 280 bypass comparators are utilized. Using half a clock for execution and half a clock for bypass, each result is available for the next instruction. Functional units are pre-enabled, reducing power consumption by 15% while eliminating a stage of result muxing and improving performance. The part is fabricated in a six-layer, 18-μm process and operates at 1.0 GHz at 1.5 V, consuming less than 130 W in about 420 mm2. . Keywords: Digital integrated circuits; Integrated circuit layout; Integrated circuit manufacture; Spurious signal noise; Bypass network; Bypassed design; Bypassed six issue integer datapath; Power consumption reduction; Pulse techniques; Microprocessor chips
2002-01-01 - A fully-bypassed 6-issue integer datapath and register file on an Itanium Microprocessor	A discussion of a fully-bypassed 6-issue integer datapath and register file on an Itanium microprocessor is presented. The architecture consists of a 6-tissue integer datapath with a 20-ported, 128×65b register file (RF). The 20-ported RF of the system is 2.2mm2 and incorporates 8 write and 12 read ports. Integer bypassing is done through four stages consisting of first stage on the RF write bit lines with second stage muxing the 8 WRB results from the RF write bit lines and 8IEU DET stage results, the third stage muxing the 6 ALU EXE results, RF read data, and the early mux results and the final stage muxing the two LID cache data returns, the middle mux result, and the opcode value. . Keywords: Buffer storage; Decoding; Feedback; Field effect semiconductor devices; Logic circuits; Multimedia systems; Multiplexing; Register file (RF); Microprocessor chips
2002-01-01 - A fully-bypassed 6-issue integer datapath and register file on an itanium microprocessor	A 6-issue integer datapath with a 20-ported 128×65 b register file in a 0.18 μm process operates up to 1.2 GHz at 1.5 V. Operands bypass through 4 stages, from 34 locations, using 1/2 clock for execution and 1/2 clock for bypass. Each result is available for the next instruction. . Keywords: Data structures; Decoding; Logic circuits; Multiplexing; Spurious signal noise; Itanium microprocessors; Microprocessor chips
2002-01-01 - A specification and verification framework for developing weak shared memory consistency protocols	A specification and verification methodology for Distributed Shared Memory consistency protocols implementing weakshared memory consistency models is proposed. Our approach uniformly describes a wide range of weakmemory models in terms of a single concept—the visibility order of loads, stores, and synchronization operations, as perceived by all the processors. A given implementation is correct with respect to a weakmemory model if it produces executions satisfying the visibility order for that memory model. Given an implementation, the designer annotates it with events from the visibility order, and runs reachability analysis to verify it against a specification that is also similarly annotated. A specification is obtained in two stages: first, the designer reverse engineers an intermediate abstraction from the implementation by replacing the coherence networkwith a logically equivalent concurrent data structure. The replacement is selected in a standard way, depending almost exclusively on the memory model. Verification of the intermediate abstraction against a visibility order specification can be accomplished using theorem-proving. The methodology was applied to four snoopybus protocols implementing aspects of the Alpha and Itanium memory models, with encouraging results. © Springer-Verlag Berlin Heidelberg 2002. . Keywords: Abstracting; Computer aided design; Concurrency control; Design; Formal methods; Memory architecture; Visibility; Concurrent data structures; Distributed shared memory; Memory consistency models; Memory modeling; Reachability analysis; Specification and verification; Synchronization operation; Visibility ordering; Specifications
2002-01-01 - An EPIC processor with pending functional units	The Itanium processor, an implementation of an Explicitly Parallel Instruction Computing (EPIC) architecture, is an in-order processor that fetches, executes, and forwards results to functional units in-order. The architecture relies heavily on the compiler to expose Instruction Level Parallelism (ILP) to avoid stalls created by in-order processing. The goal of this paper is to examine, in small steps, changing the in-order Itanium processor model to allow execution to be performed out-of-order. The purpose is to overcome memory and functional unit latencies. To accomplish this, we consider an architecture with Pending Functional Units (PFU). The PFU architecture assigns/schedules instructions to functional units in-order. Instructions sit at the pending functional units until their operands become ready and then execute out-of-order. While an instruction is pending at a functional unit, no other instruction can be scheduled to that functional unit. We examine several PFU architecture designs. The minimal design does not perform renaming, and only supports bypassing of non-speculative result values. We then examine making PFU more aggressive by supporting speculative register state, and then finally by adding in register renaming. We show that the minimal PFU architecture provides on average an 18% speedup over an in-order EPIC processor and produces up to half of the speedup that would be gained using a full out-of-order architecture. © 2002 Springer Berlin Heidelberg. . Keywords: Computer architecture; Digital signal processing; Program compilers; Parallel processing systems; Program compilers; Program processors; Architecture designs; Explicitly parallel instruction computing architectures; Functional units; Instruction level parallelism; Itanium processor; Order processing; Out of order; Out-of-order architecture; Register renaming; Architecture designs; Explicitly parallel instruction computing architectures; Functional units; Instruction level parallelism; Itanium processor; Order processing; Out-of-order architecture; Register renaming; Architectural design; Computer architecture
2002-01-01 - An on-chip 3 MB subarray-based 3rd level cache on an itanium microprocessor	Design of an on-chip 3MB subarray-based third level cache on an itanium microprocessor was described and it achieved 85% array efficiency. The chip was characterized to operate up to 1.2GHz. The cache allowed a store and a load in every four core cycles and provided a total bandwidth of 64GB/s at 1.0GHz. Small way size enabled TLB, TAG, and data array accesses all to start untranslated address bits. . Keywords: Bandwidth; Data transfer; Decoding; Feedback; Flip flop circuits; Gates (transistor); Integrated circuit layout; Microprocessor chips; Random access storage; Fully custom cells; Subarray design; Cache memory
2002-01-01 - Benchmarking and tuning the MILC code on clusters and supercomputers	Recently, we have benchmarked and tuned the MILC code on a number of architectures including Intel Itanium and Pentium IV (PIV), dual-CPU Athlon, and the latest Compaq Alpha nodes. Results will be presented for many of these, and we shall discuss some simple code changes that can result in a very dramatic speedup of the KS conjugate gradient on processors with more advanced memory systems such as PIV, IBM SP and Alpha. . Keywords:
2002-01-01 - Combining shared and distributed memory programming models on clusters of symmetric multiprocessors: Some basic promising experiments	This note presents some experiments on different clusters of SMPs, where both distributed and shared memory parallel programming paradigms can be naturally combined. Although the platforms exhibit the same macroscopic memory organization, it appears that their individual overall performance is closely dependent on the ability of their hardware to efficiently exploit the local shared memory within the nodes. In that context, cache blocking strategy appears to be very important not only to get good performance out of each individual processor but mainly good performance out of the overall computing node since sharing memory locally might become a severe bottleneck. On a very simple benchmark, representative of many large simulation codes, we show through numerical experiments that mixing the two programming models enables us to get attractive speed-ups that compete with a pure distributed memory approach. This opens promising perspectives for smoothly moving large industrial codes developed on distributed vector computers with a moderate number of processors on these emerging platforms for intensive scientific computing that are the clusters of SMPs. © 2002 Sage Publications. . Keywords: Cache memory; Computer hardware; Computer programming; Computer simulation; Multiprocessing programs; Parallel processing systems; Distributed memory; Message passing interface (MPI); OpenMP; Performance evaluation; Shared memory; Distributed computer systems
2002-01-01 - Comparing and contrasting a commercial OLTP workload with CPU2000 on IPF	With the recent introduction of Itanium Processor Family (IPF) microprocessors for enterprise servers it is imperative to understand the behavior of server class applications. This paper analyzes the behavior of the Oracle Database Benchmark (ODB), an online transaction processing (OLTP) workload, and compares it with SPEC CPU2000. This study examines code mix, instruction and data supply, and value locality. The results show that while IPF's bundle constraints cause a large injection of NOPs into the code stream, IPFs register stack engine successfully reduces the number of memory operations by nearly 50%. The control-flow predictability of ODB is better than CPU2000, in spite of ODB's large active branch footprint. Due to ODB's large memory footprint, cache misses (particularly instruction cache misses) are a much more serious problem than in CPU2000. © 2002 IEEE. . Keywords: Program processors; Enterprise servers; Instruction cache miss; Itanium processor family; Memory footprint; Memory operations; Online transaction processing; Oracle database; Register stack engines; Cache memory
2002-01-01 - Data cache design considerations for the Itanium® processor	The second member in the Itanium Processor Family, the Itanium 2 processor, was designed to meet the challenge for high performance in today's technical and commercial server applications. The Itanium 2 processor's data cache microarchitecture provides abundant memory resources, low memory latencies and cache organizations tuned to for a variety of applications. The data cache design provides four memory ports to support the many performance optimizations available in the EPIC (Explicitly Parallel Instruction Computing) design concepts, such as prediction, speculation and explicit prefetching. The three-level cache hierarchy provides a 16KB 1-cycle first level cache to support the moderate bandwidths needed by integer applications. The second level cache is 256KB with a relatively low latency and FP balanced bandwidth to support technical applications. The on-chip third level cache is 3MB and is designed to provide the low latency and the large size needed by commercial and technical applications. . Keywords: Cache memory; Computer simulation; Digital arithmetic; Parallel processing systems; Program compilers; Resource allocation; Response time (computer systems); Storage allocation (computer); Data cache design; Explicitly parallel instruction computing; Itanium processor family; Computer architecture
2002-01-01 - Efficient discovery of regular stride patterns in irregular programs and its use in compiler prefetching	Irregular data references are difficult to prefetch, as the future memory address of a load instruction is hard to anticipate by a compiler. However, recent studies as well as our experience indicate that some important load instructions in irregular programs contain stride access patterns. Although the load instructions with stride patterns are difficult to identify with static compiler techniques, we developed an efficient profiling method to discover these load instructions. The new profiling method integrates the profiling for stride information and the traditional profiling for edge frequency into a single profiling pass. The integrated profiling pass runs only 17% slower than the frequency profiling alone. The collected stride information helps the compiler to identify load instructions with stride patterns that can be prefetched efficiently and beneficially. We implemented the new profiling and prefetching techniques in a research compiler for Itanium™ Processor Family (IPF), and obtained significant performance improvement for the SPECINT2000 programs running on Itanium machines. For example, we achieved a 1.59x speedup for 181.mcf, 1.14x for 254.gap, and 1.08x for 197.parser. We also showed that the performance gain is stable across input data sets. These benefits make the new profiling and prefetching techniques suitable for production compilers. . Keywords: Computer systems; Data acquisition; Information analysis; Storage allocation (computer); Compiler prefetching; Irregular programs; Program compilers
2002-01-01 - Eliminating exception constraints of Java programs for IA-64	Java exception checks are designed to ensure that any, faulting instruction causing a hardware exception does not terminate the program abnormally. These checks, however, impose some constraints upon the execution order between an instruction potentially raising a Java exception and a faulting instruction causing a hardware exception. This reduces the effectiveness of instruction reordering optimization. We propose a new framework to effectively perform speculation for the Java language using a direct acyclic graph representation based on the SSA form. Using this framework, we apply a well-known speculation technique to a faulting load instruction to eliminate such constraints. We use edges to represent exception constraints. This allows us to accurately estimate the potential reduction of the critical path length for applying speculation. We also propose an approach to avoid extra copy instructions and to generate efficient code with minimum register pressure. We have implemented the technique in the IBM Java Just-In-Time compiler, and observed performance improvements up to 25% for micro-benchmark programs, up to 10% for Java Grande Benchmark Suite, and up to 12% for SPECjvm98 on an Itanium processor. © 2002 IEEE. . Keywords: Accident prevention; Benchmarking; Computer hardware; Computer software; Distributed computer systems; Faulting; Hardware; Laboratories; Parallel architectures; Parallel processing systems; Program compilers; Program processors; Critical path lengths; Direct acyclic graphs; Java; Just in time compilers; Potential reduction; Register pressure; Registers; Runtimes; Java programming language
2002-01-01 - FRITS - A microprocessor functional BIST method	This paper describes a novel functional Built-in-Self-Test method for microprocessors. This technique is based on the fundamental principle that complex chips have embedded functionality that can be used to implement a comprehensive self-test strategy. Functional testing has generally been associated with expensive testers. In order to lower the cost of test, there is a general trend to adopt structural test techniques like scan that enable use of low cost testers. One of the key advantages of the test method described here is that it enables functional testing of microprocessors on low cost testers. Detailed implementation of this technique, the test generation methodology, the fault grade methodology and silicon results on Intel® Pentium® 4 and Itanium™ family microprocessors are presented. . Keywords: Cache memory; Computer simulation; Data compression; Data structures; Design for testability; Fault tolerant computer systems; Interfaces (computer); Microprocessor chips; Silicon; Fault grade method; Functional Random Instruction Testing at Speed; Functional testing; Structural tester; Built-in self test
2002-01-01 - Intel 870: A building block for cost-effective, scalable servers	The 870 chipset family has been developed to provide scalable enterprise server chipsets for next-generation Intel Itanium and Xeon server processors. This building-block architecture also lets system vendors design components to work with 870 building blocks that will scale beyond 16 processors. . Keywords: Cache memory; Client server computer systems; Computer peripheral equipment; Cost effectiveness; Database systems; Electronic commerce; Integrated circuit layout; Interconnection networks; Interfaces (computer); Internet; Multiprocessing systems; Network protocols; Cache coherence protocols; Double data rate memory; Dual inline memory modules; Enterprise server systems; Scalability port; Shared memory systems; Microprocessor chips
2002-01-01 - Intel's awaited and belated Itanium 2 is launched as the company's Q2 results disappoint	[No abstract available] . Keywords:
2002-01-01 - Just-in-time Java compilation for the Itanium® processor	This paper describes a just-in-time (JIT) Java compiler for the Intel® Itanium® processor. The Itanium processor is an example of an Explicitly Parallel Instruction Computing (EPIC) architecture and thus relies on aggressive and expensive compiler optimizations for performance. Static compilers for Itanium use aggressive global scheduling algorithms to extract instruction-level parallelism. In a JIT compiler, however, the additional overhead of such expensive optimizations may offset any gains from the improved code. In this paper, we describe lightweight code generation techniques for generating efficient Itanium code. Our compiler relies on two basic methods to generate efficient code. First, the compiler uses inexpensive scheduling heuristics to model the Itanium microarchitecture. Second, the compiler uses the semantics of the Java virtual machine to extract instruction-level parallelism. © 2002 IEEE. . Keywords: Codes (symbols); Computer aided instruction; Computer architecture; Java programming language; Just in time production; Microprocessor chips; Optimization; Parallel architectures; Parallel processing systems; Program processors; Scheduling; Scheduling algorithms; Semantics; Concurrent computing; Delay; Java; Optimizing compilers; Parallel processing; Performance Gain; Processor scheduling; Registers; Virtual machining; Program compilers
2002-01-01 - Memory latency-tolerance approaches for Itanium processors: Out-of-order execution vs. speculative precomputation	The performance of in-order execution Itanium™ processors can suffer significantly due to cache misses. Two memory latency tolerance approaches can be applied for the Itanium processors. One uses an out-of-order (OOO) execution core; the other assumes multithreading support and exploits cache prefetching via speculative precomputation (SP). This paper evaluates and contrasts these two approaches. In addition, this paper assesses the effectiveness of combining the two approaches. For a select set of memory-intensive programs, an in-order SMT Itanium processor using speculative precomputation can achieve performance improvement (92%) comparable to that of an out-of-order design (87%). Applying both 000 and SP yields a total performance improvement of 141% over the baseline in-order machine. OOO tends to be effective in prefetching-for L1 misses; whereas SP is primarily good at covering L2 and L3 misses. Our analysis indicates that the two approaches can be redundant or complementary depending on the type of delinquent loads that each targets. Both approaches are effective on delinquent loads in the loop body; however only SP is effective on delinquent loads found in loop control code. © 2002 IEEE. . Keywords: Memory architecture; Microprocessor chips; Multitasking; Process design; Scheduling; Supercomputers; Surface mount technology; Yarn; Delay; Educational institutions; Micro architectures; Out of order; Prefetching; Computer architecture
2002-01-01 - On the predictability of program behavior using different input data sets	Smaller input data sets such as the test and the train input sets are commonly used in simulation to estimate the impact of architecture/micro-architecture features on the performance of SPEC benchmarks. They are also used for profile feedback compiler optimizations. In this paper, we examine the reliability of reduced input sets for performance simulation and profile feedback optimizations. We study the high level metrics such as IPC and procedure level profiles as well as lower level measurements such as execution paths exercised by various input sets on the SPEC2000int benchmark. Our study indicates that the test input sets are not suitable to be used for simulation because they do not have an execution profile similar to the reference input runs. The train data set is better than the test data sets at maintaining similar profiles to the reference input set. However, the observed execution paths leading to cache misses are very different between using the smaller input sets and the reference input sets. For current profile based optimizations, the differences in quality of profiles may not have a significant impact on performance, as tested on the Itanium processor with an Intel compiler. However, we believe the impact of profile quality will be greater for more aggressive profile guided optimizations, such as cache prefetching. © 2002 IEEE. . Keywords: Benchmarking; Computer architecture; Computer science; Computer testing; Feedback; Input output programs; Microprocessor chips; Program processors; Software testing; Statistical tests; Benchmark testing; Compiler optimizations; Design optimization; Feedback optimization; Optimizing compilers; Performance simulation; Reduced input sets; Runtimes; Program compilers
2002-01-01 - Packaging the Itanium® microprocessor	In this paper we will discuss the thermal, mechanical, and electrical requirements and design solutions for the Itanium® processor cartridge. High power dissipation level of the cartridge, with up to five active dice, poses significant thermal design challenges. The thermal solution employed is presented and the design of heat pipe lid and effectiveness of the thermal interface material is discussed. The thermal behavior of the cartridge is analyzed using numerical analysis and experimental validations. The mechanical section discusses the pieces of the cartridge enclosure designed to ensure proper bond line thickness (BLT) of the thermal interface between the die and the heat pipe lid pedestals, engagement of the cartridge with enabling components, and protection against shock and vibration under various environmental conditions. Demanding power requirements by the Itanium® processor cartridge and the high-speed buses used for communication between the CPU and other components resulted in challenging package and substrate design. High-speed layout techniques were employed to meet the electrical performance requirements. The design challenges along with the substrate technology developed for the processor cartridge form the core topic of discussion in the electrical section. The physical implementation of the packages and substrates to achieve these performance targets is also described. . Keywords: Computer hardware; Heat pipes; Interfaces (materials); Microprocessor chips; Multichip modules; Numerical analysis; Bond line thickness (BLT); Electronics packaging
2002-01-01 - Peppermint and Sled: Tools for evaluating SMP systems based on IA-64 (IPF) processors	In this paper, we describe Peppermint and Sled: tools developed for evaluations of computer systems based on IA-64 processors. Sled generates trace from applications running on IA-64 processors, while Peppermint models the complete system using cycle-accurate, trace-driven simulation. Peppermint is based on Augmint, which leaves open the possibility of doing execution-driven simulations in future. Peppermint and Sled allow us to perform a trace-based evaluation of 4 applications running on SMP systems based on Itanium and McKinley processors. We find that the improvement in IPC of McKinley relative to Itanium ranges from 7% to over 100% for our different applications. The improvement can be attributed to a variety of factors. These range from the availability of additional functional units and issue ports in the McKinley processor to our assumption of a better memory system. While the improvement in performance remains valid in SMP systems in some cases, higher contention for system bus and memory reduces the performance gain in other cases. Increasing the system bits bandwidth and size of queues for pending requests in the memory controller are identified as first steps for optimizing SMP performance. © 2002 IEEE. . Keywords:
2002-01-01 - Post-pass binary adaptation for software-based speculative precomputation	Recently, a number of thread-based prefetching techniques have been proposed. These techniques aim at improving the latency of single-threaded applications by leveraging multithreading resources to perform memory prefetching via speculative prefetch threads. Software-based speculative precomputation (SSP) is one such technique, proposed for multithreaded Itanium models. SSP does not require expensive hardware support-instead it relies on the compiler to adapt binaries to perform prefetching on otherwise idle hardware thread contexts at run time. This paper presents a post-pass compilation tool for generating SSP-enhanced binaries. The tool is able to: (1) analyze a single-threaded application to generate prefetch threads; (2) identify and embed trigger points in the original binary; and (3) produce a new binary that has the prefetch threads attached. The execution of the new binary spawns the speculative prefetch threads, which are executed concurrently with the main thread. Our results indicate that for a set of pointer-intensive benchmarks, the prefetching performed by the speculative threads achieves an average of 87% speedup on an in-order processor and 5% speedup on an out-of-order processor. . Keywords: Adaptive systems; Bandwidth; Computer hardware; Data acquisition; Program compilers; Programming theory; Surface mount technology; Software-based speculative precomputations (SSP); Computer aided software engineering
2002-01-01 - Power-performance trade-offs for energy-efficient architectures: A quantitative study	The drastic increase in power consumption by modern processors emphasizes the need for power-performance trade-offs in architecture design space exploration and compiler optimizations. This paper reports a quantitative study on the power-performance trade-offs in software pipelined schedules for an Itanium-like EPIC architecture with dual-speed pipelines, in which functional units are partitioned into fast ones and slow ones. We have developed an integer linear programming formulation to capture the power-performance tradeoffs for software pipelined loops. The proposed integer linear programming formulation and its solution method have been implemented and tested on a set of SPEC2000 benchmarks. The results are compared with an Itanium-like architecture(baseline) in which there are four functional units (FUs) and all of them are fast units. Our quantitative study reveals that by introducing a few slow FUs in place of fast Fus in the baseline architecture, the total energy consumed by FUs can be considerably reduced. When 2 out of 4 FUs are set as slow, the total energy consumed by FUs is reduced by up to 31.1% (with an average reduction of 25.2%) compared with the baseline configuration, while the performance degradation caused by using slow FUs is small. If performance demand is less critical, then energy reduction of up to 40.3% compared with the baseline configuration can be achieved. . Keywords: Algorithms; Computer architecture; Computer simulation; Integer programming; Integrated circuit testing; Iterative methods; Linear programming; Program compilers; Response time (computer systems); Central processing unit; Functional units; Integer linear programming; Software Package CPLEX; Software pipelining; Microprocessor chips
2002-01-01 - Proceedings - Annual Workshop on Interaction between Compilers and Computer Architectures, INTERACT	The proceedings contain 10 papers. The topics discussed include: compiling for fine-grain concurrency: planning and performing software thread integration; dynamically scheduling VLIW instructions with dependency information; accuracy of profile maintenance in optimizing compilers; mastering startup costs in assembler-based compiled instruction-set simulation; on the predictability of program behavior using different input data sets; quantitative evaluation of the register stack engine and optimizations for future Itanium processors; a study on data allocation of on-chip dual memory banks; code size efficiency in global scheduling for ILP processors; code compression by register operand dependency; and code cache management schemes for dynamic optimizers. . Keywords:
2002-01-01 - Quantitative evaluation of the register stack engine and optimizations for future Itanium processors	This paper examines the efficiency of the register stack engine (RSE) in the canonical Itanium architecture, and introduces novel optimization techniques to enhance the RSE performance. To minimize spills and fills of the physical register file, optimizations are applied to reduce internal fragmentation in statically allocated register stack frames. Through the use of dynamic register usage (DRU) and dead register value information (DVI), the processor can dynamically guide allocation and deallocation of register frames. Consequently, a speculatively allocated register frame with a dynamically determined frame size can be much smaller than the statically determined frame size, thus achieving minimum spills and fills. Using the register stack engine (RSE) in the canonical Itanium architecture as the baseline reference, we thoroughly study and gauge the tradeoffs of the RSE and the proposed optimizations using a set of SPEC CPU2000 benchmarks built with different compiler optimizations. A combination of frame allocation policies using the most frequent frame size and deallocation policies using dead register information proves to be highly effective. On average, a 71% reduction in aggregate spills and fills can be achieved over the baseline reference. © 2002 IEEE. . Keywords: Aggregates; Engines; Microprocessor chips; Program compilers; Program processors; Welding; Delay; Description languages; Educational institutions; Optimizing compilers; Registers; Computer architecture
2002-01-01 - Reuse distance-based cache hint selection	Modern instruction sets extend their load/store-instructions with cache hints, as an additional means to bridge the processor-memory speed gap. Cache hints are used to specify the cache level at which the data is likely to be found, as well as the cache level where the data is stored after accessing it. In order to improve a program’s cache behavior, the cache hint is selected based on the data locality of the instruction. We represent the data locality of an instruction by its reuse distance distribution. The reuse distance is the amount of data addressed between two accesses to the same memory location. The distribution allows to efficiently estimate the cache level where the data will be found, and to determine the level where the data should be stored to improve the hit rate. The Open64 EPIC-compiler was extended with cache hint selection and resulted in speedups of up to 36% in numerical and 23% in non numerical programs on an Itanium multiprocessor. © Springer-Verlag Berlin Heidelberg 2002. . Keywords: Program compilers; Cache behavior; Data locality; IMPROVE-A; Instruction set; Memory locations; Numerical programs; Processor memory; Reuse distance; Cache memory
2002-01-01 - Scalability port: A coherent interface for shared memory multiprocessors	The scalability port (SP) is a point-to-point cache consistent interface to build scalable shared memory multiprocessors. The SP interface consists of three layers of abstraction: the physical layer, the link layer and the protocol layer. The physical layer uses pin-efficient simultaneous bi-directional signaling and operates at 800 MHz in each direction. The link layer supports virtual channels and provides flow control and reliable transmission. The protocol layer implements cache consistency, TLB consistency, synchronization, and interrupt delivery functions among others. The first implementation of the SP interface is in the Intel® E8870 and E9870 chipset for the Intel Itanium®2 processor and future generations of the Itanium processor family. © 2002 IEEE. . Keywords: Bandwidth; Integrated circuit interconnects; Memory architecture; Multiprocessing systems; Network layers; Network protocols; Packaging; Scalability; Switches; Coherent interface; Delay; Integrated circuit interconnections; Itanium processor family; Network topology; Physical layers; Reliable transmission; Shared memory multiprocessor; Cache memory
2002-01-01 - Scientific computing on the Itanium® processor	The 64-bit Intelpsy210 Itanium® architecture is designed for high-performance scientific and enterprise computing, and the Itanium processor is its first silicon implementation. Features such as extensive arithmetic support, predication, speculation, and explicit parallelism can be used to provide a sound infrastructure for supercomputing. A large number of high-performance computer companies are offering Itanium® -based systems, some capable of peak performance exceeding 50 GFLOPS. In this paper we give an overview of the most relevant architectural features and provide illustrations of how these features are used in both low-level and high-level support for scientific and engineering computing, including transcendental functions and linear algebra kernels. . Keywords: Functions; Linear algebra; Natural sciences computing; Parallel processing systems; Silicon; Supercomputers; Supercomputing; Microprocessor chips
2002-01-01 - Shared memory consistency protocol verification against weak memory models: Refinement via model-checking	Weak shared memory consistency models, especially those used by modern microprocessor families, are quite complex. The bus and/or directory-based protocols that help realize shared memory multiprocessors using these microprocessors are also exceedingly complex. Thus, the correctness problem -that all the executions generated by the multiprocessor for any given concurrent program are also allowed by the memory model -is a major challenge. In this paper, we present a formal approach to verify protocol implementation models against weak shared memory models through automatable refinement checking supported by a model checker. We define a taxonomy of weak shared memory models that includes most published commercial memory models, and detail how our approach applies over all these models. In our approach, the designer follows a prescribed procedure to build a highly simplified intermediate abstraction for the given implementation. The intermediate abstraction and the implementation are concurrently run using a modelchecker, checking for refinement. The intermediate abstraction can be proved correct against the memory model specification using theorem proving. We have verified four different Alpha as well as Itanium memory model implementationsagainst their respective specifications. The results are encouraging in terms of the uniformity of the procedure, the high degree of automation, acceptable run-times, and empirically observed bug-hunting efficacy. The use of parallel model-checking, based on a version of the parallel Murφ model checker we have recently developed for the MPI library, has been essential to finish the search in a matter of a few hours. © Springer-Verlag Berlin Heidelberg 2002. . Keywords: Abstracting; Computer aided analysis; Memory architecture; Microprocessor chips; Multiprocessing systems; Parallel processing systems; Specifications; Degree of automation; Modern microprocessor; Parallel model checking; Protocol implementation; Refinement checking; Shared memory model; Shared memory multiprocessor; Weak memory models; Model checking
2002-01-01 - The 16 kB single-cycle read access cache on a next-generation 64 b itanium microprocessor	A 16 kB four-ported physically addressed cache operates at 1.2 GHz with 19.2 GB/s peak bandwidth. Circuit and microarchitectural techniques are optimized to allow a single-cycle read access latency. The cache occupies 3.2×1.8 mm2 in a 0.18 μm process. . Keywords: Bandwidth; Decoding; Electric charge; Gates (transistor); Integrated circuit layout; Microprocessor chips; Data array decoders; Data cache design; Cache memory
2002-01-01 - The 16 kB single-cycle read access cache on a next-generation 64B itanium microprocessor	The 16 kB single-cycle read access cache on a next-generation 64 b Itanium microprocessor was presented. The level 1 (L1) data cache was fabricated on a 0.18 μm 6-metal bulk silicon complementary metal oxide semiconductor process. The chip occupied 3200×1800 μm2 and was characterized to operate at 1.2 GHz and 1.5 V. . Keywords: Bandwidth; Cache memory; Decoding; Field effect transistors; Random access storage; High bandwidth memories; Microprocessor chips
2002-01-01 - The core clock system on the next generation Itanium™ microprocessor	The core clock system on the next generation Itanium™ microprocessor was discussed. The clock distribution was realized by a precision-engineered wire system and a balanced, multi-level H-tree. The implementation of balanced H-tree route presented unique challenges between the clock routing team and the block designers. Using a grouping algorithm, Elmore delay calculations and summed capacitance values, the route designer created the link structures for the base tree. . Keywords: Algorithms; Cache memory; Capacitance; Computer simulation; Graphical user interfaces; Heuristic methods; Modulation; Phase locked loops; Precision engineering; Timing circuits; Timing jitter; Clock distribution; Microprocessor chips
2002-01-01 - The core clock system on the next-generation Itanium™ microprocessor	A PLL generates a high-frequency core clock for a 1 GHz processor by multiplying up the system clock. The clock is distributed across the 19×14 mm2 core via a shielded, balanced, H-tree to the final pulsed gated buffers with <62 ps measured skew. Test features include phase shrinking and regional skew manipulation. . Keywords: Buffer circuits; Energy dissipation; Oscillators (electronic); Phase locked loops; Routers; Pulsed gated buffers; Microprocessor chips
2002-01-01 - The high-bandwidth 256 kB 2nd level cache on an Itanium Microprocessor	[No abstract available] . Keywords: Arrays; Bandwidth; Buffer storage; Data reduction; Digital arithmetic; Queueing networks; Static random access storage; Transistors; Data array implementation; Microprocessor chips
2002-01-01 - The high-bandwidth 256 kB 2nd-level cache on an Itanium Microprocessor	The high-bandwidth 256 kB 2nd-level cache on Itanium Microprocessor was discussed. A second-level 256 kB unified cache was incorporated into a 1.2 GHz next-generation Itanium Microprocessor. Results showed that the datapath structures provide a non-blocking, out-of-order interface to the processor core achieving a minimum 5-cycle latency with 72 GB/s stand-alone bandwidth. . Keywords: Bandwidth; CMOS integrated circuits; Data structures; Electric potential; Itanium microprocessors; Microprocessor chips
2002-01-01 - The implementation of the itanium 2 microprocessor	This 64-b microprocessor is the second-generation design of the new Itanium architecture, termed explicitly parallel instruction computing (EPIC). The design seeks to extract maximum performance from EPIC by optimizing the memory system and execution resources for a combination of high bandwidth and low latency. This is achieved by tightly coupling microarchitecture choices to innovative circuit designs and the capabilities of the transistors and wires in the .18-μm bulk Al metal process [16]. The key features of this design are: a short eight-stage pipeline, 11 sustainable issue ports six Integer, four floating point, half-cycle access level-1 caches, 64-GB/s level-2 cache and 3-MB level-3 cache, all integrated on a 421 mm2 die. The chip operates at over 1 GHz and is built on significant advances in CMOS circuits and methodologies. After providing an overview of the processor microarchitecture and design, this paper describes a few of these key enabling circuits and design techniques. . Keywords: Aluminum; Bandwidth; Clocks; CMOS integrated circuits; Computer architecture; Integrated circuit layout; Logic design; Optimization; Transistors; Wire; Clock design; Design methodology; Dynamic logic; Explicitly parallel instruction computing; Itanium architecture; Microprocessor chips
2002-01-01 - The implementation of the next-generation 64 b Itanium™ microprocessor	The implementation of a next-generation 64b itanium microprocessor is discussed. The microprocessor operates at > 1.2 GHz with an eight stage pipeline in a 0.18 μm process and it incorporates over 220 M transistors on a 465 mm2 die. It is found that the microprocessor has three levels of on-chip cache totaling over 3.3 MB providing > 32 GB/s bandwidth at each level. . Keywords: Bandwidth; Cache memory; Phase locked loops; Transistors; Pulsed based latching; Microprocessor chips
2002-01-01 - The implementation of the next-generation 64 b Itanium™ microprocessor	The design and implementation of the next-generation 64b Itanium ™ microprocessor to achieve industry-leading performance on a broad range of technical and commercial applications at 1.0 GHz is discussed. Optimization for both the Itanium architechture and process technology for micro-architecture and circuits was achieved. A 1.5X to 2X performance improvement over the 1st generation design in the 0.18μm process was also achieved. . Keywords: Bandwidth; Buffer storage; Logic design; Multimedia systems; Optimization; Program compilers; Itanium microprocessors; Microprocessor chips
2002-01-01 - The on-chip 3MB subarray based 3rd level cache on an Itanium microprocessor	The on-chip 3 MB subarray based 3rd level cache on an Itanium microprocessor was presented. The ItaniumTM processor family next-generation processor built on a 6 Al metal layer 0.18 μm process contained three level of on-chip cache totalling more than 3.3 MB. The subarray memory access begins at any clock cycle and requires two cycles to complete. The arrays were characterized and functionally screened through direct access test (DAT) which allowed direct access to the arrays through the front side bus units. . Keywords: Cache memory; Field effect transistors; Flip flop circuits; PROM; Static random access storage; Erasable programmable read only memory (EPROM); Microprocessor chips
2002-01-01 - The on-chip 3-MB subarray-based third-level cache on an Itanium microprocessor	The 3-MB on-chip level three cache in the Itanium 2 Processor, built on an 0.18-μm, six-layer Al metal process, employs a subarray design style that efficiently utilizes available area and flexibly adapts to floor plan changes. Through a distributed decoding scheme and compact circuit design and layout, 85% array efficiency was achieved for the subarrays. In addition, various test and reliability features were included. The cache allows for a store and a load every four core cycles and has been characterized to operate above 1.2 GHz at 1.5 V and 110 °C. When running at 1.0 GHz, the cache provides a total bandwidth of 64 GB/s. . Keywords: Aluminum; Bandwidth; Decoding; Integrated circuit layout; Microprocessor chips; Random access storage; Reliability; Array efficiency; On chip level three cache; Subarrays; Cache memory
2002-01-01 - Thermal design of a high-density server	Numerical simulation and experimental verification have yielded a high-density server concept dissipating a nameplate power of 700 W in a 1U (44.45 mm or 1.75 in) rack-mount form factor. Air-cooled thermal management is achieved through a ducted radial blower, conventional heatsinks, and a partitioned airflow management strategy. It is shown that design and optimization of the air mover system requires attention not only to pressure and flow characteristics, but also to flow distribution properties. Optimizing the system for flow and thermal performance is an iterative process, most easily done before major layout or architectural properties have been fixed. Close collaboration between thermal analysts, board layout engineers, and system architects has proven the most effective method for optimizing the complete server design. . Keywords: Blowers; Computer simulation; Computer software; Cooling; Heat sinks; Microprocessor chips; Numerical analysis; Optimization; Radial blowers; Servers
2002-01-01 - Value-profile guided stride prefetching for irregular code	Memory operations in irregular code are difficult to prefetch, as the future address of a memory location is hard to anticipate by a compiler. However, recent studies as well as our experience indicatethat many irregular programs contain loads with near-constant strides. This paper presents a novel compiler technique to profile and prefetch for those loads. The profile captures not only the dominant stride values for each profiled load, but also the differences between the successive strides of the load. The profile information helps the compiler to classify load instructions into strongly or weakly strided and singlestrided or phased multi-strided. The prefetching decisions guided by the load classifications are highly selective and beneficial. We obtain significant performance improvement for the CPU2000 integer programs running on Itanium™ machines. For example, we achieve a 1.55x speedup for "181.mcf", 1.15x for "254.gap", 1.08x for "197.parser" and smaller gains in other benchmarks. We also show that the performance gain is stable across profile data sets and that the profiling overhead is low. These benefits make the new technique suitable for a production compiler. © Springer-Verlag Berlin Heidelberg 2002. . Keywords: Classification (of information); Integer programming; Compiler techniques; Irregular programs; Load classification; Memory locations; Memory operations; Performance Gain; Profile data set; Stride prefetching; Program compilers
2003-01-01 - 2003 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2003	The proceedings contain 22 papers. The topics discussed include: integrating complete-system and user-level performance/power simulators: the SimWatch approach; inferno: a functional simulation infrastructure for modeling diverse microarchitectural data speculations; performance potentials of compiler directed data speculation; empirical evaluation of capacity estimation tools; mathematical modeling of adaptive wormhole routing in the presence of self-similar traffic; accelerating private-key cryptography via multithreading on symmetric multiprocessors; a new synthetic web-server trace generation methodology; performance analysis and tracing of technical and Java applications on the Itanium 2 processor; and on evaluating request-distribution schemes for saving energy in server clusters. . Keywords:
2003-01-01 - 2nd International Conference on Numerical Analysis and Its Applications, NAA 2000	The proceedings contain 35 papers. The special focus in this conference is on Automata Based Methods, Specification Methods and Bounded Model Checking. The topics include: Describing and analysing circuits that are not quite regular; predicate abstraction with minimum predicates; efficient symbolic model checking of software using partial disjunctive partitioning; instantiating uninterpreted functional units and memory system; analyzing the intel itanium memory ordering rules using logic programming and sat; coverage metrics for formal verification; an optimized symbolic bounded model checking engine; semi-formal verification of memory systems by symbolic simulation; towards diagrammability and efficiency in event sequence languages; executing the formal semantics of the accellera property specification language by mechanised theorem proving; on combining symmetry reduction and symbolic representation for efficient model checking; on the correctness of an intrusion-tolerant group communication protocol; exact and efficient verification of parameterized cache coherence protocols; a programming language based analysis of operand forwarding; inductive assertions and operational semantics; a compositional theory of refinement for branching time; efficient distributed sat and sat-based distributed bounded model checking; convergence testing in term-level bounded model checking; efficient hybrid reachability analysis for asynchronous concurrent systems and improved symbolic verification using partitioning techniques. . Keywords:
2003-01-01 - 5th International Workshop on Advanced Parallel Processing Technologies, APPT 2003	The proceedings contain 77 papers. The special focus in this conference is on Architecture, Software, Grid and Network. The topics include: Using split queues to improve the performance of parallel switch; a data driven loop engine on array processor; a new architecture of a fast floating-point multiplier; a highly efficient FC-san based on load stream; a new high-performance distributed shared i/o system; design and implementation of Fortran front end; an alternative superscalar architecture with integer execution units only; a high efficiency distributed mutual exclusion algorithm; the security architecture of the java operating system JX; simultaneous multithreading trace processors; a VLSI architecture design of 1-d dwt; overcoming static register pressure for software pipelining in the Itanium architecture; separating data storage, data computation, and resource management one from another in operating systems; a high performance design and implementation of the virtual interface architecture; optimization of asynchronous volume replication protocol; predicate analysis based on path information; new no-blocking permutation network; a task-oriented parallel computation model; a method of data assignment on heterogeneous disk system; orthogonal design method for optimal cache configuration; a probabilistically correct election protocol in asynchronous distributed systems; a formal specification and method for MAS as a distributed system; optimal fixed priority assignment with limited priority levels; a proof assistant for mobile processes and data space fusion based approach for effective alignment of computation and data. . Keywords:
2003-01-01 - A 1.5-GHz 130-nm Itanium® 2 processor with 6-MB on-die L3 cache	This 130-nm Itanium 2 processor implements the explicitly parallel instruction computing (EPIC) architecture and features an on-die 6-MB 24-way set-associative level-3 cache. The 374-mm2 die contains 410 M transistors and is implemented in a dual-Vt process with six Cu interconnect layers and FSG dielectric. The processor runs at 1.5 GHz at 1.3 V and dissipates a maximum of 130 W. This paper reviews circuit design and package details, power delivery, the reliability, availability, and serviceability (RAS) features, design for test (DFT), and design for manufacturability (DFM) features, as well as an overview of the design and verification methodology. The fuse-based clock deskew circuit achieves 24-ps skew across the entire die, while the scan-based skew control further reduces it to 7 ps. The 128-bit front-side bus has a bandwidth of 6.4 GB/s and supports up to four processors on a single bus. . Keywords: Cache memory; Computer architecture; Design for testability; Dielectric materials; Dies; Electric power supplies to apparatus; Electronics packaging; Integrated circuit layout; Interconnection networks; Parallel processing systems; Reliability; Timing circuits; Clock deskew circuit; Explicitly parallel instruction computing architecture; Itanium processor; Microprocessor chips
2003-01-01 - A 1.5GHz third generation Itanium® 2 processor	This 130nm Itanium® 2 processor implements the Explicitly Parallel Instruction Computing (EPIC) architecture and features an on-die 6MB, 24-way set associative L3 cache. The 374mm2 die contains 410M transistors and is implemented in a dual-Vt process with 6 layers copper interconnect and FSG dielectric. The processor runs at 1.5GHz at 1.3V and dissipates a maximum of 130W. This paper reviews circuit design and package details, power delivery, RAS, DFT and DFM features, as well as an overview of the design and verification methodology. The fuse-based clock de-skew circuit achieves 24ps skew across the entire die, while the scan-based skew control further reduces it to 7ps. The 128-bit front-side bus supports up to 4 processors on a single bus with a bandwidth of up to 6.4GB/s. . Keywords: Bandwidth; Cache memory; Dielectric materials; Parallel processing systems; Transistors; Explicitly parallel instruction computing (EPIC); Microprocessor chips
2003-01-01 - A 1.5GHz third generation Itanium® processor	A third-generation 1.5GHz Itanium® processor implements the Explicitly Parallel Instruction Computing (EPIC) architecture and features an on-die 6MB, 24-way set associative L3 cache. The 374mm2 die contains 410M transistors and is implemented in a dual-V-0.13μm technology having 6-level Cu interconnects with FSG dielectric and dissipates 130W. . Keywords: Associative storage; Built-in self test; Cache memory; Copper; Design for testability; Dielectric materials; Microelectronic processing; Parallel processing systems; Copper interconnect layer; Explicitly parallel instruction computing architecture; Third generation Itanium processor; Microprocessor chips
2003-01-01 - A 400-MT/s 6.4-GB/s multiprocessor bus interface	This paper describes the design of a system bus interface for the 130-nm Itanium® 2 processor that operates at 400 MT/s (1 megatransfer = 1 Mb/s/pin) with a peak bandwidth of 6.4 GB/s. The high-speed operation is achieved by employing source-synchronous transfer with differential strobes. Short flight time is accomplished by double-sided placement of the processors. Preboost and postboost edge-rate control enables fast clock-to-output timing with tight edge-rate range. The built-in input/output (I/O) loopback test feature enables I/O parameters to be tested on die, using a delay-locked loop and interpolator with 21-ps phase-skew error and 15-ps rms jitter. Power modeling methodology facilitates accurate prediction of system performance. . Keywords: Bandwidth; Delay circuits; Electric power supplies to apparatus; Electronics packaging; Flip chip devices; Integrated circuit testing; Interfaces (computer); Interpolation; Mathematical models; Multiprocessing systems; Substrates; Delay locked loop; Multiprocessor bus interface; Source synchronous transfer; Microprocessor chips
2003-01-01 - A framework for modeling and optimization of prescient instruction prefetch	This paper describes a framework for modeling macroscopic program behavior and applies it to optimizing prescient instruction prefetch - a novel technique that uses helper threads to improve single-threaded application performance by performing judicious and timely instruction prefetch. A helper thread is initiated when the main thread encounters a spawn point, and prefetches instructions starting at a distant target point. The target identifies a code region tending to incur I-cache misses that the main thread is likely to execute soon, even though intervening control flow may be unpredictable. The optimization of spawn-target pair selections is formulated by modeling program behavior as a Markov chain based on profile statistics. Execution paths are considered stochastic outcomes, and aspects of program behavior are summarized via path expression mappings. Mappings for computing reaching, and posteriori probability; path length mean, and variance; and expected path footprint are presented. These are used with Tarjan's fast path algorithm to efficiently estimate the benefit of spawn-target pair selections. Using this framework we propose a spawn-target pair selection algorithm for prescient instruction prefetch. This algorithm has been implemented, and evaluated for the Itanium® Processor Family architecture. A limit study finds 4.8% to 17% speedups on an in-order simultaneous multithreading processor with eight contexts, over nextline and streaming I-prefetch for a set of benchmarks with high I-cache miss rates. The framework in this paper is potentially applicable to other thread speculation techniques. Copyright 2003 ACM. . Keywords: Algorithms; Buffer storage; Computational methods; Markov processes; Mathematical models; Optimization; Analytical Modeling; Helper Threads; Instruction Prefetch; Multithreading; Path Expressions; Programming theory
2003-01-01 - A performance comparison of matrix solvers on compaq alpha, intel itanium, and intel itanium II processors	The canonical form of many large-scale scientific and technical computing problems are often linear algebra problems. As such, routines such as matrix solvers find their use in a wide range of applications. The performance of matrix solvers are also often critical in determining the performance of the application programs. This paper reports on the performance of the Cholesky solvers provided by Compaq for the Compaq Alpha architecture and by Intel for the Intel Itanium and Intel Itanium II (IA-64) architectures, and a comparison is then made with the Cholesky solvers in the OptimaNumerics Linear Algebra Library. © Springer-Verlag Berlin Heidelberg 2003. . Keywords: Algebra; Application programs; Architecture; Linear algebra; Linear equations; Architecture specific tuning; Canonical form; Linear algebra libraries; Linear algebra problems; Linear solver; Performance comparison; Systems of linear algebraic equations; Technical computing; Matrix algebra
2003-01-01 - A region-based compilation infrastructure	The traditional framework for back-end compilation is based on the scope of functions, which is a natural boundary to partition an entire program for compilation. However, the sizes and structures of functions may not be the best scope for program analyses and transformations when considering compilation resources (e.g. time and space), performance, and functionality. This problem is particularly pronounced when modern compiler optimizations resort to sophisticated and expensive algorithms to aim at high performance computing. Furthermore, it is often beneficial to give priority to optimize the more profitable portions of programs. Earlier works have proposed ways to allow some control on the size and structure of optimization scope. In this paper we develop a new region-based compilation framework driven by the considerations of performance opportunities and compilation resources. In addition, we allow some optimization-directed attributes communicated from one optimization phase to another on a region basis to guide subsequent optimizations. This region-based framework has been implemented in the Open Research Compiler targeting Itanium® Processor Family (IPF). Experimental results from the SPEC2000Int programs show that this infrastructure provides an effective control on forming regions to meet the requirements of different optimizations. For example, the compilation time of instruction scheduling is significantly reduced by this region formation infrastructure while preserving or improving the overall performance. At the highest optimization level, the performance of one program has a 15.6% improvement by employing this region-based infrastructure. © 2003 IEEE. . Keywords: Computer architecture; Computers; Microprocessor chips; Program processors; Structural optimization; High performance computing; Optimizing compilers; Partial response channel; Partitioning algorithms; Performance analysis; Size control; Tail; Program compilers
2003-01-01 - A statistical adaptive block-matching motion estimation	In this paper, we address the problem of motion estimation (ME) in digital video sequences and propose a new fast, adaptive, and efficient block-matching algorithm. Higher quality and efficiency are achieved using a statistical model for the motion vectors. This model introduces adaptation in the search window, drastically reducing the number of positions where correlation-type computation is performed. The efficiency is further improved by progressively undersampling the macroblock. Patterns for undersampling are proposed to obtain the maximum benefit from single instruction multiple data (SIMD) instructions. In contrast with existing motion-estimation techniques, search strategy and subsampled patterns are closely linked. This shows that a good search strategy is much more important than blindly reducing the number of pixels considered for the matching pattern. We describe an implementation of the proposed matching strategy that exploits the very long instruction word (VLIW) and SIMD technology available in the new Itanium Processor Family. Results show that the proposed algorithm adapts easily to the evolution of the scene avoiding annoying quality drops that can be observed with other deterministic algorithms. The total number of operations required by the proposed method is inferior to those required by traditional approaches. . Keywords: Adaptive algorithms; Computer simulation; Image compression; Image segmentation; Probability distributions; Statistical methods; Very long instruction word architecture; Adaptive motion estimation; Block matching; Digital video; Single instruction multiple data instruction; Motion estimation
2003-01-01 - An OpenMP implementation of parallel FFT and its performance on IA-64 processors	In this paper, we propose an OpenMP implementation of a recursive algorithm for parallel fast Fourier transform (FFT) on shared-memory parallel computers. A recursive three-step FFT algorithm improves performance by effectively utilizing the cache memory. Performance results of one-dimensional FFTs on the DELL PowerEdge 7150 and the hp workstation zx600 are reported. We successfully achieved performance of about 757 MFLOPS on the DELL PowerEdge 7150 (Itanium 800 MHz, 4 CPUs) and about 871 MFLOPS on the hp workstation zx6000 (Itanium2 1 GHz, 2 CPUs) for 224-point FFT. © Springer-Verlag Berlin Heidelberg 2003. . Keywords: Algorithms; Application programming interfaces (API); Cache memory; One dimensional; FFT algorithm; Itanium; Recursive algorithms; Shared-memory parallels; Fast Fourier transforms
2003-01-01 - Analyzing the Intel Itanium memory ordering rules using logic programming and SAT	We present a non-operational approach to specifying and analyzing shared memory consistency models. The method uses higher order logic to capture a complete set of ordering constraints on execution traces, in an axiomatic style. A direct encoding of the semantics with a constraint logic programming language provides an interactive and incremental framework for exercising and verifying finite test programs. The framework has also been adapted to generate equivalent boolean satisfiability (SAT) problems. These techniques make a memory model specification executable, a powerful feature lacked in most non-operational methods. As an example, we provide a concise formalization of the Intel Itanium memory model and show how constraint solving and SAT solving can be effectively applied for computer aided analysis. Encouraging initial results demonstrate the scalability for complex industrial designs. © Springer-Verlag Berlin Heidelberg 2003. . Keywords: Computer aided analysis; Computer programming languages; Design; Formal logic; Program processors; Semantics; Boolean satisfiability; Constraint logic programming languages; Constraint Solving; Higher order logic; Memory modeling; Memory ordering; Operational methods; Ordering constraints; Logic programming
2003-01-01 - Clock Generation and Distribution for the Third Generation Itanium® Processor	The clock generation and distribution of the third generation Itanium® processor was described. The processor is implemented in a 130 nm copper-interconnect CMOS process technology and operates at 1.5 GHz. Clock optimization fuses enable post-silicon speed path balancing for higher performance. . Keywords: Algorithms; CMOS integrated circuits; Electric clocks; Electric fuses; Optimization; Phase locked loops; Clock distribution systems; Microprocessor chips
2003-01-01 - Compiler and runtime support for running OpenMP programs on Pentium- and Itanium-architectures	Exploiting thread-level parallelism (TLP) is a promising way to improve the performance of applications with the advent of general-purpose cost effective uni-processor and shared-memory multiprocessor systems. In this paper, we describe the OpenMP implementation in the Intel® C++ and Fortran compilers for Intel platforms. We present our major design consideration and decisions in the Intel compiler for generating efficient multithreaded codes guided by OpenMP directives and pragmas. We describe several transformation phases in the compiler for the OpenMP parallelization. In addition to compiler support, the OpenMP runtime library is a critical part of the Intel compiler. We present runtime techniques developed in the Intel OpenMP runtime library for exploiting thread-level parallelism as well as integrating the OpenMP support with other forms of threading termed as sibling parallelism. The performance results of a set of benchmarks show good speedups over the well-optimized serial code performance on Intel® Pentium- and Itanium-processor based systems. © 2003 IEEE. . Keywords: Application programming interfaces (API); Benchmarking; C++ (programming language); Cost effectiveness; Distributed parameter networks; Memory architecture; Multiprocessing systems; Parallel processing systems; Compiler optimizations; Hyper-threading technology; OpenMP; Parallelizations; Shared memory multiprocessor; Thread level parallelism; Program compilers
2003-01-01 - Compiler and runtime support for running OpenMP programs on Pentium- and Itanium-architectures	Exploiting Thread-Level Parallelism (TLP) is a promising way to improve the performance of applications with the advent of general-purpose cost effective uni-processor and shared-memory multiprocessor systems. In this paper, we describe the OpenMP∗ implementation in the Intel® C++ and Fortran compilers for Intel platforms. We present our major design consideration and decisions in the Intel compiler for generating efficient multithreaded codes guided by OpenMP directives and pragmas. We describe several transformation phases in the compiler for the OpenMP∗ parallelization. In addition to compiler support, the OpenMP runtime library is a critical part of the Intel compiler. We present runtime techniques developed in the Intel OpenMP runtime library for exploiting thread-level parallelism as well as integrating the OpenMP support with other forms of threading termed as sibling parallelism. The performance results of a set of benchmarks show good speedups over the well-optimized serial code performance on Intel® Pentium- and Itanium-processor based systems. © 2003 IEEE. . Keywords: Application programming interfaces (API); Benchmarking; C++ (programming language); Computer programming; Cost effectiveness; Distributed parameter networks; Laboratories; Memory architecture; Microprocessor chips; Multiprocessing systems; Parallel processing systems; Parallel programming; Program processors; Conferences; Optimizing compilers; Parallel processing; Programming profession; Run-time library; Software libraries; Software performance; Program compilers
2003-01-01 - Compiler optimization-space exploration	To meet the demands of modern architectures, optimizing compilers must incorporate an ever larger number of increasingly complex transformation algorithms. Since code transformations may often degrade performance or interfere with subsequent transformations, compilers employ predictive heuristics to guide optimizations by predicting their effects a priori. Unfortunately, the unpredictability of optimization interaction and the irregularity of today's wide-issue machines severely limit the accuracy of these heuristics. As a result, compiler writers may temper high variance optimizations with overly conservative heuristics or may exclude these optimizations entirely. While this process results in a compiler capable of generating good average code quality across the target benchmark set, it is at the cost of missed optimization opportunities in individual code segments. To replace predictive heuristics, researchers have proposed compilers which explore many optimization options, selecting the best one a posteriori. Unfortunately, these existing iterative compilation techniques are not practical for reasons of compile time and applicability. We present the Optimization-Space Exploration (OSE) compiler organization, the first practical iterative compilation strategy applicable to optimizations in general-purpose compilers. Instead of replacing predictive heuristics, OSE uses the compiler writer's knowledge encoded in the heuristics to select a small number of promising optimization alternatives for a given code segment. Compile time is limited by evaluating only these alternatives for hot code segments using a general compile-time performance estimator An OSE-enhanced version of Intel's highly-tuned, aggressively optimizing production compiler for IA-64 yields a significant performance improvement, more than 20% in some cases, on Itanium for SPEC codes. © 2003 IEEE. . Keywords: Benchmarking; Codes (symbols); Computer architecture; Computer resource management; Computer science; Cosine transforms; Cost functions; Degradation; Iterative methods; Network components; Optimization; Production; Space research; Micro architectures; Optimizing compilers; Parallel processing; Performance Gain; Resource management; Program compilers
2003-01-01 - Data Remapping for Design Space Optimization of Embedded Memory Systems	In this article, we present a novel linear time algorithm for data remapping, that is, (i) lightweight; (ii) fully automated; and (iii) applicable in the context of pointer-centric programming languages with dynamic memory allocation support. All previous work in this area lacks one or more of these features.We proceed to demonstrate a novel application of this algorithm as a key step in optimizing the design of an embedded memory system. Specifically, we show that by virtue of locality enhancements via data remapping, we may reduce the memory subsystem needs of an application by 50%, and hence concomitantly reduce the associated costs in terms of size, power, and dollar-investment (61%). Such a reduction overcomes key hurdles in designing high-performance embedded computing solutions. Namely, memory subsystems are very desirable from a performance standpoint, but their costs have often limited their use in embedded systems. Thus, our innovative approach offers the intriguing possibility of compilers playing a significant role in exploring and optimizing the design space of a memory subsystem for an embedded design. To this end and in order to properly leverage the improvements afforded by a compiler optimization, we identify a range of measures for quantifying the cost-impact of popular notions of locality, prefetching, regularity of memory access, and others. The proposed methodology will become increasingly important, especially as the needs for application specific embedded architectures become prevalent. In addition, we demonstrate the wide applicability of data remapping using several existing microprocessors, such as the Pentium and UltraSparc. Namely, we show that remapping can achieve a performance improvement of 20% on the average. Similarly, for a parametric research HPL-PD microprocessor, which characterizes the new Itanium machines, we achieve a performance improvement of 28% on average. All of our results are achieved using applications from the DIS, Olden and SPEC2000 suites of integer and floating point benchmarks. Copyright © 2003, ACM. All rights reserved. . Keywords:
2003-01-01 - Dragon: An Open64-Based Interactive Program Analysis Tool for Large Applications	A program analysis tool can play an important role in helping users understand and improve large application codes. Dragon is a robust interactive program analysis tool based on the Open64 compiler, which is an open source C/C++/Fortran77/90 compiler for Intel Itanium systems. We designed and developed the Dragon analysis tool to support manual optimization and parallelization of large applications by exploiting the powerful analyses of the Open64 compiler. Dragon enables users to visualize and print the essential program structure of and obtain information on their large applications. Current features include the call graph, flow graph, and data dependences. On-going work extends both Open64 and Dragon by a new call graph construction algorithm and its related interprocedural analysis, global variable definition and usage analysis, and an external interface that can be used by other tools such as profilers and debuggers to share program analysis information. Future work includes supporting the creation and optimization of shared memory parallel programs written using OpenMP. . Keywords: C (programming language); Codes (symbols); Computation theory; Data flow analysis; Graph theory; Optimization; Program compilers; Trees (mathematics); Callgraph; Data dependences; Open64 compiler; Interactive computer systems
2003-01-01 - Efficient resource management during instruction scheduling for the EPIC architectures	Effective and efficient modelling and management of hardware resources have always been critical toward generating highly efficient code in optimizing compilers. The instruction templates and dispersal rules of the EPIC architecture add new complexity in managing resource constraints to instruction scheduler. We extended a finite state automaton (FSA) approach to efficiently manage all key resource constraints of an EPIC architecture on-the-fly during instruction scheduling. We have fully integrated the FSA-based resource management into the instruction scheduler in the Open Research Compiler for the EPIC architecture. Our integrated approach shows up to 12% speedup on some SPECint2000 benchmarks and 4.5% speedup on average for all SPECint2000 benchmarks on an Itanium machine when compares to an instruction scheduler with decoupled resource management. In the meantime, the instruction scheduling time of our approach is reduced by 4% on average. © 2003 IEEE. . Keywords: Architecture; Flow measurement; Natural resources management; Parallel architectures; Program compilers; Resource allocation; Hardware resources; Instruction scheduler; Instruction scheduling; Integrated approach; Managing resources; Open research compilers; Optimizing compilers; Resource management; Scheduling
2003-01-01 - Eliminating exception constraints of java programs for IA-64	Java exception checks are designed to ensure that any faulting instruction causing a hardware exception does not terminate the program abnormally. These checks, however, impose some constraints upon the execution order between an instruction potentially raising a Java exception and a faulting instruction causing a hardware exception. This reduces the effectiveness of instruction reordering optimizations for Java programs. Although some effective techniques have been proposed to eliminate exception checks, many exception checks still remain. We propose a new framework to perform speculation effectively for Java programs using a direct acyclic graph representation (DAG) based on the SSA form. Using this framework, we apply a well-known speculation technique to a faulting load instruction to eliminate such constraints. We use edges to represent exception constraints. This allows us to estimate the potential reduction of the critical path length accurately for applying speculation. The new framework also allows us to avoid extra copy instructions and to generate efficient code with minimum register pressure. We have implemented it in the IBM Java Just-In-Time compiler, and observed performance improvements of 30% for one of the micro-benchmark programs, up to 13% (with an average of 1.7%) for the Java Grande Benchmark Suite, and up to 12% (with an average of 1.2%) for SPECjvm98 on an Itanium processor. Our speculation technique was particularly effective for those programs that access multidimensional arrays. . Keywords: Binary codes; Computer aided design; Computer hardware; Computer simulation; Computer software; Constraint theory; Java programming language; Mathematical models; Program compilers; Directed acyclic graph (DAG); IBM (CO); Just-In-Time (JIT); Potential excepting instruction (PEI); Static single assignment (SAS); Parallel processing systems
2003-01-01 - Formal verification of square root algorithms	We discuss the formal verification of some low-level mathematical software for the Intel® Itanium® architecture. A number of important algorithms have been proven correct using the HOL Light theorem prover. After briefly surveying some of our formal verification work, we discuss in more detail the verification of a square root algorithm, which helps to illustrate why some features of HOL Light, in particular programmability, make it especially suitable for these applications. . Keywords: Algorithms; Computer architecture; Computer software; Digital arithmetic; Formal logic; Program compilers; Theorem proving; Automated theorem proving; Formal verification; Square root algorithms; Computer hardware description languages
2003-01-01 - IA-32 execution layer: A two-phase dynamic translator designed to support IA-32 applications on Itanium®-based systems	IA-32 execution layer (IA-32 EL) is a new technology that executes IA-32 applications on Intel® Itanium® processor family systems. Currently, support for IA-32 applications on Itanium-based platforms is achieved using hardware circuitry on the Itanium processors. This capability will be enhanced with IA-32 EL - software that will ship with Itanium-based operating systems and will convert IA-32 instructions into Itanium instructions via dynamic translation. In this paper, we describe aspects of the IA-32 execution layer technology, including the general two-phase translation architecture and the usage of a single translator for multiple operating systems. The paper provides details of some of the technical challenges such as precise exception, emulation of FP, MMX™, and Intel® streaming SIMD extension instructions, and misalignment handling. Finally, the paper presents some performance results. © 2003 IEEE. . Keywords: Dynamic translation; Itanium; Itanium processor; Micro architectures; Multiple operating systems; Streaming SIMD Extensions; Technical challenges; Two phase; Computer architecture
2003-01-01 - Inlining of mathematical functions in HP-UX for Itanium® 2	HP-UX compilers inline mathematical functions for Itanium processor family (IPF) systems to improve throughput 4X-8X versus external library calls, achieving speeds comparable to highly tuned vector functions, without requiring the user to code for a vector interface and without sacrificing accuracy or edge-case behaviors. This paper highlights IPF architectural features that support implementation of high-performance, high-quality mathematics functions for inlining. It discusses strategies for utilizing the features and developing inlineable sequences on a large scale, and it presents requisite compiler features and language extensions. Also, this paper describes compiler mechanisms that produce inlineable code and inline it. © 2003 IEEE. . Keywords: Application programs; Assembly; Codes (symbols); Computational linguistics; Computer programming; Computer programming languages; Computer systems programming; Functional programming; Functions; Large scale systems; Libraries; Network components; Pipelines; Standards; Throughput; Architectural features; Delay; High quality; IEC standards; Itanium processor family; Language extensions; Mathematical functions; Vector functions; Program compilers
2003-01-01 - Integrated prepass scheduling for a Java just-in-time compiler on the IA-64	We present a new integrated prepass scheduling (IPS) algorithm for a Java just-in-time (JIT) compiler which integrates register minimization into list scheduling. We use backtracking in the list scheduling when we have used up all the available registers. To reduce the overhead of backtracking, we incrementally maintain a set of candidate instructions for undoing scheduling. To maximize the ILP after undoing scheduling, we select an instruction chain with the smallest increase in the total execution time. We implemented our new algorithm in a production-level Java JIT compiler for the Intel Itanium processor. The experiment showed that, compared to the best known algorithm by Govindarajan et al., our IPS algorithm improved the performance by up to +1.8% while it reduced the compilation time for IPS by 58% on average. © 2003 IEEE. . Keywords: Algorithms; Just in time production; Network components; Scheduling; Best-known algorithms; Itanium processor; Java; Just in time compilers; Just-in-time compiler; List-scheduling; Optimizing compilers; Production level; Program compilers
2003-01-01 - Integrating high-level optimizations in a production compiler: Design and implementation experience	The High-Level Optimizer (HLO) is a key part of the compiler technology that enabled Itanium™ and Itanium™2 processors deliver leading floating-point performance at their introduction. In this paper, we discuss the design and implementation experience in integrating diverse optimizations in the HLO module. In particular, we describe decisions made in the design of HLO targeting Itanium processor family. We provide empirical data to validate the design decisions. Since HLO was implemented in a production compiler, we made certain engineering trade-offs. We discuss these trade-offs and outline key learning derived from our experience. © Springer-Verlag Berlin Heidelberg 2003. . Keywords: Commerce; Design; Digital arithmetic; Economic and social effects; Integrated circuit design; Compiler technology; Design and implementations; Design decisions; Empirical data; Floating points; High-level optimizations; Itanium processor family; Optimizers; Program compilers
2003-01-01 - Intel® Itanium® floating-point architecture	The Intel® Itanium® architecture is increasingly becoming one of the major processor architectures present in the market today. Launched in 2001, the Intel Itanium processor was followed in 2002 by the Itanium 2 processor, with increased integer and floating-point performance. Measured by the SPEC CINT2000 benchmarks, the Itanium 2 processor still trails by about 25% the Intel P4 processor in integer performance, albeit P4 runs at more than three times Itanium's clock frequency. However, its floating-point performance clearly leads in the SPEC CFP2000 charts, and its rating is about 25% higher than that of the P4 processor. While the general features of the Itanium architecture such as large register sets, predication, speculation, and support for explicit parallelism 1 have been presented in several papers, books, and mainstream college textbooks 2, its floating-point architecture has been less publicized. Two books, 3 and 4, cover well this area. The present paper focuses on the floating-point architecture of the Itanium processor family, and points out a few remarkable features suitable to be the focus of a lecture, lab session, or project in a computer architecture class. . Keywords: Computers; Digital arithmetic; Nanotechnology; Clock frequency; Itanium; Itanium 2 processor; Itanium processor; Itanium processor family; Processor architectures; Register sets; Computer architecture
2003-01-01 - Inter-Procedural Stacked Register Allocation for Itanium® Like Architecture	A hardware managed register stack, Register Stack Engine (RSE), is implemented in Itanium® architecture to provide a unified and flexible register structure to software. The compiler allocates each procedure a register stack frame with its size explicitly specified using an alloc instruction. When the total number of registers used by the procedures on the call stack exceeds the number of physical registers, RSE performs automatically register overflows and fills to ensure that the current procedure has its requested registers available. The virtual register stack frames and RSE alleviate the need of explicit spills by the compiler, but our experimental results indicate that a trade-off exists between using stacked registers and explicit spills under high register pressure due to the uneven cost between them. In this work, we introduce the stacked register quota assignment problem based on the observation that reducing stacked register usage in some procedures could reduce the total memory access time of spilling registers, which includes the time caused by the loads/stores due to explicit register spills and RSE overflows/fills. We propose a new inter-procedural algorithm to solve the problem by allocating stacked registers across procedures based on a quantitative cost model. The results show that our approach can improve performance significantly for the programs with high RSE overflow cost, e.g. perlbmk and crafty, improved by 14% and 3.7%, respectively. . Keywords: Algorithms; Computer architecture; Computer hardware; Computer programming languages; Optimization; Program compilers; Hot region; Hotspot; Inter-procedural stacked register allocation; Quota assignment; Register allocation; Storage allocation (computer)
2003-01-01 - IPF Linux feature enhancements for TX7	Linux is one of the operating systems which can run on NEC IPF servers such as Express5800/ 1160 and the TX7/i9000 series. These large servers have characteristics such as ccNUMA (Cache Coherent Non Uniform Memory Access); large system configuration, e.g., huge number of I/O devices; reliability and serviceability features, e.g., demand for detailed analysis of a system crash; etc. As Linux is nonproprietary and is being developed as open source under the GNU Public License, we decided to cooperate with the Linux community in order to enhance the operating system for the requirements of large servers such as Express5800/1160 or the TX7/i9000 series. In this paper, we describe the enhancements and distinctive features of the NEC Linux operating system for IPF. . Keywords: Data storage equipment; Interfaces (computer); Program processors; Servers; Memory affinity; Computer operating systems
2003-01-01 - Itanium 2 processor microarchitecture	The microarchitecture of itanium 2 processor and its processing capabilities is discussed. Executing upto six instructions at a time, it provides both performance and binary compatibility for itanium based applications and operating systems. The architecture defines flexible memory management schemes and several tools that compilers can use to realize performance. The front end structures fetch instructions for later use by the back end. . Keywords: Cache memory; Computer architecture; Computer operating systems; Data storage equipment; Program compilers; Binary compatibility; Microprocessor chips
2003-01-01 - Itanium2 32-way server system architecture	The Itanium2 8-way, 16-way, and 32-way server - TX7/i9000 and i6000 series has NEC's original chipset that enables very high-speed memory accesses throughout the system. In this paper, the Itanium2 32-way server - TX7/i9510 system architecture is described; the 8-way and 16-way server has the same system architecture as the 32-way TX7/i9510. The chipset presents a flat 32-way system image to the software. The TX7/i9510 supports partitioning for increased flexibility. The cell and the PCI unit can assign to each partition flexibly. The hardware components such as the service processor can have redundant configuration. With the combination of the TX7/i9510 chipset, the integrated Service Processor and Machine Check Architecture (MCA) of Itanium Processor Family (IPF) architecture, the TX7/i9510 realizes a new level of open system Reliability, Availability, Serviceability (RAS). The TX7/i9510 hardware also supports the cell hot plug feature and PCI unit hot plug feature. This paper describes overall system architecture of the TX7/i9510. . Keywords: Buffer storage; Computer architecture; Data storage equipment; Servers; Distributed shared memory (DSM); Parallel processing systems
2003-01-01 - Optimization for the Intel® Itanium® architecture register stack	The Intel® Itanium® architecture contains a number of innovative compiler-controllable features designed to exploit instruction level parallelism. New code generation and optimization techniques are critical to the application of these features to improve processor performance. For instance, the Itanium® architecture provides a compiler-controllable virtual register stack to reduce the penalty of memory accesses associated with procedure calls. The Itanium® Register Stack Engine (RSE) transparently manages the register stack and saves and restores physical registers to and from memory as needed. Existing code generation techniques for the register stack aggressively allocate virtual registers without regard to the register pressure on different control-flow paths. As such, applications with large data sets may stress the RSE, and cause substantial execution delays due to the high number of register saves and restores. Since the Itanium® architecture is developed around Explicitly Parallel Instruction Computing (EPIC) concepts, solutions to increasing the register stack efficiency favor code generation techniques rather than hardware approaches. © 2003 IEEE. . Keywords: Codes (symbols); Computer aided instruction; Concurrency control; Engines; Memory architecture; Network components; Parallel architectures; Pressure control; Program compilers; Program processors; Restoration; Stresses; Concurrent computing; Delay; Memory management; Parallel processing; Registers; Computer architecture
2003-01-01 - Optimizations to prevent cache penalties for the Intel® Itanium® 2 processor	This paper describes scheduling optimizations in the Intel® Itanium® compiler to prevent cache penalties due to various micro-architectural effects on the Itanium 2 processor. This paper does not try to improve cache hit rates but to avoid penalties, which probably all processors have in one form or another, even in the case of cache hits. These optimizations make use of sophisticated methods for disambiguation of memory references, and this paper examines the performance improvement obtained by integrating these methods into the cache optimizations. © 2003 IEEE. . Keywords: Abstracting; Computer hardware; Data structures; Network components; Scheduling; Added delay; Educational institutions; Micro architectures; Optimization method; Optimizing compilers; Prefetching; Processor scheduling; Program compilers
2003-01-01 - Optimized compiler for Intel® Itanium® processor family and compiler enhancements from NEC	NEC has released compilers for the Intel®‡ Itanium®‡ Processor family. NEC licensed the Intel® Compilers and has added a performance analysis feature. This paper first describes the baseline compilers optimization features of Intel Compilers. Then the performance analysis function developed by NEC is explained in detail, including a discussion of its implementation and examples to show its effectiveness. Finally, the future direction of compiler development is stated. . Keywords: Computer software; Optimization; Parallel processing systems; Performance; Performance tuning; Program compilers
2003-01-01 - Overcoming static register pressure for software pipelining in the itanium architecture	Software pipelining techniques have been shown to significantly improve the performance of loop-intensive scientific programs. The Itanium architecture contains many features to enhance parallel execution, such as support for efficient software pipelining of loops. The drawback of software pipelining is the high register requirements, which may lead to software pipelining failure due to limited static general registers in Itanium. This paper evaluates the register requirements of software-pipelined loops. It then presents a novel register allocation scheme, which allocates stacked registers to serve as static registers. Experimental results show that this method gains an average 2.4% improvement, and a peak 18% improvement in performance on NAS Benchmarks. © Springer-Verlag Berlin Heidelberg 2003. . Keywords: Benchmarking; Itanium; NAS benchmarks; Parallel executions; Register allocation; Register requirement; Scientific programs; Software pipelining; Static register; Computer architecture
2003-01-01 - Parallel blocked sparse matrix-vector multiplication with dynamic parameter selection method	A blocking method is a popular optimization technique for sparse matrix-vector multiplication (SpMxV). In this paper, a new blocking method which generalizes the conventional two blocking methods and its application to the parallel environment are proposed. This paper also proposes a dynamic parameter selection method for blocked parallel SpMxV which automatically selects the parameter set according to the characteristics of the target matrix and machine in order to achieve high performance on various computational environments. The performance with dynamically selected parameter set is compared with the performance with generally-used fixed parameter sets for 12 types of sparse matrices on four parallel machines: including PentiumIII, Sparc II, MIPS R12000 and Itanium. The result shows that the performance with dynamically selected parameter set is the best in most cases. © Springer-Verlag Berlin Heidelberg 2003. . Keywords: Artificial intelligence; Computers; Computational environments; Dynamic parameters; Fixed parameter sets; ITS applications; Optimization techniques; Parallel environment; Parallel machine; Sparse matrix-vector multiplication; Matrix algebra
2003-01-01 - Performance potentials of compiler-directed data speculation	Compiler-directed data speculation has been implemented on Itanium systems to allow for a compiler to move a load across a store even when the two operations are potentially aliased This not only breaks data dependency to reduce critical path length, but also allows a load to be scheduled far apart from its uses to hide cache miss latencies. However, the effectiveness of data speculation is affected by the sophistication of alias analysis technique as well as the aggressiveness of the instruction scheduler. In general, the more sophisticated is the alias analysis technique, the less performance gain is from data speculation, and the more aggressive is the instruction scheduler, the more opportunity is for data speculation. In this paper we evaluate in various scenarios the performance potentials of data speculation for SPEC2000C benchmarks. For each scenario, we determine the performance contributions of data speculation due to both critical path reduction and cache miss latency reduction. We also show interesting statistics about the effects of scheduling constraints, the percentage of critical dependencies, the impacts of cache miss latencies, and the distances between the load locations before and after data speculation. © 2003 IEEE. . Keywords: Benchmarking; Distributed computer systems; Program compilers; Scheduling; Statistics; Delay; Educational institutions; Performance analysis; Performance evaluation; Performance Gain; Processor scheduling; Runtimes; Data reduction
2003-01-01 - Procedure cloning and integration for converting parallelism from coarse to fine grain	This paper introduces a method for improving program run-time performance by gathering work in an application and executing it efficiently in an integrated thread. Our methods extend whole-program optimization by expanding the scope of the compiler through a combination of software thread integration and procedure cloning. In each experiment we integrate a frequently executed procedure with itself twice or thrice, creating two clones. Then, based on profile data we select at compile time the fastest version (original or clone) and modify call sites as needed. We demonstrate our technique by cloning and integrating three procedures from cjpeg and djpeg at the C source code level, compiling with four compilers for the Itanium EPIC architecture and measuring the performance with the on-chip performance measurement units. For cjpeg, which is not significantly constrained by the i-cache, we find integration consistently improves code generated by all compilers but one, with a mean program speedup of 11.99%. © 2003 IEEE. . Keywords: Application programs; C (programming language); Cloning; Computer architecture; Data reduction; Embedded systems; Genetic engineering; Integration; Measurements; Program processors; Units of measurement; Yarn; Application softwares; EPIC architecture; Optimizing compilers; Parallel processing; Program optimization; Run-time performance; Runtimes; Software thread integration; Program compilers
2003-01-01 - Proceedings - Working Conference on Reverse Engineering, WCRE	The proceedings contain 38 papers. The topics discussed include: unscheduling, unpredication, unspeculation: reverse engineering Itanium executables; extracting an explicitly data-parallel representation of image-processing programs; hierarchical reflexion models; moving towards quality attribute driven software architecture reconstruction; a comparative evaluation of dynamic visualization tools; analyzing and relating bug report data for feature tracking; reverse engineering the process of small novice software teams; fuzzy extensions for reverse engineering repository models; studying the chaos of code development; identification of software instabilities; and migration of non-decomposable software system to the web using screen proxies. . Keywords:
2003-01-01 - Proceedings of the International Workshop on High-Level Programming Models and Supportive Environments, HIPS	The proceedings contain 9 papers. The topics discussed include: supporting peer-2-peer interactions in the consumer grid; DPS - dynamic parallel schedules; ParoC++: a requirement-driven parallel object-oriented programming language; on the implementation of JavaSymphony; compiler and runtime support for running OpenMP programs on Pentium- and Itanium-architectures; SMP-aware message passing programming; a comparison between MPI and OpenMP branch-and-bound skeletons; a comparison between MPI and OpenMP branch-and-bound skeletons; and algorithmic concept recognition support for skeleton based parallel programming. . Keywords:
2003-01-01 - Region-based compilation framework	The traditional framework for back-end compilation is based on the scope of functions, which is a convenient way to partition an entire application program. However, the sizes and structures of functions may not be the best scope for program analysis and transformations when considering compilation resources (such as time and memory usage), performance, and functionality. This problem is particularly critical when the modern compilers resort to sophisticated and expensive algorithms to exploit instruction-level parallelism. When a function has a large size, an expensive algorithm based on the function scope often makes the usage of compilation time and space usage unacceptable. Some earlier work proposed a compilation framework to allow some control on the size and structure of an optimization scope. This paper proposes a new region-based compilation framework driven by the considerations of compilation resources and performance opportunities. And some optimization-directed attributes can be passed and observed from optimization phase to phase on a region basis. This region-based framework is being implemented in a compiler ORC (Open Research Compiler) targeting at the Itanium Processor Family (IPF). . Keywords: Optimization; Duplication ratio; Main exit probability; Region; Program compilers
2003-01-01 - The fallacy of spec-based design	The architectural reference model, a critical tool in microprocessor validation, serves as a gold standard against which a microprocessor is compared. As all validation is performed using it, the reference model must be timely, extensible beyond the original specification, customizable for specific usage models, and, most importantly, functionally flawless. Ideally, we would like a specification-based design flow that transforms the published English language specifications into compilable code that is directly incorporated into the reference model without loss of information or accuracy. At Intel, we have developed and implemented such a flow to create the Itanium&#174; Processor Family&#39;s (IPF) reference model, which is used to validate all IPF processors. In this paper, we describe the benefits and limitations of this process and discuss the practical implications of specification-based design. © 2003 IEEE. . Keywords: Design; Models; Specifications; Formal methods; Microprocessor chips; Software engineering; Specifications; Customizable; Design flows; English languages; Gold standards; Itanium; Reference modeling; Usage models; Integrated circuit design
2003-01-01 - The performance of runtime data cache prefetching in a dynamic optimization system	Traditional software controlled data cache prefetching is often ineffective due to the lack of runtime cache miss and miss address information. To overcome this limitation, we implement runtime data cache prefetching in the dynamic optimization system ADORE (ADaptive Object code Reoptimization). Its performance has been compared with static software prefetching on the SPEC2000 benchmark suite. Runtime cache prefetching shows better performance. On an Itanium 2 based Linux workstation, it can increase performance by more than 20% over static prefetching on some benchmarks. For benchmarks that do not benefit from prefetching, the runtime optimization system adds only 1%-2% overhead. We have also collected cache miss profiles to guide static data cache prefetching in the ORC compiler. With that information the compiler can effectively avoid generating prefetches for loops that hit well in the data cache. © 2003 IEEE. . Keywords: Application programs; Cache memory; Computer architecture; Computer operating systems; Computer programming; Computer science; Microprocessor chips; Professional aspects; Program compilers; Cities and towns; Data engineering; Optimizing compilers; Prefetching; Programming profession; Runtimes; Software performance; Benchmarking
2003-01-01 - Unscheduling, Unpredication, Unspeculation: Reverse Engineering Itanium Executables	EPIC (Explicitly Parallel Instruction Computing) architectures, exemplified by the Intel Itanium, support a number of advanced architectural features, such as explicit instruction-level parallelism, instruction predication, and speculative loads from memory. However, compiler optimizations to take advantage of such architectural features can profoundly restructure the program's code, making it potentially difficult to reconstruct the original program logic from an optimized Itanium executable. This paper describes techniques to undo some of the effects of such optimizations and thereby improve the quality of reverse engineering such executables. . Keywords: Algorithms; Benchmarking; Binary codes; Computational complexity; Computer aided instruction; Computer architecture; Data storage equipment; Logic programming; Optimization; Parallel processing systems; Program compilers; Scheduling; Software engineering; Explicitly parallel instruction computing (EPIC); Flow graph complexity; Instruction scheduling; Reverse engineering
2003-01-01 - Unscheduling, unpredication, unspeculation: Reverse engineering Itanium executables	EPIC (Explicitly Parallel Instruction Computing) architectures, exemplified by the Intel Itanium, support a number of advanced architectural features, such as explicit instruction-level parallelism, instruction predication, and speculative loads from memory. However, compiler optimizations to take advantage of such architectural features can profoundly restructure the program's code, making it potentially difficult to reconstruct the original program logic from an optimized Itanium executable. This paper describes techniques to undo some of the effects of such optimizations and thereby improve the quality of reverse engineering such executables. © 2003 IEEE. . Keywords: Computation theory; Computer architecture; Computer science; Microprocessor chips; Parallel architectures; Pipelines; Program compilers; Program processors; Reverse engineering; Concurrent computing; Delay; Logic; Optimizing compilers; Parallel processing; Computer aided instruction
2003-01-01 - Unspeculation	Modern architectures, such as the Intel Itanium, support speculation, a hardware mechanism that allows the early execution of expensive operations-possibly even before it is known whether the results of the operation are needed. While such speculative execution can improve execution performance considerably, it requires a significant amount of complex support code to deal with and recover from speculation failures. This greatly complicates the tasks of understanding and re-engineering speculative code. This paper describes a technique for removing speculative instructions from optimized binary programs in a way that is guaranteed to preserve program semantics, thereby making the resulting “unspeculated” programs easier to understand and more amenable to re-engineering using traditional reverse engineering techniques. © 2003 IEEE . Keywords: Reverse engineering; Semantics; Binary programs; Complex support; Execution performance; Hardware mechanism; Itanium; Modern architectures; Program semantics; Re-engineering; Reverse engineering techniques; Speculative execution; Reengineering
2003-01-01 - Workshop on Computer Architecture Education, WCAE '03, Held in conjunction with the 30th International Symposium on Computer Architecture	The proceedings contain 15 papers. The topics discussed include: Intel Itanium floating-point architecture; a CPU core for teaching basics of computer architecture; superscalar out-of-order demystified in four instructions; bridging the gap between undergraduate and graduate experience in computer systems studies; integration of computer security laboratories into computer architecture courses to enhance undergraduate curriculum; combining learning strategies in a first course in computer architecture; building resources for teaching computer architecture through electronic peer review; laboratory options for the computer science major; activating computer architecture with classroom presenter; the Liberty simulation environment as a pedagogical tool; multimedia components for the visualization of dynamic behavior in computer architectures; didactic architectures and simulator for network processor learning; and use of HDLs in teaching of computer hardware courses. . Keywords:
2004-01-01 - A 0.13μm triple-Vt 9MB third level on-die cache for the Itanium® 2 processor	The 18-way set-associative, single-ported 9MB cache for the Itanium®2 Processor uses 210 identical 48kB sub-arrays with a 2.21μm 2 cell in a 0.13μm 6M technology. A staged mode ECC scheme avoids a latency increase in the L3 Tag. A high Vt implant improves the read stability and reduces the sub-threshold leakage. . Keywords: Bandwidth; Buffer storage; CMOS integrated circuits; Data transfer; Dielectric materials; Electric network analysis; Field effect transistors; Routers; Cache data arrays; Routing; Subarrays; Program processors
2004-01-01 - A compiler framework for recovery code generation in general speculative optimizations	A general framework that integrates both control and data speculation using alias profiling and/or compiler heuristic rules has shown to improve SPEC2000 performance on Itanium systems. However, speculative optimizations require check instructions and recovery code to ensure correct execution when speculation fails at runtime. How to generate check instructions and their associated recovery code efficiently and effectively is an issue yet to be well studied. Also, it is very important that the recovery code generated in the earlier phases integrate gracefully in the later optimization phases. At the very least, it should not hinder later optimizations, thus, ensuring overall performance improvement. This paper proposes a framework that uses an if-block structure to facilitate check instructions and recovery code generation for general speculative optimizations. It allows speculative instructions and their recovery code generated in the early compiler optimization phases to be integrated effectively with the subsequent optimization phases. It also allows multi-level speculation for multi-level pointers and multi-level expression trees to be handled with no additional complexity. The proposed recovery code generation framework has been implemented in the Open Research Compiler (ORC). . Keywords: Algorithms; Codes (symbols); Mathematical models; Optimization; Sustainable development; Code generation; Compiler optimization; Multi-level pointers; Open research compiler (ORC); Program compilers
2004-01-01 - A multi-platform Co-array Fortran compiler	Co-array Fortran (CAF) - a small set of extensions to Fortran 90 - is an emerging model for scalable, global address space parallel programming. CAF's global address space programming model simplifies the development of single-program-multiple-data parallel programs by shifting the burden for managing the details of communication from developers to compilers. This paper describes cafe - a prototype implementation of an open-source, multiplatform CAF compiler that generates code well-suited for today's commodity clusters. The cafc compiler translates CAF into Fortran 90 plus calls to one-sided communication primitives. The paper describes key details of cafc's approach to generating efficient code for multiple platforms. Experiments compare the performance of CAF and MPI versions of several NAS parallel benchmarks on an Alpha cluster with a Quadrics interconnect, an Itanium 2 cluster with a Myrinet 2000 interconnect and an Itanium 2 cluster with a Quadrics interconnect. These experiments show that cafc compiles CAF programs into code that delivers performance roughly equal to that of hand-optimized MPI programs. . Keywords: Arrays; Computer programming languages; Data transfer; Mathematical models; Matrix algebra; Parallel processing systems; Vectors; Communication layers; Data layout; Data mapping; Memory management; Program compilers
2004-01-01 - A real time MPEG-4 parallel encoder on software distributed shared memory systems	This paper is dedicated to developing a real-time MEPG-4 parallel encoder on software distributed shared memory systems. Basically, the performance of a MPEG-4 parallel encoder implemented on distributed systems is mainly determined by the latency of data synchronization and disk I/O, and the cost of data computation. For reducing the impact of data synchronization latency, we invent a pipeline algorithm to minimize the number of data synchronization points necessary for video encoding. In addition, we employ a master-slave node structure to overlay computation and I/O in order for alleviating the impact of I/O latency. On the other hand, we propose a two-level partitioning method to minimize the cost of data computation, and overlap the encoding times of two different GOVs. We have implemented the proposed MPEG-4 encoder on a test bed called Teamster. The experimental results show the proposed MPEG-4 encoder has successfully met the requirement of real time through the support of previous techniques via 32 SMP machines, which are equipped with dual 1.5 GHz Itanium II processors per node and connected by Gigabit Ethernet. © Springer-Verlag Berlin Heidelberg 2004. . Keywords: Encoding (symbols); Memory architecture; Motion Picture Experts Group standards; Real time systems; Synchronization; Video signal processing; Signal encoding; Data computation; Data synchronization; Disk I/O; Distributed systems; Number of datum; Performance; Real- time; Software distributed shared-memory systems; Synchronization points; Video encodings; Gigabit Ethernet; Partitioning methods; Software distributed shared memory system; Signal encoding; Distributed computer systems
2004-01-01 - A split at the core W. Wayt Gibbs	The need for the microchip industry to develop new chips with much smaller transistors and much higher clock speeds is discussed. Steadily increasing performance of Intel microprocessors has come mainly from quickening the clock pulse that sets the pace for the device's transistors. Latest generation of Intel processors will use a radically different multicore architecture that may initially run at clock speeds below the current maximum frequency of 3,600 megahertz. The Pentium 4 and Itanium 2 are capable of burning more power than current heat sinks are designed to dissipate. . Keywords: Cache memory; Computer architecture; Investments; Parallel processing systems; Personal computers; Program processors; Sales; Servers; Software engineering; Transistors; Cisco Systems (CO); Intel (CO); Microchip industry; Opteron processors; Microprocessor chips
2004-01-01 - Agile methods applied to embedded firmware development	This paper describes the experience of applying Agile approaches to the development of firmware for the Intel® Itanium® processor family. Embedded development (i.e. firmware) projects are quite different from object-oriented and pure software endeavors, yet they face many of the same challenges that Agile software development practices address. Several unique challenges are described, including team members' specialized domain knowledge, technical backgrounds and attitudes toward change, and the impact hardware plays in firmware design. We found Agile approaches to be well-suited for our project, despite the fact that most Agile methodologists come from very different backgrounds. © 2004 IEEE. . Keywords: Embedded systems; Firmware; Knowledge acquisition; Problem solving; Product development; Project management; 64-bit architecture; Agile methods; Embedded firmware development; Run-time services; Agile manufacturing systems
2004-01-01 - An empirical performance analysis of commodity memories in commodity servers	This work details a performance study of six different types of commodity memories in two commodity server nodes. A number of micro-benchmarks are used that measure low-level performance characteristics, as well as two applications representative of the ASC workload. The memories vary both in terms of performance, including latency and bandwidths, and in terms of their physical properties and manufacturer. The two server nodes analyzed were an Itanium-II Madison based system, and a Xeon based system. All memories can be used within both of these processing nodes. This allows the performance of the memories to be directly examined while keeping all other factors within a node the same (processor, motherboard, operating system etc.). The results of this study show that there can be a significant difference in application performance depending on the actual memory used - by as much as 20%. The achieved performance is a result of the integration of the memory into the node as well as how the applications actually utilize it. Copyright 2004 ACM. . Keywords: Computer operating systems; Servers; Application performance; Empirical performance analysis; Itanium; Memory modules; Memory systems; Operating systems; Performance analysis; Performance characteristics; Performance measurements; Performance study; Processing nodes; Server nodes; Benchmarking
2004-01-01 - Applications of storage mapping optimization to register promotion	Storage mapping optimization is a flexible approach to folding array dimensions in numerical codes. It is designed to reduce the memory footprint after a wide spectrum of loop transformations, whether based on uniform dependence vectors or more expressive polyhedral abstractions. Conversely, few loop transformations have been proposed to facilitate register promotion, namely loop fusion, unroll-and-jam or tiling. Building on array data-flow analysis and expansion, we extend storage mapping optimization to improve opportunities for register promotion. Our work is motivated by the empirical study of a computational biology benchmark, the approximate string matching algorithm BPR from NR-grep, on a wide issue micro-architecture. Our experiments confirm the major benefit of register tiling (even on non-numerical benchmarks) but also shed the light on two novel issues: prior array expansion may be necessary to enable loop transformations that finally authorize profitable register promotion, and more advanced scheduling techniques (beyond tiling and unrolland-jam) may significantly improve performance in fine-tuning register usage and instruction-level parallelism. . Keywords: Algorithms; Approximation theory; Codes (symbols); Computer architecture; Computer simulation; Data flow analysis; Optimization; Pattern matching; Scheduling; Vectors; Array contraction; Array folding; Blocking; Itanium; Register promotion; String matching; Tiling; Data storage equipment
2004-01-01 - Applying MPI derived datatypes to the NAS benchmarks: A case study	MPI derived datatypes are a powerful method to define arbitrary collections of non-contiguous data in memory and to enable non-contiguous data communication in a single MPI function call. In this paper, we employ MPI datatypes in four NAS benchmarks (MG, LU, BT, and SP) to transfer non-contiguous data. Comprehensive performance evaluation was carried out on two clusters: an Itanium-2 Myrinet cluster and a Xeon InfiniBand cluster. Performance results show that using datatypes can achieve performance comparable to manual packing/unpacking in the original benchmarks, though the MPI implementations that were studied also perform internal packing and unpacking on non-contiguous datatype communication. In some cases, better performance can be achieved because of the reduced costs to transfer non-contiguous data. This is because some optimizations in the MPI packing/unpacking implementations can be easily overlooked in manual packing and unpacking by users. Our case study demonstrates that MPI datatypes simplify the implementation of non-contiguous communication and lead to application code with portable performance. We expect that with further improvement of datatype processing and datatype communication such as, datatypes can outperform the conventional methods of noncontiguous data communication. Our modified NAS benchmarks can be used to evaluate datatype processing and datatype communication in MPI implementations. . Keywords: Algorithms; Computer programming; Data storage equipment; Data structures; Data transfer; Interfaces (computer); Mathematical models; Parallel processing systems; Datatype communications; Message passing interface (MPI) datatypes; Parallel programming model; Remote memory access (RMA); Data communication systems
2004-01-01 - ChipPower: An architecture-level leakage simulator	Leakage power is projected to be one of the major challenges in future technology generations. The temperature profile, process variation, and transistor count all have strong impact on the leakage power distribution of a processor. We have built a simulator to estimate the dynamic/leakage power for a VLIW architecture considering dynamic temperature feedback and process variation. The framework is based on architecture similar to the Intel Itanium IA64 and is extended to simulate its power when implemented in 65nm technology. Our experimental results show that leakage power will become more than 50% of the power budget in 65nm technology. Moreover, without including the process variation, the total leakage power will be underestimated by as much as 30%. ©2004 IEEE. . Keywords: Capacitance; Computer simulation; Electric inverters; Flip flop circuits; Gates (transistor); Integrated circuit layout; Mathematical models; Microprocessor chips; MOS devices; Simulators; Static random access storage; Thermal effects; Threshold voltage; Gate length; Leakage power; Power breakdown; Process variations; Power electronics
2004-01-01 - Clock Generation and Distribution for the 130-nm Itanium™ 2 Processor With 6-MB On-Die L3 Cache	The clock generation and distribution system for the 130-nm Itanium 2 processor operates at 1.5 GHz with a skew of 24 ps. The Itanium 2 processor features 6 MB of on-die L3 cache and has a die size of 374 mm2. Fuse-based clock de-skew enables post-silicon clock optimization to gain higher frequency. This paper describes the clock generation, global clock distribution, local clocking, and the clock skew optimization feature. . Keywords: Algorithms; Buffer storage; Clocks; CMOS integrated circuits; Electric network topology; Iterative methods; Optimization; Photons; Printed circuit design; Transistors; Clock distribution; Clock generation; Clock skew; De-skew; Fuse-based de-skew; Local clock buffers; Microprocessor chips
2004-01-01 - Co-array fortran performance and potential: An NPB experimental study	Co-array Fortran (CAF) is an emerging model for scalable, global address space parallel programming that consists of a small set of extensions to the Fortran 90 programming language. Compared to MPI, the widely-used message-passing programming model, CAF's global address space programming model simplifies the development of single-program-multiple-data parallel programs by shifting the burden for choreographing and optimizing communication from developers to compilers. This paper describes an open-source, portable, and retargetable CAP compiler under development at Rice University that is well-suited for today's high-performance clusters. Our compiler translates CAF into Fortran 90 plus calls to one-sided communication primitives. Preliminary experiments comparing CAF and MPI versions of several of the NAS parallel benchmarks on an Itanium 2 cluster with a Myrinet 2000 interconnect show that our CAF compiler delivers performance that is roughly equal to or, in many cases, better than that of programs parallelized using MPI, even though support for global optimization of communication has not yet been implemented in our compiler. © Springer-Verlag 2004. . Keywords: Benchmarking; Computational linguistics; Global optimization; Message passing; Open source software; Parallel programming; Program compilers; Room and pillar mining; Co-array Fortran; Global address spaces; High performance cluster; NAS parallel benchmarks; One sided communication; Optimization of communications; Programming models; Single program multiple data; FORTRAN (programming language)
2004-01-01 - Compiler optimizations for transaction processing workloads on itanium® linux systems	This paper discusses a repertoire of well-known and new compiler optimizations that help produce excellent server application performance and investigates their performance contributions. These optimizations combined produce a 40% speed-up in on-line transaction processing (OLTP) performance and have been implemented in the Intel C/C++ Itanium compiler. In particular, the paper presents compiler optimizations that take advantage of the Itanium register stack, proposes an enhanced Linux preemption model and demonstrates their performance potential for server applications. © 2004 IEEE. . Keywords: Computer operating systems; Data structures; Object oriented programming; Online systems; Optimization; Servers; Storage allocation (computer); Computer Operating Systems - Linux; On-line transaction processing (OLTP); Transaction processing; Workloads; Program compilers
2004-01-01 - Compiler orchestrated prefetching via speculation and predication	This paper introduces a compiler-orchestrated prefetching system as a unified framework geared toward ameliorating the gap between processing speeds and memory access latencies. We focus the scope of the optimization on specific subsets of the program dependence graph that succinctly characterize the memory access pattern of both regular array-based applications and irregular pointer-intensive programs. We illustrate how program embedded precomputation via speculative execution can accurately predict and effectively prefetch future memory references with negligible overhead. The proposed techniques reduce the total running time of seven SPEC benchmarks and two OLDEN benchmarks by 27% on an Itanium 2 processor. The improvements are in addition to several state-of-the-art optimizations including software pipelining and data prefetching. In addition, we use cycle-accurate simulations to identify important and lightweight architectural innovations that further mitigate the memory system bottleneck. In particular, we focus on the notoriously challenging class of pointer-chasing applications, and demonstrate how they may benefit from a novel scheme of sentineled prefetching. Our results for twelve SPEC benchmarks demonstrate that 45% of the processor stalls that are caused by the memory system are avoidable. The techniques in this paper can effectively mask long memory latencies with little instruction overhead, and can readily contribute to the performance of processors today. Copyright 2004 ACM. . Keywords: Computation theory; Computer architecture; Embedded systems; Graph theory; Information retrieval systems; Optimization; Precomputation; Predicated execution; Prefetching; Speculation; Program compilers
2004-01-01 - Compiler orchestrated prefetching via speculation and predication	This paper introduces a compiler-orchestrated prefetching system as a unified framework geared toward ameliorating the gap between processing speeds and memory access latencies. We focus the scope of the optimization on specific subsets of the program dependence graph that succinctly characterize the memory access pattern of both regular array-based applications and irregular pointer-intensive programs. We illustrate how program embedded precomputation via speculative execution can accurately predict and effectively prefetch future memory references with negligible overhead. The proposed techniques reduce the total running time of seven SPEC benchmarks and two OLDEN benchmarks by 27% on an Itanium 2 processor. The improvements are in addition to several state-of-the-art optimizations including software pipelining and data prefetching. In addition, we use cycle-accurate simulations to identify important and lightweight architectural innovations that further mitigate the memory system bottleneck. In particular, we focus on the notoriously challenging class of pointer-chasing applications, and demonstrate how they may benefit from a novel scheme of sentineled prefetching. Our results for twelve SPEC benchmarks demonstrate that 45% of the processor stalls that are caused by the memory system are avoidable. The techniques in this paper can effectively mask long memory latencies with little instruction overhead, and can readily contribute to the performance of processors today. . Keywords: Buffer storage; Computer architecture; Optimization; Program processors; Scheduling; Set theory; Simulators; Supervisory and executive programs; Orchestrated prefetching; Precomputation; Predicated execution; Speculation; Program compilers
2004-01-01 - Compiler orchestrated prefetching via speculation and predication	This paper introduces a compiler-orchestrated prefetching system as a unified framework geared toward ameliorating the gap between processing speeds and memory access latencies. We focus the scope of the optimization on specific subsets of the program dependence graph that succinctly characterize the memory access pattern of both regular array-based applications and irregular pointer-intensive programs. We illustrate how program embedded precomputation via speculative execution can accurately predict and effectively prefetch future memory references with negligible overhead. The proposed techniques reduce the total running time of seven SPEC benchmarks and two OLDEN benchmarks by 27% on an Itanium 2 processor. The improvements are in addition to several state-of-the-art optimizations including software pipelining and data prefetching. In addition, we use cycle-accurate simulations to identify important and lightweight architectural innovations that further mitigate the memory system bottleneck. In particular, we focus on the notoriously challenging class of pointer-chasing applications, and demonstrate how they may benefit from a novel scheme of sentineled prefetching. Our results for twelve SPEC benchmarks demonstrate that 45% of the processor stalls that are caused by the memory system are avoidable. The techniques in this paper can effectively mask long memory latencies with little instruction overhead, and can readily contribute to the performance of processors today. . Keywords: Benchmarking; Computational methods; Computer science; Data processing; Microprocessor chips; Optimization; Scheduling; Storage allocation (computer); Precomputation; Predicted execution; Prefetching; Speculation; Program compilers
2004-01-01 - Compiler-assisted cache replacement: Problem formulation and performance evaluation	Recent research results show that conventional hardware-only cache solutions result in unsatisfactory cache utilization for both regular and irregular applications. To overcome this problem, a number of architectures introduce instruction hints to assist cache replacement. For example, Intel Itanium architecture augments memory accessing instructions with cache hints to distinguish data that will be referenced in the near future from the rest. With the availability of such methods, the performance of the underlying cache architecture critically depends on the ability of the compiler to generate code with appropriate cache hints. In this paper we formulate this problem - giving cache hints to memory instructions such that cache miss rate is minimized - as a 0/1 knapsack problem, which can be efficiently solved using a dynamic programming algorithm. The proposed approach has been implemented in our compiler testbed and evaluated on a set of scientific computing benchmarks. Initial results show that our approach is effective on reducing the cache miss rate and improving program performance. © Springer-Verlag; 2004. . Keywords: Combinatorial optimization; Dynamic programming; Memory architecture; Program compilers; 0/1 knapsack problems; Cache architecture; Compiler-assisted; Dynamic programming algorithm; Irregular applications; Problem formulation; Program performance; Recent researches; Cache memory
2004-01-01 - Construction and performance characterization of parallel interior point solver on 4-way intel Itanium 2 multiprocessor system	In recent years the interior point method (IPM) has became a dominant choice for solving large convex optimization problems for many scientific, engineering and commercial applications. Two reasons for the success of the IPM are its good scalability on existing multiprocessor systems with a small number of processors and its potential to deliver a scalable performance on systems with a large number of processors. The scalability of a parallel IPM is determined by several key issues such as exploiting parallelism due to sparsity of the problem, reducing communication overhead and proper load balancing. In this paper we present an implementation of a parallel linear programming IPM workload and characterize its scalability on a 4-way Itanium® 2 system. We show a speedup of up to 3-times for some of the datasets. We also present a detailed micro-architectural analysis of the workload using VTune ™ performance analyzer. Our results suggest that a good IPM implementation is latency-bound. Based on these findings, we make suggestions on how to improve the performance of the IPM workload in the future. © 2004 IEEE. . Keywords: Algorithms; Computer hardware; Linear programming; Optimization; Parallel processing systems; Problem solving; Set theory; Vectors; Interior point method (IPM); Matrix-vector multiplication (MVM); Micro-architectural analysis; Parallel distributed-memory multi-frontal approach; Multiprocessing systems
2004-01-01 - Continuous adaptive object-code re-optimization framework	ynamic optimization presents opportunities for finding run-time bottlenecks and deploying optimizations in statically compiled programs. In this paper, we discuss our current implementation of our hardware sampling based dynamic optimization framework and applying our dynamic optimization system to various SPEC2000 benchmarks compiled with the ORC compiler at optimization level O2 and executed on an Itanium-2 machine. We use our optimization system to apply memory prefetching optimizations, improving the performance of multiple benchmark programs. © Springer-Verlag 2004. . Keywords: Benchmarking; Benchmark programs; Compiled programs; Dynamic optimization; Object code; Optimization framework; Optimization levels; Optimization system; Sampling-based; Embedded systems
2004-01-01 - Data centric cache measurement on the intel itanium 2 processor	Processor speed continues to increase faster than the speed of access to main memory, making effective use of memory caches more important. Information about an application's interaction with the cache is therefore critical to performance tuning. To be most useful, tools that measure this information should relate it to the source code level data structures in an application. We describe how to gather such information by using hardware performance counters to sample cache miss addresses, and present a new tool named Cache Scope that does this using the Intel Itanium 2 performance monitors. We present experimental results concerning Cache Scope's accuracy and perturbation of cache behavior. We also describe a case study of using Cache Scope to tune two applications, achieving 24% and 19% reductions in running time. © 2004 IEEE. . Keywords: Computer applications; Computer hardware; Data structures; Microprocessor chips; Performance; Tuning; Hardware performance counters; Intel (CO); Itanium 2 processor; Source codes; Cache memory
2004-01-01 - Data Centric Cache Measurement on the Intel ltanium 2 Processor	Processor speed continues to increase faster than the speed of access to main memory, making effective use of memory caches more important. Information about an applications interaction with the cache is therefore critical to performance tuning. To be most useful, tools that measure this information should relate it to the source code level data structures in an application. We describe how to gather such information by using hardware performance counters to sample cache miss addresses, and present a new tool named Cache Scope that does this using the Intel Itanium 2 performance monitors. We present experimental results concerning Cache Scopes accuracy and perturbation of cache behavior. We also describe a case study of using Cache Scope to tune two applications, achieving 24% and 19% reductions in running time. © 2004 IEEE. . Keywords: 2 performance; Cache behavior; Cache measurements; Data centric; Hardware performance counters; Performance tuning; Processor speed; Running time; Cache memory
2004-01-01 - Data dependence profiling for speculative optimizations	Data dependence analysis is the foundation to many reordering related compiler optimizations and loop parallelization. Traditional data dependence analysis algorithms are developed primarily for Fortran-like subscripted array variables. They are not very effective for pointer-based references in C or C++. With more advanced hardware support for speculative execution, such as the advanced load instructions in Intel's IA64 architecture, some data dependences with low probability can be speculatively ignored. However, such speculative optimizations must be carefully applied to avoid excessive cost associated with potential mis-speculations. Data dependence profiling is one way to provide probabilistic information on data dependences to guide such speculative optimizations and speculative thread generation. Software-based data dependence profiling requires detailed tracing of memory accesses, therefore, could be very time consuming. In this paper, we examine issues related to data dependence profiling, and propose various techniques to improve the efficiency of data dependence profiling. We use the Open Research Compiler (ORC) [15,16] to test the efficiency of our data profiling techniques. We also study the effectiveness of data dependence profiling on data speculative optimizations on Itanium systems. Our results show that efficient data dependence profiling could improve the performance for data speculative optimizations. © Springer-Verlag 2004. . Keywords: C++ (programming language); Efficiency; Compiler optimizations; Data dependence analysis; Hardware supports; Loop parallelization; Open research compilers; Probabilistic information; Speculative execution; Speculative optimization; Program compilers
2004-01-01 - Digital tomosynthesis mammography using a parallel maximum likelihood reconstruction method	A parallel reconstruction method, based on an iterative maximum likelihood (ML) algorithm, is developed to provide fast reconstruction for digital tomosynthesis mammography. Tomosynthesis mammography acquires 11 low-dose projections of a breast by moving an x-ray tube over a 50° angular range. In parallel reconstruction, each projection is divided into multiple segments along the chest-to-nipple direction. Using the 11 projections, segments located at the same distance from the chest wall are combined to compute a partial reconstruction of the total breast volume. The shape of the partial reconstruction forms a thin slab, angled toward the x-ray source at a projection angle 0°. The reconstruction of the total breast volume is obtained by merging the partial reconstructions. The overlap region between neighboring partial reconstructions and neighboring projection segments is utilized to compensate for the incomplete data at the boundary locations present in the partial reconstructions. A serial execution of the reconstruction is compared to a parallel implementation, using clinical data. The serial code was run on a PC with a single PentiumIV 2.2GHz CPU. The parallel implementation was developed using MPI and run on a 64-node Linux cluster using 800MHz Itanium CPUs. The serial reconstruction for a medium-sized breast (5cm thickness, 11cm chest-to-nipple distance) takes 115 minutes, while a parallel implementation takes only 3.5 minutes. The reconstruction time for a larger breast using a serial implementation takes 187 minutes, while a parallel implementation takes 6.5 minutes. No significant differences were observed between the reconstructions produced by the serial and parallel implementations. . Keywords: Data reduction; Digital image storage; Image reconstruction; Iterative methods; Medical imaging; Projection systems; Tissue; X ray analysis; 3-D reconstruction; Breast imaging; Iterative reconstruction; Parallel computation; Tomosynthesis; Mammography
2004-01-01 - Early performance results on the NRL SGI Altix 3000 computer	Since December 2002 the Naval Research Laboratory (NRL) has been evaluating an Altix 3000, the newest high performance computing system available from Silicon Graphics, Inc. (SGI). The Altix is a departure from the previous products from SGI in that instead of using MIPS processors and the IRIX operating system, the Altix uses the Intel Itanium IA-64 processor and SGI ProPack (based on Linux Red Hat) operating system. The Altix still has the brick concept of system configuration and the SGI NUMAlink for the inter-module network for the shared memory. The Altix runs under a single image of the operating system and supports parallel programming through OpenMP, MPI, CoArray, FORTRAN, and an automatic parallelizing compiler. Various codes have been evaluated with respect to their ease of portability and their performance on the Altix as compared to other high performance computers. . Keywords: Codes (symbols); Computer networks; Computer operating systems; Parallel processing systems; Program compilers; Program processors; Altix 3000 computers; High performance computers; Parallizing compiler; Silicon graphics, Inc (CO); Computers
2004-01-01 - Effect of optimizations on performance of openMP programs	In this paper, we describe several compiler optimization techniques and their effect on the performance of OpenMP programs. We elaborate on the major design considerations in a high performance OpenMP compiler and present experimental data based on the implementation of the optimizations in the Intel® C++ and Fortran compilers for Intel platforms. Interactions of the OpenMP translation phase with other sequential optimizations in the compiler are discussed. The techniques in this paper are responsible for achieving significant performance improvements on the industry standard SPEC* OMPM2001 and SPEC* OMPL2001 benchmarks, and these results are presented for Intel® Pentium® and Itanium® processor based systems. © Springer-Verlag 2004. . Keywords: Application programming interfaces (API); C++ (programming language); C compilers; Compiler optimizations; Design considerations; Fortran compilers; OpenMP compilers; OpenMP programs; OpenMP translations; Optimisations; Optimization techniques; Performance; Industry standards; Processor based systems; Sequential optimization; Program compilers
2004-01-01 - Efficient data driven run-time code generation	Knowledge of data values at run-time allows us to generate better code in terms of eficiency, size and power consumption. This paper introduces a low-level compiling technique based on a minimal code generator with parametric embedded sections to generate binary code at run-time. This generator called a "compilet" creates code and allocates registers using the data input. Then, it generates the needed instructions. Our measurements, performed on Itanium 2 and PowerPC platforms have shown a speed improvement of 43% on the Itanium 2 platform and 41% on the PowerPC one. The proposed technique proves to be particularly useful in the case of intensively reused functions in graphic applications, where the advantages of dynamic compilation have not been fully taken into account yet. . Keywords: Automatic programming; Fading (radio); Program compilers; Code generators; Compiling techniques; Data driven; Data values; Dynamic compilation; Graphic applications; Run-time code generation; Speed improvement; Codes (symbols)
2004-01-01 - Efficient formal verification of pipelined processors with instruction queues	Presented is a method for formal verification of pipelined processors with long instruction queues. The execution engine and the fetch engine (where the instruction queue is) are formally verified separately, after abstracting the other engine with a non-deterministic FSM derived from the high-level specification of that engine. Without the presented method, the monolithic formal verification of 9-stage, 9-wide VLIW processors - implementing many realistic and speculative features inspired by the Intel Itanium-scaled for models with 5 instruction-queue entries, but ran out of memory if the instruction queue was longer. The presented method resulted in 2 orders of magnitude speedup for the processor with 5 instruction-queue entries, and enabled scaling for designs with 64 instruction-queue entries. . Keywords: Boolean functions; Computer simulation; Mathematical models; Problem solving; Program processors; Queueing networks; Logic of equality; Positive equality; SAT; Pipeline processing systems
2004-01-01 - Efficient modeling of Itanium® architecture during instruction scheduling using extended finite state automata	Effective and efficient modeling and management of hardware resources have always been critical toward generating highly efficient code in optimizing compilers. The instruction templates and dispersal rules of the Itanium® architecture add new complexity in managing resource constraints to instruction scheduler. We extended a finite state automaton (FSA) approach to efficiently manage all key resource constraints of an Itanium® architecture on-the-fly during instruction scheduling. We have fully integrated the FSA-based resource management into the instruction scheduler in the Open Research Compiler for the Itanium® architecture. Our integrated approach shows up to 12% speedup on some SPECint2000 benchmarks and 4.5% speedup on average for all SPECint2000 benchmarks on an Itanium®-based system when compares to an instruction scheduler with decoupled resource management. In the meantime, the instruction scheduling time of our approach is reduced by 4% on average. . Keywords: Benchmarking; Computer hardware; Constraint theory; Costs; Finite automata; Optimization; Program compilers; Scheduling; Instruction scheduling; Resource management (RM); Schedulers; Computer architecture
2004-01-01 - Exploiting free execution slots on EPIC processors for efficient and accurate runtime profiling	Dynamic optimization relies on runtime profile information to improve the performance of program execution. Traditional profiling techniques incur significant overhead and are not suitable for dynamic optimization. In this paper, we propose a new profiling technique that incorporates the strength of both software and hardware to achieve near-zero overhead profiling. The compiler passes profiling requests as a few bits of information in branch instructions to the hardware, and the hardware uses the free execution slots available in a user program to execute profiling operations. We have implemented the compiler instrumentation of this technique using an Itanium research compiler. Our result shows that the accurate block profiling incurs very little overhead to the user program in terms of the program scheduling cycles. For example, the average overhead is 0.6% for the SPECint95 benchmarks. The hardware support required for the new profiling is practical. We believe this will enable many profile-driven dynamic optimizations for EPIC processors such as the Itanium processors. © Springer-Verlag 2004. . Keywords: Computer hardware; Hardware; Branch instructions; Dynamic optimization; Hardware supports; Itanium processor; Program execution; Program scheduling; Run-time profiling; Software and hardwares; Program compilers
2004-01-01 - Exploring the performance potential of itanium® processors with ILP-based scheduling	HP and Intel's Itanium Processor Family (IPF) is considered as one of the most challenging processor architectures to generate code for. During global instruction scheduling, the compiler must balance the use of strongly interdependent techniques like code motion, speculation and predication. A too conservative application of these features can lead to empty execution slots, contrary to the EPIC philosophy. But overuse can cause resource shortage which spoils the benefit. We tackle this problem using integer linear programming (ILP), a proven standard optimization method. Our ILP model comprises global, partial-ready code motion with automated generation of compensation code as well as vital IPF features like control / data speculation and predication. The ILP approach can - with some restrictions - resolve the interdependences between these decisions and deliver the global optimum. This promises a speedup for compute-intensive applications as well as some theoretically funded insights into the potential of the architecture. Experiments with several hot functions from the SPEC benchmarks show substantial improvements: Our postpass optimizer reduces the schedule lengths produced by Intel's compiler by about 20-40%. The resulting speedup of these routines is 16% on average. . Keywords: Automation; Codes (symbols); Integer programming; Linear programming; Scheduling; Software engineering; Code generation; Instruction-level parallelism (ILP); Integer linear programming (ILP); Program processors
2004-01-01 - Field-testing IMPACT EPIC research results in Itanium 2	Explicitly-Parallel Instruction Computing (EPIC) provides architectural features, including predication and explicit control speculation, intended to enhance the compiler's ability to expose instruction-level parallelism (ILP) in control-intensive programs. Aggressive structural transformations using these features, though described in the literature, have not yet been fully characterized in complete systems. Using the Intel Itanium 2 microprocessor, the SPECint2000 benchmarks and the IMPACT Compiler for IA-64, a research compiler competitive with the best commercial compilers on the platform, we provide an in situ evaluation of code generated using aggressive, EPIC-enabled techniques in a reality-constrained microarchitecture. Our work shows a 1.13 average speedup (up to 1.50) due to these compilation techniques, relative to traditionally-optimized code at the same inlining and pointer analysis levels, and a 1.55 speedup (up to 2.30) relative to GNU GCC, a solid traditional compiler. Detailed results show that the structural compilation approach provides benefits far beyond a decrease in branch misprediction penalties and that it both positively and negatively impacts instruction cache performance. We also demonstrate the increasing significance of runtime effects, such as data cache and TLB, in determining end performance and the interaction of these effects with control speculation. . Keywords: Benchmarking; Computer software; Control systems; Microprocessor chips; Performance; Program compilers; Scheduling; Control structures; Explicitly-parallel instruction computing (EPIC); Instruction-level parallelism (ILP); Intel (CO); Parallel processing systems
2004-01-01 - Global instruction scheduling technique in ORC	IA-64 is a novel architecture that provides ample hardware support to exploit instruction level parallelism. And it is exemplified by the Itanium processor. Authors adapt D. Bernstein's algorithm, which is targeted for superscalar processor, to EPIC processor Itanium. And authors take full advantage of Itanium specific details, and have two improvements to D. Bernstein's algorithm: (1) Apply hierarchical region structure into this algorithm. (2) Integrate P-Ready instruction scheduling. P-Ready is proposed under a context that is totally different with D. Bernstein's. The algorithm proposed is implemented in the open research compiler ORC. Authors' results show that the global scheduler achieves 8.4% runtime speedup on CPU2000int benchmarks. As an indicator of the advantage of applying hierarchical region structure, the technique that moves instruction across nested loops can obtain up to 12.9% runtime speedup. The integrated P-Ready instruction scheduling can obtain up to 7.6% speedup and 1.37% speedup on average for all CPU2000int benchmarks. . Keywords: Algorithms; Computer architecture; Program processors; Scheduling; Global instruction scheduling; Hierarchical structured regions; Itanium processors; P ready scheduling; Program compilers
2004-01-01 - Hardware support for prescient instruction prefetch	This paper proposes and evaluates hardware mechanisms for supporting prescient instruction prefetch - an approach to improving single-threaded application performance by using helper threads to perform instruction prefetch. We demonstrate the need for enabling store-to-load communication and selective instruction execution when directly pre-executing future regions of an application that suffer I-cache misses. Two novel hardware mechanisms, safe-store and YAT-bits, are introduced that help satisfy these requirements. This paper also proposes and evaluates finite state machine recall, a technique for limiting pre-execution to branches that are hard to predict by leveraging a counted I-prefetch mechanism. On a research Itanium® SMT processor with next line and streaming I-prefetch mechanisms that incurs latencies representative of next generation processors, prescient instruction prefetch can improve performance by an average of 10.0% to 22% on a set of SPEC 2000 benchmarks that suffer significant I-cache misses. Prescient instruction prefetch is found to be competitive against even the most aggressive research hardware instruction prefetch technique: fetch directed instruction prefetch. . Keywords: Benchmarking; Codes (symbols); Computational methods; Computer hardware; Computer software; Database systems; Finite automata; Performance; Memory latencies; Prescient instruction prefetch; Simultaneous multithreading (SMT); Microprocessor chips
2004-01-01 - Helper threads via virtual multithreading on an experimental itanium® 2 processor-based platform	Helper threading is a technology to accelerate a program by exploiting a processor's multithreading capability to run "assist" threads. Previous experiments on hyper-threaded processors have demonstrated significant speedups by using helper threads to prefetch hard-to-predict delinquent data accesses. In order to apply this technique to processors that do not have built-in hardware support for multithreading, we introduce virtual multithreading (VMT), a novel form of switch-on-event user-level multithreading, capable of fly-weight multiplexing of event-driven thread executions on a single processor without additional operating system support. The compiler plays a key role in minimizing synchronization cost by judiciously partitioning register usage among the user-level threads. The VMT approach makes it possible to launch dynamic helper thread instances in response to long-latency cache miss events, and to run helper threads in the shadow of cache misses when the main thread would be otherwise stalled. The concept of VMT is prototyped on an Itanium® 2 processor using features provided by the Processor Abstraction Layer (PAL) firmware mechanism already present in currently shipping processors. On a 4-way MP physical system equipped with VMT-enabled Itanium 2 processors, helper threading via the VMT mechanism can achieve significant performance gains for a diverse set of real-world workloads, ranging from single-threaded workstation benchmarks to heavily multithreaded large scale decision support systems (DSS) using the IBM DB2 Universal Database. We measure a wall-clock speedup of 5.8% to 38.5% for the workstation benchmarks, and 5.0% to 12.7% on various queries in the DSS workload. . Keywords: Buffer storage; Computer hardware; Computer operating systems; Computer workstations; Database systems; Multiplexing; Optimization; Program compilers; Scheduling; Synchronization; Cache miss prefetching; DB2 database; Helper thread; Itanium processor; PAL; Switch-on-event; Virtual multithreading (VMT); Multiprocessing systems
2004-01-01 - Helper threads via virtual multithreading on an experimental Itanium® 2 processor-based platform	Helper threading is a technology to accelerate a program by exploiting a processor's multithreading capability to run "assist" threads. Previous experiments on hyper-threaded processors have demonstrated significant speedups by using helper threads to prefetch hard-to-predict delinquent data accesses. In order to apply this technique to processors that do not have built-in hardware support for multithreading, we introduce virtual multithreading (VMT), a novel form of switch-on-event user-level multithreading, capable of fly-weight multiplexing of event-driven thread executions on a single processor without additional operating system support. The compiler plays a key role in minimizing synchronization cost by judiciously partitioning register usage among the user-level threads. The VMT approach makes it possible to launch dynamic helper thread instances in response to long-latency cache miss events, and to run helper threads in the shadow of cache misses when the main thread would be otherwise stalled. The concept of VMT is prototyped on an Itanium® 2 processor using features provided by the Processor Abstraction Layer (PAL) firmware mechanism already present in currently shipping processors. On a 4-way MP physical system equipped with VMT-enabled Itanium 2 processors, helper threading via the VMT mechanism can achieve significant performance gains for a diverse set of real-world workloads, ranging from single-threaded workstation bench-marks to heavily multithreaded large scale decision support systems (DSS) using the IBM DB2 Universal Database. We measure a wall-clock speedup of 5.8% to 38.5% for the workstation benchmarks, and 5.0% to 12.7% on various queries in the DSS workload. Copyright 2004 ACM. . Keywords: Cache memory; Computer hardware; Computer operating systems; Data transfer; Database systems; Decision support systems; Multiplexing; Cache miss prefetching; DB2 database; Helper threads; Itanium processors; Multithreading; Processor Abstraction Layer (PAL); Switch-on-events; Microprocessor chips
2004-01-01 - Helper threads via virtual multithreading on an experimental itanium® 2 processor-based platform	Helper threading is a technology to accelerate a program by exploiting a processor's multithreading capability to run "assist" threads. Previous experiments on hyper-threaded processors have demonstrated significant speedups by using helper threads to prefetch hard-to-predict delinquent data accesses. In order to apply this technique to processors that do not have built-in hardware support for multithreading, we introduce virtual multithreading (VMT), a novel form of switch-on-event user-level multithreading, capable of fly-weight multiplexing of event-driven thread executions on a single processor without additional operating system support. The compiler plays a key role in minimizing synchronization cost by judiciously partitioning register usage among the user-level threads. The VMT approach makes it possible to launch dynamic helper thread instances in response to long-latency cache miss events, and to run helper threads in the shadow of cache misses when the main thread would be otherwise stalled. The concept of VMT is prototyped on an Itanium® 2 processor using features provided by the Processor Abstraction Layer (PAL) firmware mechanism already present in currently shipping processors. On a 4-way MP physical system equipped with VMT-enabled Itanium 2 processors, helper threading via the VMT mechanism can achieve significant performance gains for a diverse set of real-world workloads, ranging from single-threaded workstation benchmarks to heavily multithreaded large scale decision support systems (DSS) using the IBM DB2 Universal Database. We measure a wall-clock speedup of 5.8% to 38.5% for the workstation benchmarks, and 5.0% to 12.7% on various queries in the DSS workload. . Keywords: Benchmarking; Buffer storage; Computer operating systems; Database systems; Decision support systems; Optimization; Program compilers; Switching; Virtual reality; Cache miss prefetching; DB2 database; Helper threading; Itanium processor; Processor abstraction layer (PAL); Switch on-event; Virtual multithreading (VMT); Microprocessor chips
2004-01-01 - HP Pioneers 64-bit Computing	The Itanium workstations from Hewlett-Packard running on 64-bit chip capable of computing same power as compared to UNIX allows analysis of software running on Windows XP and opens 500,000-component model on single workstation. This workstation performs top-level interference checking and collaborative fly-through design reviews. The 64-bit workstation runs Windows XP Professional 64-bit edition, HP-UX or Linux Red Hat operating systems. The Itanium workstation has more physical memory which stimulates efficient interactive performance. . Keywords: Computer simulation; Computer workstations; Data storage equipment; Garages (parking); Mathematical models; Optimization; Personal computers; Program assemblers; Random access storage; Software engineering; UNIX; Desktop workstations; Simulation software; Computer aided design
2004-01-01 - Improving 64-bit Java IPF performance by compressing heap references	64-bit processor architectures like the Intel® Itanium® Processor Family are designed for large applications that need large memory addresses. When running applications that fit within a 32-bit address space, 64-bit CPUs are at a disadvantage compared to 32-bit CPUs because of the larger memory footprints for their data. This results in worse cache and TLB utilization, and consequently lower performance because of increased miss ratios. This paper considers software techniques for virtual machines that allow 32-bit pointers to be used on 64-bit CPUs for managed runtime applications that do not need the full 64-bit address space. We describe our pointer compression techniques and discuss our experience implementing these for Java applications. In addition, we give performance results with our techniques for both the SPEC JVM98 and SPEC JBB2000 benchmarks. We demonstrate a 12% performance improvement on SPEC JBB2000 and a reduction in the number of garbage collections required for a given heap size. . Keywords: Benchmarking; Buffer storage; Java programming language; Microprocessor chips; Object oriented programming; Optimization; Garbage collection; Memory addresses; Pointer compression techniques; Virtual machines; Multiprocessing systems
2004-01-01 - Improving load/store queues usage in scientific computing	Memory disambiguation mechanisms, coupled with load/store queues in out-of-order processors, are crucial to increase Instruction Level Parallelism (ILP), especially for memory-bound scientific codes. Designing ideal memory disambiguation mechanisms is too complex because it would require precise address bits comparators; thus, modern microprocessors implement simplified and imprecise ones that perform only partial address comparisons. In this paper, we study the impact of such simplifications on the sustained performance of some real processors such that Alpha 21264, Power 4 and Itanium 2. Despite all the advanced features of these processors, we demonstrate in this article that memory address disambiguation mechanisms can cause significant performance loss. We demonstrate that, even if data are located in low cache levels and enough ILP exist, the performance degradation can be up to 21 times slower if no care is taken on the order of accessing independent memory addresses. Instead of proposing a hardware solution to improve load/store queues, as done in [1, 6, 5, 7, 4], we show that a software (compilation) technique is possible. Such solution is based on the classical (and robust) Id/st vectorization. Our experiments highlight the effectiveness of such method on BLAS 1 codes that are representative of vector scientific loops. . Keywords: Buffer storage; Computer software; Microprocessor chips; Parallel processing systems; Program compilers; Queueing networks; Storage allocation (computer); Hardware mechanisms; Instruction level parallelism (ILP); Memory addresses; Queues usage; Natural sciences computing
2004-01-01 - Increasing software-pipelined loops in the itanium-like architecture	The Itanium architecture (IPF) contains features such as register rotation to support efficient software pipelining. One of the drawbacks of software pipelining is its high register requirement, which may lead to failure when registers provided by architecture are insufficient. This paper evaluates the register requirements of software-pipelined loops in Itanium architecture and presents two new methods, which try to reduce static general register requirements in software pipelined loops by either reducing instructions in the loop body or allocating unused rotating registers for variants using static registers. We have implemented our methods in the Open Research Compiler (ORC) targeted for Itanium processors, and experiments show that number of software-pipelined loops in NAS Benchmarks increased. For some benchmarks, the performance is improved by more than 18%. © Springer-Verlag Berlin Heidelberg 2004. . Keywords: Pipeline processing systems; Program compilers; Benchmarking; Computer architecture; Itanium; Itanium processor; NAS benchmarks; Open research compilers; Performance; Register requirement; Software pipelining; Static register; Benchmarking; Pipeline processing systems
2004-01-01 - In-system FPGA prototyping of an itanium microarchitecture	We describe an effort to prototype an Itanium microarchitecture using an FPGA. The microarchitecture model is written in the Bluespec hardware description language (HDL) and supports a subset of the Itanium instruction set architecture. The microarchitecture model includes details such as multi-bundle instruction fetch, decode and issue; parallel pipelined execution units with scoreboarding and predicated bypassing; and multiple levels of cache hierarchies. The microarchitecture model is synthesized and prototyped on a special FPGA card that allows the processor model to interface directly to the memory bus of a host PC. This is an effort toward developing a flexible microprocessor prototyping framework for rapid design exploration. © 2004 IEEE. . Keywords: Cache memory; Computer architecture; Computer hardware description languages; Field programmable gate arrays; Microprocessor chips; Program processors; Semantics; Cache hierarchies; Functional programming; Itanium Microarchitecture; Microprocessor prototyping; Software prototyping
2004-01-01 - Integrating formal verification with murcp of distributed cache coherence protocols in FAME multiprocessor system design	Flexible Architecture for Multiple Environments (FAME) is Bull architecture for large symmetrical multiprocessors based on Intel's Itanium® 2 family, which is used in Bull NovaScale® servers series. A key point in the development of this distributed shared memory architecture is the definition of its cache coherence protocol. This paper reports experiences and results of integrating formal verification of FAME cache coherence protocol, on 4 successive versions of this architecture. The goal is to find protocol definition bugs (not implementation) in the early phases of the design, focusing on: cache coherency, data integrity and deadlock-freeness properties. We have performed modeling and verification using Murcp tool and language, because of its easiness of use and its efficient state reduction techniques. The analysis of the results shows that this approach is cost-effective, and in spite of the state explosion problem, it has helped us in finding hard-to-simulate protocol bugs, before the implementation is far ahead. © IFIP International Federation for Information Processing 2004. . Keywords: Cache memory; Cost effectiveness; Memory architecture; Modeling languages; Multiprocessing systems; Network architecture; Cache coherence protocols; Deadlock freeness; Distributed cache; Distributed shared memory; Flexible architectures; Modeling and verifications; Multi processor systems; State explosion problems; Formal verification
2004-01-01 - Inter-procedural register allocation for RSE optimization	In Itanium® architecture, a hardware managed register stack is introduced, register stack engine (RSE) can change the register stack frame pointers and spill/fill registers automatically. This mechanism can reduce load/store operations of register across call sites efficiently. The number of stacked registers used by a procedure could be specified by alloc instruction explicitly. Traditional intra-procedural register allocation algorithm will allocate max stacked registers required by a procedure but no more than the total number of stack registers. But a high stack register pressure will lead to frequent register stack spill/fill. If this event happens frequently, the performance will be seriously harmed. This paper proposes an innovative algorithm, which could reduce the RSE cost efficiently. This algorithm is already implemented in ORC. Experimental results show that the performance is improved obviously when this algorithm is applied, especially for perlbmk, it has 14% performance improvement and crafty also has 3.2% performance improvement. . Keywords: Optimization; Resource allocation; Itanium; Register stack engine; Register stack spill fill; Microprocessor chips
2004-01-01 - Ispike: A post-link optimizer for the Intel®Itanium® architecture	Ispike is a post-link optimizer developed for the Intel® Itanium Processor Family (IPF) processors. The IPF architecture poses both opportunities and challenges to post-link optimizations. IPF offers a rich set of performance counters to collect detailed profile information at a low cost, which is essential to post-link optimization being practical At the same time, the predication and bundling features on IPF make post-link code transformation more challenging than on other architectures. In Ispike, we have implemented optimizations like code layout, instruction prefetching, data layout, and data prefetching that exploit the IPF advantages, and strategies that cope with the IPF-specific challenges. Using SPEC CINT2000 as benchmarks, we show that Ispike improves performance by as much as 40% on the Itanium® 2 processor, with average improvement of 8.5% and 9.9% over executables generated by the Intel® Electron compiler and by the Gcc compiler, respectively. We also demonstrate that statistical profiles collected via IFF performance counters and complete profiles collected via instrumentation produce equal performance benefit, but the profiling overhead is significantly lower for performance counters. . Keywords: Buffer storage; Computer architecture; Computer operating systems; Data processing; Optimization; Program compilers; Statistical methods; Fine-grain performance monitoring; Load-latency comparators; Post-link code transformation; Post-link optimizers; Microprocessor chips
2004-01-01 - Itanium 2 processor 6M: Higher frequency and larger L3 cache	This article presents new design of the next generation Itanium 2 processor which features a 6-Mbyte, larger L3 cache increasing frequency by 50 percent. It includes ball-grid array package with portions of CMOS integrated circuits incorporating built-in self-test with high-performance computing than the RISC. . Keywords: Bandwidth; Codes (symbols); Microprocessor chips; Silica; Systems analysis; On die cache size; Power dissipation; Parallel processing systems
2004-01-01 - Loop unrolling optimization for software pipelining	Loop unrolling can make software pipelining achieve fractional initiation interval and improve resource utilization. Optimizations based on unrolling can reduce resource requirements and the heights of critical paths. An algorithm named UTBPC (unrolling times based program characteristics) for determining unrolling factors and unrolling based optimization for software data prefetching were proposed. These optimizations were implemented in ORC (open research compiler), and SPEC CPU2000 benchmarks were tested in Itanium processor. The average performance was improved by 2.6%. The results show that UTBPC algorithm and unrolling based optimization for software data prefetching can improve the overall performance of compilers. . Keywords: Algorithms; Computer programming; Critical path analysis; Data processing; Optimization; Requirements engineering; Software engineering; Data prefetching; Loop unrolling; Software pipelining; Unrolling times; Computer software
2004-01-01 - Making sense of 64-bit processors	The suggestions for making the right decision toward selecting 64-bit processors are discussed. The Itanium's ability to run multiple instructions in parallel, along with its high-performance floating point capabilities, ideally positions it for business intelligence. The Itanium is also ideally positioned for, ERP, security transactions, and compute-intensive custom applications. The AMD chips use a considerably different approach to delivering high performance, where memory management is built into the AMD 64-bit processors which eliminates the need for an FSB processor and enables the processor to directly address memory. . Keywords: Buffer storage; Computer networks; Computer operating systems; Costs; Database systems; Decision making; Enterprise resource planning; Hard disk storage; Parallel processing systems; Purchasing; Reduced instruction set computing; Servers; AMD (CO); Explicitly parallel instruction computing (EPIC); Intel Corp (CO); Processor pyrotechnics; Program processors
2004-01-01 - MDA: Revenge of the modelers or UML Utopia?	Software industry uses Unified Modeling Language for many information technology applications. Metadata provides semantics to support OMG's Model Driven Architecture including decision tables, XML and middleware. Domain-oriented programming and domain-specific languages are useful computer-aided software engineering to build automated code generates for Itanium processor. . Keywords: Computer hardware description languages; Computer simulation; Knowledge based systems; Mathematical models; Model-driven agile development; Model-driven architecture; Software engineering
2004-01-01 - Micro-architecture techniques in the Intel® E8870 scalable memory controller	This paper describes several selected micro-architectural tradeoffs and optimizations for the scalable memory controller of the Intel E8870 chipset architecture. The Intel E8870 chipset architecture supports scalable coherent multiprocessor systems using 2 to 16 processors, and a point-to-point Scalability Port (SP) Protocol. The scalable memory controller micro-architecture applies a number of micro-architecture techniques to reduce the local & remote idle and loaded latencies. The performance optimizations were achieved within the constraints of maintaining functional correctness, while reducing implementation complexity and cost. High bandwidth point-to-point interconnects and distributed memory are expected to be more common in future platforms to support powerful multi-core processors. The selected techniques discussed in this paper will be applicable to scalable memory controllers needed in those platforms. These techniques have been proven for production systems for the Itanium® II Processor platforms. Copyright 2004 ACM. . Keywords: Computers; Controllers; Cost reduction; Network architecture; Scalability; Architectural tradeoffs; Chipset; distributed coherency; Distributed Memory; High bandwidth; Implementation complexity; Itanium; Memory latencies; Micro architectures; Multi processor systems; Multi-core processor; Performance optimizations; Production system; Scalable memory; Computer architecture
2004-01-01 - Microarchitecture-level power modelling and analyzing for high-performance microprocessors	Power dissipation is becoming a serious challenge to microprocessor design, which requires to consider power and performance trade-offs during architecture design phase. Two baseline architecture models with single clock and multi-clock domains respectively are presented based on the Itanium 2 microprocessor architecture. The circuit characteristics are abstracted at microarchitecture level and a microarchitectural peak power estimation model is implemented. An event-scheduling algorithm is presented to support the simulation of the multi-clock-domain systems. The dynamic power simulation model is implemented based on the IMPACT simulation engine. Compared with similar model Wattch, the peak power estimation accuracy is improved by 3% and the simulation speed is improved by 42%. Experiment is also made to demonstrate the power efficiency of multi-clock-domain system. In one scenario of multi-clock and multi-voltage control, the power and energy are reduced by 21% and 38% respectively with acceptable performance loss. This power model is suitable for architecture-level low power design. . Keywords: Algorithms; Buffer storage; Cache memory; Energy dissipation; Mathematical models; Low power design; Microarchitecture; Power model; Microprocessor chips
2004-01-01 - Multi-level approach for high-precision cache fault isolation - Case study: Itanium® II processor low voltage cache yield improvement	Fault Isolation / Failure Analysis (FI/FA) of increasingly complex embedded memory in microprocessors is becoming more difficult due to process scaling and presence of subtle defects. As physical failure analysis (PFA) is destructive and involves expensive and time-consuming processes, fault diagnosis needs to be as precise as possible to ensure successful physical defect sighting. This paper introduces a cache Fault Isolation methodology that focuses on exhaustive data collection to derive concrete hypothesis of physical fault location and to overcome the existing FA/FI challenges. The methodology involves a novel application of existing DFT techniques in combination with circuit analysis, pattern hacking, defect localization and PFA tools. Some of the techniques, for example pattern modification or circuit simulation, are applied repeatedly in order to obtain higher-level of isolation - from cell/logic level to transistor/gate level, and finally down to physical structure/layer level. This multi-level FI approach is the key to localize the failing area to greater precision, which had proven itself in Intel Itanium® II processor yield improvement process. . Keywords: Cache memory; Computer simulation; Defects; Design for testability; Integrated circuits; Liquid crystals; Microprocessor chips; Static random access storage; Cache fault isolation methodology; Circuit simulation; Emission microscopy; Physical failure analysis (PFA); Failure analysis
2004-01-01 - Optimizing sparse matrix-vector product computations using unroll and jam	Large-scale scientific applications frequently compute sparse matrix-vector products in their computational core. For this reason, techniques for computing sparse matrix-vector products efficiently on modern architectures are important. In this paper we describe a strategy for improving the performance of sparse matrix-vector product computations using a loop transformation known as unroll-and-jam. We describe a novel sparse matrix representation that enables us to apply this transformation. Our approach is best suited for sparse matrices that have rows with a small number of predictable lengths. This work was motivated by sparse matrices that arise in SAGE, an application from Los Alamos National Laboratory. We evaluate the performance benefits of our approach using sparse matrices produced by SAGE for a pair of sample inputs. We show that our strategy is effective for improving sparse matrix-vector product performance using these matrices on MIPS R12000, Alpha Ev67, IBM Power 3, and Itanium 2 processors. Our measurements show that for this class of sparse matrices, our strategy improves sparse matrix-vector product performance from a low of 41% on MIPS to well over a factor of 2 on Itanium. . Keywords: Bandwidth; Buffer storage; Computer architecture; Computer systems; Data structures; Optimization; Vectors; Microfactors; Performance optimization; Sparse matrix format; Sparse matrix-vector product; Matrix algebra
2004-01-01 - Performance and performance counters on the Itanium 2 - A benchmarking case study	[No abstract available] . Keywords:
2004-01-01 - Performance modeling of unstructured mesh particle transport computations	The performance of unstructured mesh applications presents a number of complexities and subtleties that do not arise for dense structured meshes. From a programming point of view, handling an unstructured mesh has an increased complexity to manage the necessary data structures and interactions between mesh-cells. From a performance point of view, there are added difficulties in understanding both the processing time on a single processor and the scaling characteristics. In this work we present a performance model for the calculation of deterministic SN transport on unstructured meshes. It builds upon earlier work that successfully modeled the same calculation on structured meshes. The model captures the key processing characteristics and is parametric using both the system performance data (latency, bandwidth, processing rate etc.) and application data (mesh size etc.) as input. The model is validated on two clusters (an HP AlphaServer and an Itanium-2 system) showing high accuracy. Importantly it is also shown that a single formulation of the model can be used to predict the performance of two quite different implementations of the same calculation. . Keywords: Algorithms; Bandwidth; Benchmarking; Data reduction; Installation; Optimization; Parallel processing systems; Parameter estimation; Accelerated Strategic Computing Initiative (ASCI); Earth Simulators; SMP clusters; Sweeping; Computational complexity
2004-01-01 - Performance of hyperspectral imaging algorithms using itanium architecture	This paper describes the experiences and results on implementing a set of hyperspectral imaging analysis algorithms on the Itanium Processor Family. On Itanium architecture all instructions are transformed into bundles of instructions and these bundles are processed in a parallel fashion by the different functional units. Experimental results show that exploiting implicit parallelism and linking HP Mathematical LIBrary optimized for Itanium yield significant improvement in performance. . Keywords: Algorithms; Data reduction; Information analysis; Matrix algebra; Optimization; Parameter estimation; Performance; Vectors; HP-MLIB; Hyperspectral Imaging; IA64; Image Classifiers; Itanium; Imaging techniques
2004-01-01 - Pinpointing representative portions of large intel® itanium® programs with dynamic instrumentation	Detailed modeling of the performance of commercial applications is difficult. The applications can take a very long time to run on real hardware and it is impractical to simulate them to completion on performance models. Furthermore, these applications have complex execution environments that cannot easily be reproduced on a simulator, making porting the applications to simulators difficult. We attack these problems using the well-known SimPoint methodology to find representative portions of an application to simulate, and a dynamic instrumentation framework called Pin to avoid porting altogether. Our system uses dynamic instrumentation instead of simulation to find representative portions -called PinPoints - for simulation. We have developed a toolkit that automatically detects PinPoints, validates whether they are representative using hardware performance counters, and generates traces for large Itanium® programs. We compared SimPoint-based selection to random selection of simulation points. We found for 95% of the SPEC2000 programs we tested, the PinPoints prediction was within 8% of the actual whole-program CPI, as opposed to 18% for random selection. We measure the end-to-end error, comparing real hardware to a performance model, and have a simple and efficient methodology to determine the step that introduced the error. Finally, we evaluate the system in the context of multiple configurations of real hardware, commercial applications, and industrial-strength performance models to understand the behavior of a complete and practical workload collection system. We have successfully used our system with many commercial Itanium® programs, some running for trillions of instructions, and have used the resulting traces for predicting performance of those applications on future Itanium processors. © 2004 IEEE. . Keywords: Algorithms; Computational fluid dynamics; Computer architecture; Computer hardware; Computer simulation; Database systems; Mathematical models; Microprocessor chips; Dynamic instrumentation; Hardware performance counter; SimPoint profiles; Whole-system simulators; Computer program listings
2004-01-01 - Platform-independent cache optimization by pinpointing low-locality reuse	For many applications, cache misses are the primary performance bottleneck. Even though much research has been performed on automatically optimizing cache behavior at the hardware and the compiler level, many program executions remain dominated by cache misses. Therefore, we propose to let the programmer optimize, who has a better high-level program overview, needed to resolve many cache problems. In order to assist the programmer, a visualization of memory accesses with poor locality is developed. The aim is to indicate causes of cache misses independent of actual cache parameters such as associativity or size. In that way, the programmer is steered towards platform-independent locality optimizations. The visualization was applied to three programs from the SPEC2000 benchmarks. After optimizing the source code based on the visualization, an average speedup of 3.06 was obtained on different platforms with Athlon, Itanium and Alpha processors; indicating the feasibility of platform-independent cache optimizations. © Springer-Verlag 2004. . Keywords: Computer programming; Visualization; Cache optimization; Cache parameters; High-level program; Locality optimization; Low localities; Performance bottlenecks; Platform independent; Program execution; Program compilers
2004-01-01 - Prefetch injection based on hardware monitoring and object metadata	Cache miss stalls hurt performance because of the large gap between memory and processor speeds - for example, the popular server benchmark SPEC JBB2000 spends 45% of its cycles stalled waiting for memory requests on the Itanium® 2 processor. Traversing linked data structures causes a large portion of these stalls. Prefetching for linked data structures remains a major challenge because serial data dependencies between elements in a linked data structure preclude the timely materialization of prefetch addresses. This paper presents Mississippi Delta (MS Delta), a novel technique for prefetching linked data structures that closely integrates the hardware performance monitor (HPM), the garbage collector's global view of heap and object layout, the type-level metadata inherent in type-safe programs, and JIT compiler analysis. The garbage collector uses the HPM's data cache miss information to identify cache miss intensive traversal paths through linked data structures, and then discovers regular distances (deltas) between these linked objects. JIT compiler analysis injects prefetch instructions using deltas to materialize prefetch addresses. We have implemented MS Delta in a fully dynamic profile-guided optimization system: the StarJIT dynamic compiler and the ORP Java virtual machine, We demonstrate a 28-29% reduction in stall cycles attributable to the high-latency cache misses targeted by MS Delta and a speedup of 11-14% on the cache miss intensive SPEC JBB2000 benchmark. . Keywords: Benchmarking; Computer hardware; Data structures; Dynamic random access storage; Information management; Metadata; Optimization; Program compilers; Program processors; Cache misses; Compiler optimization; Garbage collection; Prefetching; Profile-guided optimization; Virtual machines; Cache memory
2004-01-01 - Prefetch injection based on hardware monitoring and object metadata	Cache miss stalls hurt performance because of the large gap between memory and processor speeds - for example, the popular server benchmark SPEC JBB2000 spends 45% of its cycles stalled waiting for memory requests on the Itanium® 2 processor. Traversing linked data structures causes a large portion of these stalls. Prefetching for linked data structures remains a major challenge because serial data dependencies between elements in a linked data structure preclude the timely materialization of prefetch addresses. This paper presents Mississippi Delta (MS Delta), a novel technique for prefetching linked data structures that closely integrates the hardware performance monitor (HPM), the garbage collector's global view of heap and object layout, the type-level metadata inherent in type-safe programs, and JIT compiler analysis. The garbage collector uses the HPM's data cache miss information to identify cache miss intensive traversal paths through linked data structures, and then discovers regular distances (deltas) between these linked objects. JIT compiler analysis injects prefetch instructions using deltas to materialize prefetch addresses. We have implemented MS Delta in a fully dynamic profile-guided optimization system: the Star JIT dynamic compiler [1] and the ORP Java virtual machine [9]. We demonstrate a 28-29% reduction in stall cycles attributable to the high-latency cache misses targeted by MS Delta and a speedup of 11-14% on the cache miss intensive SPEC JBB2000 benchmark. . Keywords: Cache memory; Codes (symbols); Data storage equipment; Data structures; Java programming language; Metadata; Microprocessor chips; Optimization; Virtual reality; Cache misses; Compiler optimization; Garbage collection; Prefetching; Profile-guided optimization; Virtual machines; Program compilers
2004-01-01 - Proceedings of the 2004 7th Annual IEEE International Workshop on Workload Characterization, WWC-7	The proceedings contain 10 papers from the Proceedings of the 2004 7th Annual IEEE International Workshop on Workload Characterization, WWC-7. The topics discussed include: characterizing the impact of different memory intensity levels; evaluation of speculative multithreading compiler by characterizing program dependencies; on the extraction and analysis of prevalent dataflow patterns; experiments with subsetting benchmark suites; construction and performance characterization of parallel interior point solver on 4-way intel itanium multiprocessor system; the USAR characterization model; micro-architectural anatomy of a commercial TCP/IP stack; and performance characterization of BLAST on intel xeon and itanium2 processors. . Keywords: Bandwidth; Buffer storage; Computer architecture; Computer hardware; Computer simulation; Computer systems; Data storage equipment; Integrated circuit layout; Interfaces (computer); Network protocols; Online systems; Optimization; Product design; Program compilers; Servers; Web browsers; Benchmark program; Dataflow patterns; EiRev; Generator of itneractive user media sessions (GENIUS); Instruction set architecture (ISA); Inter-arrival time (IAT); Media distributions; Memory-intensity levels; Trace-collection systems; User profiles; Multiprocessing systems
2004-01-01 - Proceedings of the 2004 Workshop on Computer Architecture Education, WCAE 2004 - Held in Conjunction with the 31st International Symposium on Computer Architecture, ISCA 2004	The proceedings contain 24 papers. The topics discussed include: a computer architecture education curriculum through the design and implementation of original processors using FPGAs; extending FPGA-based teaching boards into the area of distributed memory multiprocessors; teaching computer architecture using an architecture description language; creating sharable learning objects from existing digital course content; introduction to formal processor verification at logic level: a case study; bridges to computer architecture education; improving instruction set architecture learning results; integrating research and e-learning in advanced computer architecture courses; teaching basics of instruction pipelining with HDLDLX; software implementations of division and square-root operations for Intel Itanium processors; Pin: a binary instrumentation tool for computer architecture research and education; and a combined virtual and remotely accessible microprocessor laboratory. . Keywords:
2004-01-01 - QB or not QB: An efficient execution verification tool for memory orderings	We study the problem of formally verifying shared memory multiprocessor executions against memory consistency models - an important step during post-silicon verification of multiprocessor machines. We employ our previously reported style of writing formal specifications for shared memory models in higher order logic (HOL), obtaining intuitive as well as modular specifications. Our specification consists of a conjunction of rules that constrain the global visibility order. Given an execution to be checked, our algorithm generates Boolean constraints that capture the conditions under which the execution is legal under the visibility order. We initially took the approach of specializing the memory model HOL axioms into equivalent (for the execution to be checked) quantified boolean formulae (QBF). As this technique proved inefficient, we took the alternative approach of converting the HOL axioms into a program that generates a SAT instance when run on an execution. In effect, the quantifications in our memory model specification were realized as iterations in the program. The generated Boolean constraints are satisfiable if and only if the given execution is legal under the memory model. We evaluate two different approaches to encode the Boolean constraints, and also incremental techniques to generate and solve Boolean constraints. Key results include a demonstration that we can handle executions of realistic lengths for the modern Intel Itanium memory model. Further research into proper selection of Boolean encodings, incremental SAT checking, efficient handling of transitivity, and the generation of unsatisfiable cores for locating errors are expected to make our technique practical. © Springer-Verlag Berlin Heidelberg 2004. . Keywords: Algorithms; Boolean functions; Computer aided analysis; Encoding (symbols); Formal logic; Formal specification; Memory architecture; Multiprocessing systems; Specifications; Visibility; Incremental techniques; Memory consistency models; Modular specifications; Multiprocessor machines; Quantified Boolean formulas; Shared memory model; Shared memory multiprocessor; Visibility ordering; Boolean algebra
2004-01-01 - Region-based partial dead code elimination on predicated code	This paper presents the design, implementation and experimental evaluation of a practical region-based partial dead code elimination (PDE) algorithm on predicated code in an existing compiler framework. Our algorithm processes PDE candidates using a worklist and reasons about their partial deadness using predicate partition graphs. It operates uniformly on hyperblocks and regions comprising basic blocks and hyperblocks. The result of applying our algorithm to an SEME region is optimal: partially dead code cannot be removed without changing the branching structure of the program or potentially introducing new predicate defining instructions. We present statistic evidence about the PDE opportunities in the 12 SPECint2000 benchmarks. In addition to exhibit small compilation overheads, our algorithm achieves moderate performance improvements in 8 out of the 12 benchmarks on an Itanium machine. Our performance results and statistics show the usefulness of our algorithm as a pass applied before instruction scheduling. © Springer-Verlag 2004. . Keywords: Algorithms; Benchmarking; Codes (symbols); Program compilers; Basic blocks; Branching structures; Experimental evaluation; Hyperblocks; Instruction scheduling; Itanium; Region-based; Optimal systems
2004-01-01 - Scalable graphics: Faster, better, sooner graphics coming to A PC near you part 2 of 2	The developments made in the scaling of graphics by the firm Silicon Graphics are discussed. The SGI Scalable Graphics technology concentrates the power of multiple GPU onto a single screen. A large number of Application Programming Interfaces (API) have emerged that add to the commercially available API such as OpenGL Performer. Some of the leading Open Source codes are Open SceneGraph, OpenSG and OpenRM, all of which enable multiple GPU to be used easily in a scalable system, allowing the programmer to concentrate on their main focus. The convergence of mainstrean graphics scalability and the scalable Linux and Itanium 2 processor-based Silicon Graphics architectures creates an exciting option for users. . Keywords: Coding; Computer Graphics; Image Analysis; Magnetic Resonance; Microcomputers; Codes (symbols); Computer architecture; Computer software; Distributed computer systems; Magnetic resonance imaging; Motion pictures; Personal computers; User interfaces; Application programming interfaces (API); Open source codes; Silicon Graphics (CO); Computer graphics
2004-01-01 - Software implementations of division and square root operations for intel® itanium® processors	Division and square root are basic operations defined by the IEEE Standard 754-1985 for Binary Floating-Point Arithmetic [1], and are implemented in hardware in most modern processors. In recent years however, software implementations of these operations have become competitive. The first IEEE-correct implementations in software of the division and square root operations in a mainstream processor appeared in the 1980s [2]. Since then, several major processor architectures adopted similar solutions for division and square root algorithms, including the Intel® Itanium® Processor Family (IPF). Since the first software algorithms for division and square root were designed and used, improved algorithms were found and complete correctness proofs were carried out. It is maybe possible to improve these algorithms even further. The present paper gives an overview of the IEEE-correct division and square root algorithms for Itanium processors. As examples, a few algorithms for single precision are presented and properties used in proving their IEEE correctness are stated. Non-IEEE variants, less accurate but faster, of the division, square root and also reciprocal and reciprocal square root operations are discussed. Finally, accuracy and performance numbers are given. The algorithms presented here are inlined by the Intel and other compilers for IPF, whenever division and square root operations are performed. . Keywords: Algorithms; Correctness proofs; Floating-point arithmetic; Itanium processor; Processor architectures; Software algorithms; Software implementation; Square-root algorithms; Square-root operations; Computer architecture
2004-01-01 - Software pipelining: An effective scheduling technique for VLIW machines	The basic idea behind software pipelining was first developed by Patel and Davidson for scheduling hardware pipe-lines. As instruction-level parallelism made its way into general-purpose computing, it became necessary to automate scheduling. How and whether instructions can be scheduled statically have major ramifications on the design of computer architectures. Rau and Glaeser were the first to use software pipelining in a compiler for a machine with specialized hardware designed to support software pipelining. In the meantime, trace scheduling was touted to be the scheduling technique of choice for VLIW (Very Long Instruction Word) machines. The most important contribution from this paper is to show that software pipelining is effective on VLIW machines without complicated hardware support. Our understanding of software pipelining subsequently deepened with the work of many others. And today, software pipelining is used in all advanced compilers for machines with instruction-level parallelism, none of which, except the Intel Itanium, relies on any specialized support for software pipelining. . Keywords: Automation; Computer architecture; Computer software; Heuristic methods; Hierarchical systems; Image processing; Iterative methods; Libraries; Optimization; Program compilers; Scheduling; Signal processing; Software engineering; Cross iteration boundaries; Image-processing library; Instruction level parallelism (ILP); Software pipelining; Very long instruction word architecture
2004-01-01 - Super Scalar Sample Sort	Sample sort, a generalization of quicksort that partitions the input into many pieces, is known as the best practical comparison based sorting algorithm for distributed memory parallel computers. We show that sample sort is also useful on a single processor. The main algorithmic insight is that element comparisons can be decoupled from expensive conditional branching using predicated instructions. This transformation facilitates optimizations like loop unrolling and software pipelining. The final implementation, albeit cache efficient, is limited by a linear number of memory accesses rather than the (n log n) comparisons. On an Itanium 2 machine, we obtain a speedup of up to 2 over std::sort from the GCC STL library, which is known as one of the fastest available quicksort implementations. © Springer-Verlag 2004. . Keywords: Algorithms; Optimization; Cache-efficient; Distributed-memory parallel computers; Element comparison; Loop unrolling; Memory access; Single processors; Software pipelining; Sorting algorithm; Cache memory
2004-01-01 - SYZYGY - A framework for scalable cross-module IPO	Performing analysis across module boundaries for an entire program is important for exploiting several runtime performance opportunities. However, due to scalability problems in existing full-program analysis frameworks, such performance opportunities are only realized by paying tremendous compile-time costs. Alternative solutions, such as partial compilations or user assertions, are complicated or unsafe and as a result, not many commercial applications are compiled today with cross-module optimizations. This paper presents SYZYGY, a practical framework for performing efficient, scalable, interprocedural optimizations. The framework is implemented in the HP-UX Itanium® compilers and we have successfully compiled many very large applications consisting of millions of lines of code. We achieved performance improvements of up to 40% over optimization level two and compilation time improvements in the order of 100% and more compared to a previous approach. . Keywords: Buffer storage; Hard disk storage; Mathematical models; Optimization; Program compilers; Program debugging; Interprocedural optimization (IPO); Memory resident processes; Open Research compiler (ORC); Summary collection; Microprocessor chips
2004-01-01 - Task-queue based hybrid parallelism: A case study	In this paper we report on our experiences with hybrid parallelism in PARDISO, a high-performance sparse linear solver. We start with the OpenMP-parallel numerical factorization algorithm and reorganize it using a central dynamic task queue to be able to add message passing functionality. The hybrid version allows the solver to run on a larger number of processors in a cost effective way with very reasonable performance. A speed-up of more than nine running on a four-node quad Itanium 2 SMP cluster is achieved in spite of the fact that a large potential to minimize MPI communication is not yet exploited in the first version of the implementation. © Springer-Verlag 2004. . Keywords: Application programming interfaces (API); Cost effectiveness; Multiprocessing systems; Cost effective; Dynamic tasks; Factorization algorithms; Hybrid parallelisms; Linear solver; MPI communications; SMP clusters; Speed up; Message passing
2004-01-01 - Testing and Reliability Techniques for High-Bandwidth Embedded RAMs	Application-specific integrated circuits (ASICs) and high-performance processors such as Itanium and Compaq Alpha use a total of almost 75% of chip real estate for accommodating various types of embedded (or on-chip) memories. Although most of these embedded memories are single-port static (and in relatively few cases, dynamic) RAMs today, the high demand for bandwidth in digital television, fast signal processing, and high-speed networking applications will also fuel the need for on-chip multiport memories in the foreseeable future. The reliability of a complex VLSI chip will depend largely on the reliability of these embedded memory blocks. With device dimensions moving rapidly toward the ultimate physical limits of device scaling, which is in the regime of feature sizes of 50 nm or so, a host of complex failure modes is expected to occur in memory circuits. This tutorial underlines the need for appropriate testing and reliability techniques for the present to the next generation of embedded RAMs. Topics covered include: reliability and quality testing, fault modeling, advanced built-in self-test (BIST), built-in self-diagnosis (BISD), and built-in self-repair (BISR) techniques for high-bandwidth embedded RAMs. . Keywords: Application specific integrated circuits; Bandwidth; Built-in self test; Digital television; Embedded systems; Failure analysis; Reliability; VLSI circuits; Embedded memories; Error scrubbing; Random access storage
2004-01-01 - The compiler as a validation and evaluation tool	Like a processor executes flawlessly at different frequencies, a compiler should produce correct results at any optimization level. The Intel® Itanium® processor family with its new features, like the register stack engine and control- and data speculation, provides new and unique challenges for ported software and compiler technology. This paper describes validation and evaluation techniques that can be employed in compilation tools and can help to get a cleaner port of an application, a more robust compilation system and even insights into performance tuning opportunities. Using Itanium as a specific example, the paper explains why the register stack engine (RSE), the large register file, or control- and data speculation can potentially expose bugs in poorly written or compiled software. It then demonstrates validation and evaluation techniques to find or expose these bugs. An evaluation team can employ them to find, eliminate and evaluate software bugs. A compiler team can use them to make the compiler more stable and robust. A performance analysis team can use them to uncover performance opportunities in an application. We demonstrate our validation and evaluation techniques on code examples and provide run-time data to indicate the cost of some of our methods. ©2003 Published by Elsevier Science B.V. . Keywords: Codes (symbols); Computer simulation; Computer software; Costs; Mathematical models; Optimization; Compilers; Optimization level; Register stack engine (RSE); Robust compliance system; Program processors
2004-01-01 - Validating the Itanium 2 exception control unit: A unit-level approach	The exception control unit (XPN) is one of the most complex functional units in the Itanium 2 processor. Its primary purpose is to prioritize exception handler address while maintaining the proper interruption-related architectural state. It is shown that a unit-level testing environment can succeed in flushing bugs out of a design when standard approaches fail. . Keywords: Computational complexity; Design for testability; Logic design; Microprocessor chips; State space methods; Exception control unit; Full chip testing; Machine specific registers; Unit level functional verification; Computer aided design
2004-01-01 - VBlades: Optimized paravirtualization for the itanium processor family	Virtualization of an "uncooperative"architecture often has severe performance consequences. Paravirtualization has recently been suggested as a solution to performance issues, but it introduces unacceptable supportability problems. The HP Labs vBlades project has identified a novel hybrid approach - which we call optimized paravirtualization. We examine methods for both virtualizing and paravirtualizing the Itanium processor, and then demonstrate optimized paravirtualization to maximize performance while simultaneously minimizing supportability concerns.  © 2004 by The USENIX Association. . Keywords: Engineering research; Java programming language; Network security; Virtualization; Hp labs; Hybrid approach; Itanium processor; Itanium processor family; Para-virtualization; Performance issues; Supportability; Virtual machine
2004-01-01 - WBTK: A new set of microbenchmarks to explore memory system performance for scientific computing	Memory hierarchies are a key component in obtaining high performance on modern microprocessors. To satisfy the ever-increasing demand on data rate access, they are also becoming increasingly complex: multilevel caches, non-blocking caches, sophisticated instructions for supporting prefetch and cache control, etc. If all of these advanced features promise to offer large performance gains, they also generate in some cases performance "anomalies" (i.e. bad performance triggered by specific code patterns). For precisely locating and understanding these anomalies, a new set of microbenchmarks called WBTK is introduced. We show through systematic experimentation on Alpha 21264, Power4 and Itanium 1 that this microbenchmark first allowed us to detect most of the anomalies encountered on simple BLAS1 type codes. Secondly, it led us to demonstrate that vectorization of memory access was an efficient workaround for most of these anomalies. . Keywords: Benchmarking; Cache memory; Computational complexity; Microprocessor chips; Optimization; Performance; Program compilers; Storage allocation (computer); Cache control; Memory hierarchy; Microbenchmarking; Out of order processing; Performance evaluation; Vector loops; Parallel processing systems
2005-01-01 - 2005 International Conference on Integrated Circuit Design and Technology, ICICDT	The proceedings contain 39 papers from the 2005 International Conference on Integrated Circuit Design and Technology, ICICDT. The topics discussed include: the design and implementation of the double-precision multiplier in a first-generation CELL processor; a scalable stepped gate sensing scheme for sub-100nm multilevel flash memory; the implementation of a 2-core multi-threaded Itanium family processor; design of transceiver circuits for NRZ signaling in inductive inter-chip wireless superconnect; challenges in the high-K dielectric implementation for 45nm technology node; charging damage and SOI; influence of DT gate current on NBTI in ultra thin gate oxide; and the urgency of deep sub-ambient cooling for gigascale integration. . Keywords: Dielectric devices; Electric currents; Flash memory; Double-precision multipliers; Gate currents; Gate oxides; Transceiver circuits; Integrated circuit layout
2005-01-01 - A 130-nm triple-Vt 9-MB third-level on-die cache for the 1.7-GHz Itanium® 2 processor	The 18-way set-associative, single-ported 9 MB cache for the Itanium® 2 Processor uses 210 identical 48-kB sub-arrays with a 2.21 -μm2 cell in a 130-nm 6-metal technology. The processor runs at 1.7 GHz at 1.35 V and dissipates 130 W. The 432-mm2 die contains 592 M transistors, the largest transistor count reported for a microprocessor. This paper reviews circuit design and implementation details for the L3 cache data and tag arrays. The staged mode ECC scheme avoids a latency increase in the L3 Tag. A high Vt implant improves the read stability and reduces the sub-threshold leakage. . Keywords: Buffer storage; Computer architecture; Data reduction; Decoding; Design for testability; Dielectric materials; Energy dissipation; Natural frequencies; Thermal effects; Circuit design; Clock distribution; Manufacturability; On-die cache; Power redeuction; Tag array; Microprocessor chips
2005-01-01 - A 90nm variable-frequency clock system for a power-managed itanium®-family processor	A clock-generation system delivers fixed- and variable-frequency clocks for adaptive power control on a 1.7B-transistor dual-core CPU. Frequency synthesizers digitally divide a fixed-frequency PLL clock in 1/64th cycle steps using programmable voltage-frequency-converter loops. 1-cycle loop response tracks supply transients with adaptive modulation, improving CPU performance by over 10% compared to a fixed-frequency design. © 2005 IEEE. . Keywords: Electric converters; Electric potential; Electric power systems; Frequency synthesizers; Modulators; Power control; Program processors; Transistors; Adaptive modulation; Clock systems; Fixed-frequency design; Variable-frequency clocks; Electric clocks
2005-01-01 - A 90nm variable-frequency clock system for a power-managed itanium®-family processor	[No abstract available] . Keywords:
2005-01-01 - A design of high speed AGTL+ output buffer	AGTL+ (Assisted Gunning Transceiver Logic+) signal transmission and interface technology are analyzed in this paper. To resolve the problem on such short high-level duration time in traditional design, we have proposed an auxiliary charged circuit structure. According to what I have analyzed, we design and realize a AGTL+ interface circuit, which is completely compatible with Itanium 2 interface and has high-speed and high noise margin. The operating frequency of circuit reaches to 500MHz by SPICE simulation in the condition of 0.18μm standard CMOS process. © 2005 IEEE. . Keywords: Buffer circuits; CMOS integrated circuits; Computer simulation; Gunn devices; Integrated circuit layout; Natural frequencies; Signal processing; Assisted Gunning Transceiver Logic+ (AGTL+); Auxiliary charged circuit structure; Signal transmission; SPICE simulation; Transceivers
2005-01-01 - A general performance model of structured and unstructured mesh particle transport computations	The performance of unstructured mesh applications presents a number of complexities and subtleties that do not arise for dense structured meshes. From a programming point of view, the handling of unstructured meshes has an increased complexity in order to manage the necessary data structures and interactions between mesh-cells. From a performance point of view, there are added difficulties in understanding both the processing time on a single processor and the scaling characteristics when using large-scale parallel systems. In this work we present a general performance model for the calculation of deterministic S N transport on unstructured meshes that is also applicable to structured meshes. The model captures the key processing characteristics of the calculation and is parametric using both system performance data (latency, bandwidth, processing rate etc.) and application data (mesh size etc.) as input. A single formulation of the model is used to predict the performance of two quite different implementations of the same calculation. It is validated on two clusters (an HP AlphaServer and an Itanium-2 system) showing high prediction accuracy. © 2005 Springer Science + Business Media, Inc. . Keywords: Bandwidth; Computational methods; Data structures; Microprocessor chips; Parallel processing systems; High performance computing; Large-scale systems; Parallel processing; Performance analysis; Performance modeling; SN transport; Unstructured meshes; Mathematical models
2005-01-01 - A mummy mystery revealed	Silicon Graphics' (SGI) Reality Center Theater, equipped with a curved, 25-foot, 3840×1024-pixel projection screen, and Silicon Graphics Prism visualization system powered by 24 Itanium 2 processors, revealed the mystery of Skerit, the child mummy. The images were processed on SGI's Prism visualization platform with Intel Itanium 2 processors running VGL ray tracing software from Volume Graphics GmBh. Ray tracing software traces the path taken by a ray of light through a scene, such as the mummy, and calculates reflection, refraction, or absorption. Real anatomy exists in three dimensions, so any time the user can view anatomical data in 3-D. . Keywords: Computer Graphics; Computer Programs; Light Absorption; Prisms; Projection; Screens; Computer graphics; Computer software; Light absorption; Light reflection; Light refraction; Prisms; Program processors; Ray tracing; Visualization; Prism visualization system; Reality Center Theater; Silicon Graphics (SGI) (CO); Projection screens
2005-01-01 - An empirical study of data speculation use on the intel itanium 2 processor	The Intel Itanium architecture uses a dedicated 32-entry hardware table, the Advanced Load Address Table (ALAT) to support data speculation via an instruction set interface. This study presents an empirical evaluation of the use of the ALAT and data speculative instructions for several optimizing compilers. We determined what and how often compilers generated the different speculative instructions, and used the Itanium's hardware performance counters to evaluate their run-time behavior. We also performed a limit study by modifying one compiler to always generate data speculation when possible. We found that this aggressive approach significantly increased the amount of data speculation and often resulted in performance improvements, of as much as 10% in one case. Since it worsened performance only for one application and then only for some inputs, we conclude that more aggressive data speculation heuristics than those employed by current compilers are desirable and may further improve performance gains from data speculation. . Keywords: Computer architecture; Computer hardware; Evaluation; Heuristic methods; Optimization; Program processors; Data speculative instructions; Instruction set interfaces; Data reduction
2005-01-01 - An overview of the open research compiler	The Open Research Compiler (ORC), jointly developed by Intel Microprocessor Technology Labs and the Institute of Computing Technology at Chinese Academy of Sciences, has become the leading open source compiler on the Itanium™ Processor Family (IPF, previously known as IA-64). Since its first release in 2002, it has been widely used in academia and industry worldwide as a compiler and architecture research infrastructure and as code base for further development. In this paper, we present an overview of the design of the major components in ORC, especially those new features in the code generator. We discuss the development methodology that is important to achieving the objectives of ORC. Performance comparisons with other IPF compilers and a brief summary of the research work based on ORC are also presented. © Springer-Verlag Berlin Heidelberg 2005. . Keywords: Codes (symbols); Computer architecture; Microprocessor chips; Network components; Research and development management; Computing technology; Open research compilers (ORC); Program compilers
2005-01-01 - Analyis of path profiling information generated with performance monitoring hardware	Even with the breakthroughs in semiconductor technology that will enable billion transistor designs, hardwarebased architecture paradigms alone cannot substantially improve processor performance. The challenge in realizing the full potential of these future machines is to find ways to adapt program behavior to application needs and processor resources. As such, run-time optimization will have a distinct role in future high performance systems. However, as these systems are dependent on accurate, fine-grain profile information, traditional approaches to collecting profiles at run-time result in significant slowdowns during program execution. A novel approach to low-overhead profiling is to exploit hardware Performance Monitoring Units (PMUs) present in modern microprocessors. The Itanium-2 PMU can periodically sample the last few taken branches in an executing program and this information can be used to recreate partial paths of execution. With compiler-aided analysis, the partial paths can be correlated into full paths. As statistically hot paths are most likely to occur in PMU samples, even infrequent sampling can accurately identify these paths. While traditional path profiling techniques carry a high overhead, a PMU-based path profiler represents an effective lightweight profiling alternative. This paper characterizes the PMU-based path information and demonstrates the construction of such a PMU-based path profiler for a run-time system. . Keywords: Computer hardware; Computer programming; Microprocessor chips; Optimization; Transistors; Fine-grain profile information; Hardwarebased architecture paradigms; Performance monitoring hardwares; Information analysis
2005-01-01 - Building applications for the Linux Standard Base	The goal of the Linux™ Standard Base (LSB) is to develop and promote a set of standards that will increase compatibility among Linux distributions and enable software applications to run on any compliant Linux system. There are currently LSB specifications available for the Intel Architecture IA-32™ processors and for the 32- and 64-bit PowerPC™, Itanium™, 31 - and 64-bit zSeries™, and AMD64™ architectures. This paper describes the process of building LSB-compliant applications, and covers the use of the LSB development environments, testing of binaries, and packaging. © Copyright 2005 by International Business Machines Corporation. . Keywords: C (programming language); Computer architecture; Computer operating systems; Interfaces (computer); Open systems; Program compilers; Standards; Application binary interface (ABI); Linux standard base; Software engineering
2005-01-01 - Cache aware optimization of stream programs	Effective use of the memory hierarchy is critical for achieving high performance on embedded systems. We focus on the class of streaming applications, which is increasingly prevalent in the embedded domain. We exploit the widespread parallelism and regular communication patterns in stream programs to formulate a set of cache aware optimizations that automatically improve instruction and data locality. Our work is in the context of the Synchronous Dataflow model, in which a program is described as a graph of independent actors that communicate over channels. The communication rates between actors are known at compile time, allowing the compiler to statically model the caching behavior. We present three cache aware optimizations: 1) execution scaling, which judiciously repeats actor executions to improve instruction locality, 2) cache aware fusion, which combines adjacent actors while respecting instruction cache constraints, and 3) scalar replacement, which converts certain data buffers into a sequence of scalar variables that can be register allocated. The optimizations are founded upon a simple and intuitive model that quantifies the temporal locality for a sequence of actor executions. Our implementation of cache aware optimizations in the StreamIt compiler yields a 249% average speedup (over unoptimized code) for our streaming benchmark suite on a StrongARM 1110 processor. The optimizations also yield a 154% speedup on a Pentium 3 and a 152% speedup on an Itanium 2. Copyright 2005 ACM. . Keywords: Codes (symbols); Computer programming; Embedded systems; Graph theory; Optimization; Parallel processing systems; Cache optimization; Stream programming; Synchronous dataflow; Buffer storage
2005-01-01 - Cache aware optimization of stream programs	Effective use of the memory hierarchy is critical for achieving high performance on embedded systems. We focus on the class of streaming applications, which is increasingly prevalent in the embedded domain. We exploit the widespread parallelism and regular communication patterns in stream programs to formulate a set of cache aware optimizations that automatically improve instruction and data locality. Our work is in the context of the Synchronous Dataflow model, in which a program is described as a graph of independent actors that communicate over channels. The communication rates between actors are known at compile time, allowing the compiler to statically model the caching behavior. We present three cache aware optimizations: 1) execution scaling, which judiciously repeats actor executions to improve instruction locality, 2) cache aware fusion, which combines adjacent actors while respecting instruction cache constraints, and 3) scalar replacement, which converts certain data buffers into a sequence of scalar variables that can be register allocated. The optimizations are founded upon a simple and intuitive model that quantifies the temporal locality for a sequence of actor executions. Our implementation of cache aware optimizations in the StreamIt compiler yields a 249% average speedup (over unoptimized code) for our streaming benchmark suite on a StrongARM 1110 processor. The optimizations also yield a 154% speedup on a Pentium 3 and a 152% speedup on an Itanium 2. Copyright 2005 ACM. . Keywords: Constraint theory; Data flow analysis; Data storage equipment; Data structures; Embedded systems; Hierarchical systems; Program processors; Cache; Cache Optimizations; Embedded; Stream Programing; Synchronous Dataflow; Computer programming
2005-01-01 - Clock distribution on a dual-core multi-threaded itanium®-family processor	Clock distribution on the 90nm ltanium® processor is detailed. A region-based active de-skew system reduces the PVT sources of skew across the entire die during normal operation. Clock vernier devices inserted at each local clock buffer allow up to a 10% clock-cycle adjustment via firmware or scan. The system supports a constantly varying frequency and consumes <25W from PLL to latch while providing <10ps of skew across PVT. © 2005 IEEE. . Keywords: Dies; Firmware; Flip flop circuits; Program processors; Scanning; Clock distribution; Clock vernier; Clock-cycle; De-skew systems; Electric clocks
2005-01-01 - Clock distribution on a dual-core, multi-threaded itanium® family microprocessor	[No abstract available] . Keywords:
2005-01-01 - Clock distribution on a dual-core, multi-threaded itanium®-family processor	[No abstract available] . Keywords:
2005-01-01 - Compiler optimization-space exploration	To meet the performance demands of modern architectures, compilers incorporate an ever-increasing number of aggressive code transformations. Since most of these transformations are not universally beneficial, compilers traditionally control their application through predictive heuristics, which attempt to judge an optimization's effect on final code quality a priori. However, complex target architectures and unpredictable optimization interactions severely limit the accuracy of these judgments, leading to performance degradation because of poor optimization decisions. This performance loss can be avoided through the iterative compilation approach, which advocates exploring many optimization options and selecting the best one a posteriori. However, existing iterative compilation systems suffer from excessive compile times and narrow application domains. By overcoming these limitations, Optimization-Space Exploration (OSE) becomes the first iterative compilation technique suitable for general-purpose production compilers. OSE narrows down the space of optimization options explored through limited use of heuristics. A compiler tuning phase further limits the exploration space. At compile time, OSE prunes the remaining optimization configurations in the search space by exploiting feedback from earlier configurations tried. Finally, rather than measuring actual runtimes, OSE compares optimization outcomes through static performance estimation, further enhancing compilation speed. An OSE-enhanced version of Intel's reference compiler for the Itanium architecture yields a performance improvement of more than 20% for some SPEC benchmarks. . Keywords:
2005-01-01 - Computer muscle pumps up productivity	Korea's Hyundai Heavy Industries Co. (HHI) has added a Silicon Graphic's (SGI) Altix 350 server system with eight Intel Itanium 2 processors to aid in the construction of large containerships. The SGI Altix server family leverages SGI NUMAflex architecture, which provides the ability for 512 processors to operate under a single system image under a super cluster environment. Vancouver, Canada-based ARL's upcoming release of ShipConstructor 2006 represents the most significant upgrade of the software since its initial release. ShipConstructor 2006 provides designers and builders with significant cost savings and allows them to react quicker to late customer changes. . Keywords: Computer software; Customer satisfaction; Program processors; Servers; Containerships; Cost savings; Silicon Graphics (CO); Marine applications
2005-01-01 - Computer systems	Computing systems show significant results in the fields such as interplanetary missions, atmospheric and orbital flight, processors and architecture. Research in robotics and swarms is bringing fruitful results in computer hardware, software, intelligent systems, flight control. Columbia an SGI Altix cluster composed of 10,240 Intel Itanium 2 processors, builds on experience with a predecessor cluster at Ames, named Kalpana after Kalpana Chawla. Current Nanotechnology builds features on the order of 100 NM wide by chemical vapor deposition-diffusing impurities into silicon, creating wells of positive and negative charge. . Keywords: Chemical vapor deposition; Computer hardware; Computer software; Flight dynamics; Intelligent structures; Nanotechnology; Planets; Intel Itanium 2 processors; Interplanetary missions; Orbital flight; Computer systems
2005-01-01 - Cores and effect	Intel has unveiled details of its first dual core device, code named Montecito. It integrates two Itanium cores, running two threads, so to the operating system a Montecito looks like four processors. The main problem faced by Intel at the design of Montecito was power reduction, because the last three generations of Itanium processors have all had the same power consumption, despite each being implemented in a smaller process. So to achieve the required performance within a tolerable power envelope, several power management techniques were developed. . Keywords: Analog to digital conversion; Electric power utilization; Electric resistance; Parameter estimation; Power control; Specifications; Substrates; Transistors; Clock buffers; Delay line loops; Intel (CO); Power density; Microprocessor chips
2005-01-01 - Creating a database with cardioscopy and intra-operative imaging	The abilities for both computer technology, and intra-operative video-imaging, are evolving rapidly. The merger of these two sciences can be very beneficial, both to congenital cardiac surgeons in general, and in facilitating the creation of a cardioscopic database in particular. © 2005 Cambridge University Press. . Keywords: Cardiac Surgical Procedures; Child, Preschool; Databases; Diagnostic Imaging; Electrocardiography; Female; Heart Defects, Congenital; Humans; Infant; Infant, Newborn; Male; Medical Records Systems, Computerized; Monitoring, Intraoperative; Sensitivity and Specificity; United States; aorta subvalvular stenosis; computer program; digital imaging; electronic medical record; endoscopic surgery; factual database; Fallot tetralogy; heart atrium septum defect; heart surgery; heart ventricle septum defect; human; image analysis; imaging system; Internet; intraoperative period; medical photography; minimally invasive cardiac surgery; open heart surgery; patent ductus arteriosus; pediatric surgery; review; comparative study; congenital heart malformation; data base; diagnostic imaging; electrocardiography; female; infant; male; medical record; methodology; newborn; patient monitoring; preschool child; sensitivity and specificity; United States
2005-01-01 - Design of a software distributed shared memory system using an MPI communication layer	We designed and implemented a software distributed shared memory (DSM) system, SCASH-MPI, by using MPI as the communication layer of the SCASH DSM. With MPI as the communication layer, we could use high-speed networks with several clusters and high portability. Furthermore, SCASH-MPI can use high-speed networks with MPI, which is the most commonly available communication library. On the other hand, existing software DSM systems usually use a dedicated communication layer, TCP, or UDP-Ethernet. SCASH-MPI avoids the need for a large amount of pin-down memory for shared memory use that has limited the applications of the original SCASH. In SCASH-MPI, a thread is created to support remote memory communication using MPI. An experiment on a 4-node Itanium cluster showed that the Laplace Solver benchmark using SCASH-MPI achieves a performance comparable to the original SCASH. Performance degradation is only 6.3% in the NPB BT benchmark Class B test. In SCASH-MPI, page transfer does not start until a page fault is detected. To hide the latency of page transmission, we implemented a prefetch function. The latency in BT Class B was reduced by 64% when the prefetch function was used. © 2005 IEEE. . Keywords: Computer software; Network protocols; Remote control; Storage allocation (computer); User interfaces; Distributed shared memory (DSM) system; High-speed networks; MPI communication; Page transmission; Distributed computer systems
2005-01-01 - Dragon: A static and dynamic tool for OpenMP	A program analysis tool can play an important role in helping users understand and improve OpenMP codes. Dragon is a robust interactive program analysis tool based on the Open64 compiler, an open source OpenMP, C/C++/Fortran77/90 compiler for Intel Itanium systems. We developed the Dragon tool on top of Open64 to exploit its powerful analyses in order to provide static as well as dynamic (feedback-based) information which can be used to develop or optimize OpenMP codes. Dragon enables users to visualize and print essential program structures and obtain runtime information on their applications. Current features include static/dynamic call graphs and control flow graphs, data dependence analysis and interprocedural array region summaries, that help understand procedure side effects within parallel loops. On-going work extends Dragon to display data access patterns at runtime, and provide support for runtime instrumentation and optimizations. © Springer-Verlag Berlin Heidelberg 2005. . Keywords: C (programming language); Data processing; Graph theory; Image coding; Optimization; Program compilers; Robustness (control systems); Dynamic tools; Interactive program analysis; Static tools; Computer program listings
2005-01-01 - Dual-core CPUs raise the system-throughput bar	Intel and Advanced Micro Devices (AMD) are planning to release their dual-core CPUs to improve the system throughput. The CPUs of AMD will be compatible with the 940-pin sockets for the single-core Opteron processors and the 939-pin sockets for the single core Athion 64 processors. AMD claims that its dual-core 64x2 CPUs will improve the throughput by 90% compared to a single-core CPU. Intel plans to unviel another dual-core Itanium called Montevale processor in the year 2006. . Keywords: Computer systems; Performance; Servers; Throughput; Advanced Micro Devices (AMD) (CO); Central processor unit (CPU); Dual-core CPUs; Intel (CO); Program processors
2005-01-01 - Efficiently implementing LL/SC objects shared by an unknown number of processes	Over the past decade, a pair of instructions called load-linked (LL) and store-conditional (SC) have emerged as the most suitable synchronization instructions for the design of lock-free algorithms. However, current architectures do not support these instructions; instead, they support either CAS (e.g., UltraSPARC, Itanium) or restricted versions of LL/SC (e.g., POWER4, MIPS, Alpha). To bridge this gap, a flurry of algorithms that implement LL/SC from CAS have appeared in the literature. Some of these algorithms assume that N, the maximum number of participating processes, is fixed and known in advance. Others make no such assumption, but are either non-blocking (not wait-free), implement small LL/SC objects, or require that a process performs O(N) work to join the algorithm. Specifically, no constant-time, word-sized, wait-free LL/SC algorithm that does not require the knowledge of N exists. In this paper, we present such an algorithm. © Springer-Verlag Berlin Heidelberg 2005. . Keywords: Algorithms; Computer architecture; Decision support systems; Information analysis; Knowledge acquisition; LL/SC algorithm; Load-linked (LL); Lock-free algorithms; Store-conditional (SC); Synchronization
2005-01-01 - Fine-grain stacked register allocation for the itanium architecture	The introduction of a hardware managed register stack in the Itanium Architecture creates an opportunity to optimize both the frequency in which a compiler requests allocation of registers from this stack and the number of registers requested. The Itanium Architecture specifies the implementation of a Register Stack Engine (RSE) that automatically performs register spills and fills. However, if the compiler requests too many registers, through the alloc instruction, the RSE will be forced to execute unnecessary spill and fill operations. In this paper we introduce the formulation of the fine-grain register stack frame sizing problem. The normal interaction between the compiler and the RSE suggested by the Itanium Architecture designers is for the compiler to request the maximum number of registers required by a procedure at the procedure invocation. Our new problem formulation allows for more conservative stack register allocation because it acknowledges that the number of registers required in different control flow paths varies significantly. We introduce a basic algorithm to solve the stack register allocation problem, and present our preliminary performance results from the implementation of our algorithm in the Open64 compiler. © Springer-Verlag Berlin Heidelberg 2005. . Keywords: Algorithms; Computer architecture; Optimization; Problem solving; Program compilers; Itanium architecture; Register Stack Engine (RSE); Stack register allocation problem; Resource allocation
2005-01-01 - Fujitsu's chipset development for high-performance, high-reliability mission-critical IA servers PRIMEQUEST	Fujitsu has developed a new mission-critical IA server in close collaboration with Intel Corp. The new server, PRIMEQUEST, uses Intel's latest high-performance, high-reliability Itanium 2 processor. PRIMEQUEST offers linear scalability from a single CPU to 32 CPUs (64 CPUs in the second generation) and represents a highly reliable technology equivalent to that of a mainframe. We have also developed six new chipsets using our cutting-edge CS101 ASIC technology and employed new technologies such as a high-speed interconnection between chipsets, address and system mirroring (including mirroring of ASIC internal blocks), and a new standard high-speed I/O interface called PCI-Express. This paper gives an overview of the new chipsets. . Keywords: Application specific integrated circuits; Program processors; Codes (standards); Input output programs; Interfaces (computer); Linear systems; Microprocessor chips; Chipset development; PCI-Express; PRIMEQUEST; Intel Corp. (CO); Linear scalability; Servers
2005-01-01 - Generating cache hints for improved program efficiency	One of the new extensions in EPIC architectures are cache hints. On each memory instruction, two kinds of hints can be attached: a source cache hint and a target cache hint. The source hint indicates the true latency of the instruction, which is used by the compiler to improve the instruction schedule. The target hint indicates at which cache levels it is profitable to retain data, allowing to improve cache replacement decisions at run time. A compile-time method is presented which calculates appropriate cache hints. Both kind of hints are based on the locality of the instruction, measured by the reuse distance metric. Two alternative methods are discussed. The first one profiles the reuse distance distribution, and selects a static hint for each instruction. The second method calculates the reuse distance analytically, which allows to generate dynamic hints, i.e. the best hint for each memory access is calculated at run-time. The implementation of the static hints scheme in the Open64-compiler for the Itanium processor shows a speedup of 10% on average on a set of pointer-intensive and regular loop-based programs. The analytical approach with dynamic hints was implemented in the FPT-compiler and shows up to 34% reduction in cache misses. © 2004 Elsevier B.V. All rights reserved. . Keywords: Computer programming; Computer software reusability; Data processing; Optimization; Program compilers; Program processors; Compiler optimization; EPIC; Replacement policy; Reuse distance; Source cache hint; Target cache hint; Cache memory
2005-01-01 - Guide to RISC processors: For programmers and engineers	Recently, there has been a trend toward processor design based on the RISC (Reduced Instruction Set Computer) model: Example RISC processors are the MIPS, SPARC, PowerPC, ARM, and even Intel's 64-bit processor Itanium. This guidebook provides an accessible and all-encompassing compendium on RISC processors, introducing five RISC processors: MIPS, SPARC, PowerPC, ARM, and Itanium. Initial chapters explain the differences between the CISC and RISC designs and clearly discuss the core RISC design principles. The text then integrates instruction on MIPS assembly language programming, thereby enabling readers to concretely grasp concepts and principles introduced earlier. Readers need only have a basic knowledge of any structured, high-level language to obtain the full benefits here. Features: *Includes MIPS simulator (SPIM) download instructions, so that readers can get hands-on assembly language programming experience *Presents material in a manner suitable for flexible self-study • Assembly language programs permit reader executables using the SPIM simulator • Integrates core concepts to processor designs and their implementations • Supplies extensive and complete programming examples and figures • Contains chapter-by-chapter overviews and summaries * Provides source code for the MIPS language at the book's website Guide to RISC Processors provides a uniquely comprehensive introduction and guide to RISC-related concepts, principles, design philosophy, and actual programming, as well as the all the popular modern RISC processors and their assembly language. Professionals, programmers, and students seeking an authoritative and practical overview of RISC processors and assembly language programming will find the guide an essential resource. Sivarama P. Dandamudi is a professor of computer science at Carleton University in Ottawa, Ontario, Canada, as well as associate editor responsible for computer architecture at the International Journal of Computers and Their Applications. He has more than two decades of experience teaching about computer systems and organization. Key Topics * Processor design issues * Evolution of CISC and RISC processors * MIPS, SPARC, PowerPC, Itanium, and ARM architectures * MIPS assembly language * SPIM simulator and debugger * Conditional execution * Floating-point and logical and shift operations * Number systems Computer Architecture/Programming Beginning/Intermediate Level. © 2005 Springer Science+Business Media, Inc., All rights reserved. . Keywords:
2005-01-01 - Hardware-software collaborative techniques for runtime profiling and phase transition detection	Dynamic optimization relies on runtime profile information to improve the performance of program execution. Traditional profiling techniques incur significant overhead and are not suitable for dynamic optimization. In this paper, a new profiling technique is proposed, that incorporates the strength of both software and hardware to achieve near-zero overhead profiling. The compiler passes profiling requests as a few bits of information in branch instructions to the hardware, and the processor executes profiling operations asynchronously in available free slots or on dedicated hardware. The compiler instrumentation of this technique is implemented using an Itanium research compiler. The result shows that the accurate block profiling incurs very little overhead to the user program in terms of the program scheduling cycles. For example, the average overhead is 0.6% for the SPECint95 benchmarks. The hardware support required for the new profiling is practical. The technique is extended to collect edge profiles for continuous phase transition detection. It is believed that the hardware-software collaborative scheme will enable many profile-driven dynamic optimizations for EPIC processors such as the Itanium processors. © 2005 Springer Science + Business Media, Inc. . Keywords: Computer hardware; Computer software; Optimization; Phase transitions; Program compilers; Program processors; Scheduling; Dynamic optimizations; Hardware-software collaboration; Phase transition detection; Runtime profiling; Computer supported cooperative work
2005-01-01 - I/O performance characteristics for volume managers on Linux 2.6 servers	This study investigates the I/O performance characteristics of raw volumes in a Linux 2.6 environment running on 64-bit processors (AMD Opteron and Intel Itanium 2). Both sequential and random I/O are examined on RAID-0 devices. The volume managers used are VxVM and Linux LVM2. Synthetic workloads are used in the study. Measurement results are reported. © 2005 Symantec Corporation. All rights reserved. . Keywords: Management; Managers; AMD Opteron; I/O performance; Itanium; Measurement results; Synthetic workloads; Computer operating systems
2005-01-01 - Impact of compiler-based data-prefetching techniques on SPEC OMP application performance	In this paper, we evaluate the benefits achievable from software data-prefetching techniques for OpenMP* C/C++ and Fortran benchmark programs, using the framework of the Intel production compiler for the Intel® Itanium® 2 processor. Prior work on software data-prefetching study has primarily focused on benchmark performance in the context of a few software data-prefetching schemes developed in research compilers. In contrast, our study is to examine the impact of an extensive set of software data-prefetching schemes on the performance of multi-threaded execution using a full set of SPEC OMPM2001 applications with a product compiler on a commercial multiprocessor system. This paper presents performance results showing that compiler-based software data-prefetching supported in the Intel compiler results in significant performance gain, viz., 11.88% to 99.85% gain for 6 out of 11 applications, 3.83% to 6.96% gain for 4 out of 11 applications, with only one application obtaining less than 1% gain on an Intel® Itanium® 2 processor based SGI* Altix* 32-way shared-memory multiprocessor system. . Keywords: C (programming language); Computer software; Data processing; Evaluation; FORTRAN (programming language); Multiprocessing systems; Parallel processing systems; Performance; Compiler optimization; OpenMP; Performance evaluation; Prefetching; Thread-level parallelism; Program compilers
2005-01-01 - Improving GCC instruction scheduling for IA-64	The instruction scheduler is one of the weakest points of GCC on IA-64 architecture. This paper will describe an ongoing project for improving GCC instruction scheduling for Itanium. We aim at adding support for IA-64 control and data speculation to GCC, doing this similarly to the existing implementation of interblock motions. Our work on this issue forces us to address weakness of the alias analysis used in the scheduler. Absence of the address displacement on IA-64 makes the problem even more relevant. We suggest the following improvements: 1) propagation of points-to information from tree-ssa to RTL for pointers, and 2) tracking pointer arithmetic, which is performed for addressing non-pointer variables, in alias.c. . Keywords: Alias analysis; Data speculations; Instruction scheduler; Instruction scheduling; Itanium; Pointer arithmetic; Computer architecture
2005-01-01 - Itanium — A system implementor’s tale	Itanium is a fairly new and rather unusual architecture. Its defining feature is explicitly-parallel instruction-set computing (EPIC), which moves the onus for exploiting instruction-level parallelism (ILP) from the hardware to the code generator. Itanium theoretically supports high degrees of ILP, but in practice these are hard to achieve, as present compilers are often not up to the task. This is much more a problem for systems than for application code, as compiler writers’ efforts tend to be focused on SPEC benchmarks, which are not representative of operating systems code. As a result, good OS performance on Itanium is a serious challenge, but the potential rewards are high. EPIC is not the only interesting and novel feature of Itanium. Others include an unusual MMU, a huge register set, and tricky virtualisation issues. We present a number of the challenges posed by the architecture, and show how they can be overcome by clever design and implementation. © 2005 USENIX Association. All rights reserved. . Keywords: Physical addresses; Program compilers; Application codes; Clever designs; Code generators; Compiler writers; Instruction level parallelism; Parallel instructions; Potential rewards; Virtualisation; Benchmarking
2005-01-01 - Making CSB+-trees processor conscious	Cache-conscious indexes, such as CSB+-tree, are sensitive to the underlying processor architecture. In this paper, we focus on how to adapt the CSB+-tree so that it performs well on a range of different processor architectures. Previous work has focused on the impact of node size on the performance of the CSB+-tree. We argue that it is necessary to consider a larger group of parameters in order to adapt CSB+-tree to processor architectures as different as Pentium and Itanium. We identify this group of parameters and study how it impacts the performance of CSB+-tree on Itanium 2. Finally, we propose a systematic method for adapting CSB+-tree to new platforms. This work is a first step towards integrating CSB+-tree in MySQL's heap storage manager. Copyright 2005 ACM. . Keywords: Architecture; Fuzzy control; Nanotechnology; B+-trees; Cache-conscious; IT impact; Itanium; Pentium; Processor architectures; Storage manager; Systematic method; Computer architecture
2005-01-01 - Module-aware translation for real-life desktop applications	A dynamic binary translator is a just-in-time compiler that translates source architecture binaries into target architecture binaries on the fly. It enables the fast running of the source architecture binaries on the target architecture. Traditional dynamic binary translators invalidate their translations when a module is unloaded, so later reloading of the same module will lead to a full retranslation. Moreover, most of the loading and unloading are performed on a few "hot" modules, which causes the dynamic binary translator to spend a significant amount of time on repeatedly translating these "hot" modules. Furthermore, the retranslation may lead to excessive memory consumption if the code pages containing the translated codes that have been invalidated are not timely recycled. In addition, we observed that the overhead for translating real-life desktop applications is a big challenge to the overall performance of the applications, and our detailed analysis proved that real-life desktop applications dynamically load and unload modules much more frequently as compared to popular benchmarks, such as SPEC CPU2000. To address these issues, we propose a translation reuse engine that uses a novel verification method and a module-aware memory management mechanism. The proposed approach was fully implemented in IA-32 Execution Layer (IA-32 EL) [1], a commercial dynamic binary translator that enables the execution of IA-32 applications on Intel® Itanium® processor family. Collected results show that the module-aware translation improves the performance of Adobe* Illustrator by 14.09% and Microsoft* Publisher by 9.73%. The overhead brought by the translation reuse engine accounts for no more than 0.2% of execution time. Copyright 2005 ACM. . Keywords: Computer applications; Computer architecture; Program compilers; Program processors; Storage allocation (computer); Dynamic loaded module; Memory management; Translation reuse; Real time systems
2005-01-01 - Montecito: A dual-core, dual-thread itanium processor	Intel's Montecito is the first Itanium processor to feature duplicate, dual-thread cores and cache hierarchies on a single die. It features a landmark 1.72 billion transistors and server-focused technologies, and it requires only 100 watts of power. . Keywords: Bandwidth; Cache memory; Servers; Transistors; Demand base switching; Itanium processor; Multithreading; Microprocessor chips
2005-01-01 - MX2 processor module: Twice the processors in half the volume	Computer manufacturer's are constantly trying to tweek more performance out of their existing products by using the highest performing processors. Typically, manufacturers upgrade the platforms by simply replacing the old processor with the latest speed processor. Like other manufacturers, HP generally follows this practice with the exception ot HP's innovative mx2 module. This unique module used two Itanium-2 "Madison" processors packaged in the same physical volume as a single Itanium-2 processor. In addition, the module plugs into a standard Itanium-2 motherboard socket and requires no additional power capacity. As a result, the development team was able get 50% more performance[1] from a socket without increasing power by actively managing the power to the two processors. Thus, the performance per watt was substantially improved. This paper will provide an overview of some of the key packaging and power innovations that made the processor module a reality such as: 1) mezzanine power for space savings. The standard Itanium 2 processor has a power converter adjacent to the processor. HP engineers chose to put power on top of the processor which provided more room but made cooling the processors a challenge. 2) high performance mechnical gap filler. One of the biggest issues in the module was to develop a thermal gap filler that absorbed 0.060" of tolerance between the two processors. The thermal resistance of this technology was an order of magnitude better than anything commercially available in the industry. 3) Power Aware Architecture. This newly developed power mangement technology actively controls power to the processors. When system (thermal and power) extremes were exceeded by worst case abnormal code, the performance was throttled down until the worst case scenario had past. The combination of these advancements has delivered an innovative solution for a highly challenging design problem. This module is now shipping as the mx2 processor module in HP's Integrity Servers and has been viewed as an engineering marvel by HP executives. Copyright © 2005 by ASME. . Keywords: Printed circuit boards; Servers; Systems analysis; Technology transfer; Power mangement technology; Processor module; Program processors
2005-01-01 - N-bit unsigned division via N-bit multiply-add	Integer division on modern processors is expensive compared to multiplication. Previous algorithms for performing unsigned division by an invariant divisor, via reciprocal approximation, suffer in the worst case from a common requirement for n+1 bit multiplication, which typically must be synthesized from n-bit multiplication and extra arithmetic operations. This paper presents, and proves, a hybrid of previous algorithms that replaces n+1 bit multiplication with a single fused multiply-add operation on n-bit operands, thus reducing any n-bit unsigned division to the upper n bits of a multiply-add, followed by a single right shift. An additional benefit is that the prerequisite calculations are simple and fast. On the Itanium® 2 processor, the technique is advantageous for as few as two quotients that share a common run-time divisor. © 2005 IEEE. . Keywords: Algorithms; Approximation theory; Integer programming; Program processors; Bit multiplication; Integer division; Invariant divisor; Reciprocal approximations; Digital arithmetic
2005-01-01 - Near overhead-free heterogeneous thread-migration	Thread migration moves a single call-stack to another machine to improve either load balancing or locality. Current approaches for checkpointing and thread migration are either not heterogeneous or they introduce large runtime overhead. In general, previous approaches add overhead by instrumenting each function in a program. The instrumentation costs are then even incurred when no thread migration is performed. In this respect our system is near-overhead free: nearly no overhead is caused if no migration is performed. Our implementation instead generates metafunctions for each location in the code where a function is called. These functions portably save and rebuild activation records to and from a machine-independent format. Each variable of an activation record is described in terms of its usages in a machine-independent 'Usage Descriptor String' to enable heterogeneous, near overhead free thread migration with as few as possible changes to a compiler. Our resulting thread migration solution is, for example, able to move a thread between an x86 machine (few registers, 32 bits) and an Itanium machine (many registers, 64 bits). Furthermore, we (optionally) move the decision on when and where to migrate to the application programmer instead of implementing a fixed 'fits-all' heuristics as in previous approaches. . Keywords: Check-pointing; Cluster computing; Descriptor; International conferences; Itanium; Load Balancing; Metafunctions; Run-time; Thread migration; Chlorine compounds
2005-01-01 - Neutron ser characterization of microprocessors	Hadrons generated by the primary cosmic rays penetrating the atmosphere have a negative impact on the reliability of semiconductor devices. The electrical charge induced by high energy particles manifests as a current spike and can affect both storage elements and combinational logic. Frequency of occurrence of the errors induced by this failure mechanism is referred to as soft error rate (SER). Continuous shrinking of the electronic devices and the lower supply voltages, in conjunction with the increased complexity of VLSI circuits, have led to higher SER. The impact of semiconductor technology scaling on neutron induced SER is discussed in this report. The experimental methodology and results of accelerated measurements carried out on Intel Itanium® microprocessors, at Los Alamos Neutron Science Center (LANSCE), are presented. Statistically significant values of the MTTF induced by high-energy neutrons are also derived, as a function of the number of upsets observed over the duration of the experiment. The presented approach doesn't require any proprietary data about the microprocessor under evaluation and, as a consequence, can be used as a dependability benchmarking tool both by manufacturers and independent evaluators. © 2005 IEEE. . Keywords: Benchmarking; Cosmic rays; Electric charge; Error detection; Formal logic; Neutrons; Reliability theory; Semiconductor devices; VLSI circuits; Voltage control; Combinational logic; High-energy neutrons; Neutron SER; Semiconductor technology; Microprocessor chips
2005-01-01 - New generation scalable and dependable servers	Today's applications are being built in new ways. The advent of service-oriented architectures, modern networking protocols and industry-standard components at all levels open seemingly endless possibilities for new applications and services. But these new possibilities carry new challenges as well, especially in the area of highperformance, mission-critical, transactional applications, often involving financial and other sensitive data which carries security and auditability requirements. This talk will focus on the particular challenges for servers and how these challenges are being addressed. The NonStop Enterprise Division of HP (formerly Tandem Computers) has built massively parallel, hardware and software fault-tolerant servers since 1975. These servers power many of the most challenging transaction processing environments that exist, including major stock exchanges, funds transfer applications, credit and debit card applications, E911 applications, major telephony and instant messaging applications, and integrated hospital applications. The system is based on a message-oriented architecture and thus bears many similarities internally to the architectural principals being espoused for today's service-oriented architectures. HP is bringing to market a new generation of Integrity NonStop Servers, based on an innovative design using Intel Itanium 2 processors and leveraging many other industry standards. This talk will describe how the new generation of servers can accomplish even higher levels of availability than current systems while at the same time leveraging industry standards and meeting other extreme requirements. The talk will also touch on the lessons learned over the years when dealing with mission-critical applications and how those lessons apply to today's distributed, service-oriented architectures. The underlying message is that careful use of encapsulation, limiting the use of distributed algorithms to carefully controlled situations, and overall design rigor are required even more than ever. © 2005 IEEE. . Keywords: Algorithms; Computer architecture; Computer hardware; Distributed computer systems; Information services; Security of data; Debit card applications; Industry standards; Service-oriented architectures; Transactional applications; Servers
2005-01-01 - NonStop® advanced architecture	For nearly 30 years the Hewlett Packard NonStop Enterprise Division (formerly Tandem Computers Inc.) has produced highly available, fault-tolerant, massively parallel NonStop computer systems. These vertically integrated systems use a proprietary operating system and specialized hardware for detecting, isolating, and recovering from faults. The NonStop Advanced Architecture (NSAA) uses dual or triple modular redundant fault-tolerant servers built from standard HP 4-way SMP Itanium®2 server processor modules, memory boards, and power infrastructure. A unique synchronization mechanism allows fully compared operations from loosely synchronized processor modules. In addition, the NSAA improves system availability by additional hardware fault masking, and significantly lowers cost by leveraging existing high-volume Itanium server components. © 2005 IEEE. . Keywords: Computer operating systems; Fault tolerant computer systems; Hardware; Program processors; Servers; Storage allocation (computer); Fault-tolerant servers; Memory boards; NonStop Advanced Architecture (NSAA); Server processor modules; Computer architecture
2005-01-01 - Optimizing structures in object oriented programs	In this paper we demonstrate that effective structure optimization is essential to improve code quality and reduce compilation overhead for object-oriented programs. We propose to address this problem by using an effective representation of structure operation, folding indirect memory accesses to structure fields, flattening structures judiciously, and allowing more aggressive procedure inlining, These techniques enable the existing scalar optimizations, which were well tuned for the traditional imperative languages, to work effectively on object-oriented programs, allowing them to make better use of the performance enhancing-features available on modern processors. We have implemented this strategy in an SSA based global optimization framework in the Open Research Compiler, targeting the Itanium Processor Family. The experimental results with representative C++ benchmarks show that the applications' performance can be improved significantly. For instance, Eon's performance is improved by 35.6%, while the execution time of the Stepanov benchmark is reduced by a factor of 24. . Keywords: Codes (symbols); Computer programming languages; Data storage equipment; Optimization; Problem solving; Program compilers; Program processors; Code quality; Compilation; Optimizing structures; Scalar optimizations; Object oriented programming
2005-01-01 - Pairwise local structural alignment of RNA sequences with sequence similarity less than 40%	Motivation: Searching for non-coding RNA (ncRNA) genes and structural RNA elements (eleRNA) are major challenges in gene finding today as these often are conserved in structure rather than in sequence. Even though the number of available methods is growing, it is still of interest to pairwise detect two genes with low sequence similarity, where the genes are part of a larger genomic region. Results: Here we present such an approach for pairwise local alignment which is based on FOLDALIGN and the Sankoff algorithm for simultaneous structural alignment of multiple sequences. We include the ability to conduct mutual scans of two sequences of arbitrary length while searching for common local structural motifs of some maximum length. This drastically reduces the complexity of the algorithm. The scoring scheme includes structural parameters corresponding to those available for free energy as well as for substitution matrices similar to RIBOSUM. The new FOLDALIGN implementation is tested on a dataset where the ncRNAs and eleRNAs have sequence similarity <40% and where the ncRNAs and eleRNAs are energetically indistinguishable from the surrounding genomic sequence context. The method is tested in two ways: (1) its ability to find the common structure between the genes only and (2) its ability to locate ncRNAs and eleRNAs in a genomic context. In case (1), it makes sense to compare with methods like Dynalign, and the performances are very similar, but FOLDALIGN is substantially faster. The structure prediction performance for a family is typically around 0.7 using Matthews correlation coefficient. In case (2), the algorithm is successful at locating RNA families with an average sensitivity of 0.8 and a positive predictive value of 0.9 using a BLAST-like hit selection scheme. © The Author 2005. Published by Oxford University Press. All rights reserved. . Keywords: untranslated RNA; article; computer program; controlled study; correlation coefficient; gene identification; gene location; genetic algorithm; genetic selection; Human immunodeficiency virus 1; intermethod comparison; multigene family; nonhuman; nucleotide sequence; prediction; priority journal; RNA sequence; RNA structure; scoring system; sensitivity analysis; sequence alignment; sequence homology
2005-01-01 - Performance improvements for GCC using architecture features on IA-64	The Intel IA-64 architecture provides a rich set of features to aid the compiler in exploiting instruction-level parallelism to achieve high performance. Currently, GCC is a widely used open source compiler on IA-64 platform, but its performance, especially floating-point performance is poor compared with commercial compilers because it has not fully utilized those features. We have been absorbed in improving performance of GCC on IA-64 architecture since late 2003. This paper reports some of our work on several algorithms concerning with architecture features of IA-64 for achieving higher floating-point performance of GCC, including FORTRAN alias analysis, induction variable optimizations, loop unrolling, and prefetch of loop arrays. These improvements have markedly optimized the floatingpoint performance of GCC on Itanium-2 systems. . Keywords: Digital arithmetic; Optimization; Program compilers; Alias analysis; Floatingpoint; Improving performance; Induction variables; Instruction level parallelism; Loop arrays; Loop unrolling; Open sources; Performance improvements; Prefetches; Computer architecture
2005-01-01 - Performance of OSCAR multigrain parallelizing compiler on SMP servers	This paper describes performance of OSCAR multigrain parallelizing compiler on various SMP servers, such as IBM pSeries 690, Sun Fire V880, Sun Ultra 80, NEC TX7/i6010 and SGI Altix 3700. The OSCAR compiler hierarchically exploits the coarse grain task parallelism among loops, subroutines and basic blocks and the near fine grain parallelism among statements inside a basic block in addition to the loop parallelism. Also, it allows us global cache optimization over different loops, or coarse grain tasks, based on data localization technique with inter-array padding to reduce memory access overhead. Current performance of OSCAR compiler is evaluated on the above SMP servers. For example, the OSCAR compiler generating OpenMP parallelized programs from ordinary sequential Fortran programs gives us 5.7 times speedup, in the average of seven programs, such as SPEC CFP95 tomcatv, swim, su2cor, hydro2d, mgrid, applu and turb3d, compared with IBM XL Fortran compiler 8.1 on IBM pSeries 690 24 processors SMP server. Also, it gives us 2.6 times speedup compare with Intel Fortran Itanium Compiler 7.1 on SGI Altix 3700 Itanium 2 16 processors server, 1.7 times speedup compared with NEC Fortran Itanium Compiler 3.4 on NEC TX7/i6010 Itanium 2 8 processors server, 2.5 times speedup compared with Sun Forte 7.0 on Sun Ultra 80 UltraSPARC II 4 processors desktop workstation, and 2.1 times speedup compare with Sun Forte compiler 7.1 on Sun Fire V880 UltraSPARC III Cu 8 processors server. © Springer-Verlag Berlin Heidelberg 2005. . Keywords: Computer software; Hierarchical systems; High level languages; Performance; Program compilers; Servers; Coarse grain; Compiler hierarchy; Loops; Parallelizing compiler; Parallel processing systems
2005-01-01 - Pin: Building customized program analysis tools with dynamic instrumentation	Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level with-out the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium®, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website. Copyright 2005 ACM. . Keywords: Computer architecture; Performance; Program compilers; Robustness (control systems); Websites; Dynamic compilation; Instrumentation; Program analysis tools; Computer programming
2005-01-01 - Pin: Building customized program analysis tools with dynamic instrumentation	Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium®, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website. Copyright 2005 ACM. . Keywords: Computer architecture; Computer operating systems; Computer programming; Computer software; Robustness (control systems); Dynamic compilation; Instrumentation system; Linux platforms; Pintools; Program analysis tools; Computer aided software engineering
2005-01-01 - Power and temperature control on a 90nm itanium®-family processor	[No abstract available] . Keywords:
2005-01-01 - Power and temperature control on a 90nm Itanium®-family processor	This paper describes the embedded feedback and control system on a 90nm Itanium®-family processor, code-named Montecito, that maximizes performance while staying within a target power and temperature (PT) envelope. This system utilizes on-chip sensors and an embedded micro-controller to measure PT and modulate voltage and frequency to meet PT constraints. © 2005 IEEE. . Keywords: Constraint theory; Control systems; Electric potential; Feedback control; Microcontrollers; Power control; Sensors; Temperature control; Embedded feedback; Modulate voltage; On-chip sensors; Target power; Program processors
2005-01-01 - Power optimization for predicated execution	Predicated execution improves the performance by eliminating branches but wastes energy due to extra instructions. Nullifying FALSE-predicted instruction earlier can save energy. Based on the Itanium 2 microprocessor, we modify the pipeline function; advance the read of predicate and present methods to stall the pipeline on predicates dependency. Simulation results show that the advanced predicate reading and FALSE-predicated instruction nullifying can reduce energy waste and improve the efficiency of energy. . Keywords:
2005-01-01 - Practical compiler techniques on efficient multithreaded code generation for OpenMP programs	State-of-the-art multiprocessor systems pose several difficulties: (i) the user has to parallelize the existing serial code; (ii) explicitly threaded programs using a thread library are not portable; (iii) writing efficient multi-threaded programs requires intimate knowledge of machine's architecture and micro-architecture. Thus, well-tuned parallelizing compilers are in high demand to leverage state-of-the-art computer advances of NUMA-based multiprocessors, simultaneous multi-threading processors and chip-multiprocessor systems in response to the performance quest from the high-performance computing community. On the other hand, OpenMP* has emerged as the industry standard parallel programming model. Applications can be parallelized using OpenMP with less effort in a way that is portable across a wide range of multiprocessor systems. In this paper, we present several practical compiler optimization techniques and discuss their effect on the performance of OpenMP programs. We elaborate on the major design considerations in a high performance OpenMP compiler and present experimental data based on the implementation of the optimizations in the Intel® C++ and Fortran compilers. Interactions of the OpenMP transformation with other sequential optimizations in the compiler are discussed. The techniques in this paper have achieved significant performance improvements on the industry standard SPEC* OMPM2001 and SPEC* OMPL2001 benchmarks, and these performance results are presented for Intel® Pentium® and Itanium® processor based systems. © The Author 2005. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved. . Keywords: C (programming language); FORTRAN (programming language); Multiprocessing programs; Multiprocessing systems; Open systems; Optimization; Parallel processing systems; Compiler optimization; Multithreaded code generation; Mutithreading processors; Parallelizing compilers; Program compilers
2005-01-01 - Pre-silicon validation of IPF memory ordering for multi-core processors	This paper presents a pre-silicon validation methodology of Intel® Itanium® Processor Family (IPF) memory ordering [2] for multi-core processors. The validation methodology includes a multi-core simulation environment, a shared memory multi-processor reference model, memory ordering checkers, and a tightly combined strategy of stimulus and coverage, specifically developed for IPF memory ordering. The latest result showed that memory ordering specific focused tests and pseudo-random exercisers were very effective in finding memory ordering bugs in the pre-Silicon validation stage. © 2005 IEEE. . Keywords: Computer simulation; Program debugging; Program processors; Silicon; Memory ordering checkers; Multi-processor reference model; Pseudo-random exercisers; Storage allocation (computer)
2005-01-01 - Proceedings 9th Annual Workshop on Interaction between Compilers and Computer Architectures INTERACT-9	The proceedings contain 9 papers. The topics discussed include: compiler analysis for trace-level speculative multithreaded architectures; multigrain parallel processing on compiler cooperative chip multiprocessor; an empirical study of data speculation use on the Intel Itanium 2 processor; analysis of path planning information generated with performance monitoring hardware; cooperative caching with keep-me and evict-me; hybrid compiler and microarchitecture technique for cache traffic optimization; a tile size selection analysis for blocked array layouts; automatic low overhead program instrumentation with the LOPI framework; and optimizing structures in object oriented programs. . Keywords: Computer architecture; Integrated circuits; Microprocessor chips; Multitasking; Optimization; Telecommunication traffic; Cache traffic optimization; Microarchitecture; Multithreaded architectures; Program compilers
2005-01-01 - RAxML-OMP: An efficient program for phylogenetic inference on SMPs	Inference of phylogenetic trees comprising hundreds or even thousands of organisms based on the Maximum Likelihood (ML) method is computationally extremely intensive. In order to accelerate computations we implemented RAxML-OMP, an efficient OpenMP-parallelization for Symmetric Multi-Processing machines (SMPs) based on the sequential program RAxML-V (Randomized Axelerated Maximum Likelihood). RAxML-V is a program for inference of evolutionary trees based upon the ML method and incorporates several advanced search algorithms like fast hill-climbing and simulated annealing. We assess performance of RAxML-OMP on the widely used Intel Xeon, Intel Itanium, and AMD Opteron architectures. RAxML-OMP scales particularly well on the AMD Opteron architecture and achieves even super-linear speedups for large datasets (with a length ≥ 5.000 base pairs) due to improved cache-efficiency and data locality. RAxML-OMP is freely available as open source code. © Springer-Verlag Berlin Heidelberg 2005. . Keywords: Buffer storage; Computer architecture; Mathematical models; Parallel algorithms; Maximum likelihood (ML); OpenMP-parallelization; Phylogenetic trees; Symmetric multiprocessing (SMP) machines; Computer program listings
2005-01-01 - Register allocation for software pipelined multi-dimensional loops	Software pipelining of a multi-dimensional loop is an important optimization that overlaps the execution of successive outermost loop iterations to explore instruction-level parallelism from the entire n-dimensional iteration space. This paper investigates register allocation for software pipelined multi-dimensional loops. For single loop software pipelining, the lifetime instances of a loop variant in successive iterations of the loop form a repetitive pattern. An effective register allocation method is to represent the pattern as a vector of lifetimes (or a vector lifetime using Rau's terminology) and map it to rotating registers. Unfortunately, the software pipelined schedule of a multi-dimensional loop is considerably more complex, and so are the vector lifetimes in it. In this paper, we develop a way to normalize and represent vector lifetimes in multi-dimensional loop software pipelining, which capture their complexity, while exposing their regularity that enables us to develop a simple, yet powerful solution. Our algorithm is based on the development of a metric, called distance, that quantitatively determines the degree of potential overlapping (conflicts) between two vector lifetimes. We show how to calculate and use the distance, conservatively or aggressively, to guide the register allocation of the vector lifetimes under a bin-packing algorithm framework. The classical register allocation for software pipelined single loops is subsumed by our method as a special case. The method has been implemented in the ORC compiler and produced code for the Itanium architecture. We report the effectiveness of our method on 134 loop nests with 348 loop levels. Several strategies for register allocation are compared and analyzed. Copyright 2005 ACM. . Keywords: Algorithms; Computer software; Iterative methods; Optimization; Pipeline processing systems; Program compilers; Itanium architecture; Parallelism; Register Allocation; Software Pipelining; Storage allocation (computer)
2005-01-01 - Register allocation for software pipelined multi-dimensional loops	Software pipelining of a multi-dimensional loop is an important optimization that overlaps the execution of successive outermost loop iterations to explore instruction-level parallelism from the entire n-dimensional iteration space. This paper investigates register allocation for software pipelined multi-dimensional loops. For single loop software pipelining, the lifetime instances of a loop variant in successive iterations of the loop form a repetitive pattern. An effective register allocation method is to represent the pattern as a vector of lifetimes (or a vector lifetime using Rau's terminology) and map it to rotating registers. Unfortunately, the software pipelined schedule of a multi-dimensional loop is considerably more complex, and so are the vector lifetimes in it. In this paper, we develop a way to normalize and represent vector lifetimes in multi-dimensional loop software pipelining, which capture their complexity, while exposing their regularity that enables us to develop a simple, yet powerful solution. Our algorithm is based on the development of a metric, called distance, that quantitatively determines the degree of potential overlapping (conflicts) between two vector lifetimes. We show how to calculate and use the distance, conservatively or aggressively, to guide the register allocation of the vector lifetimes under a bin-packing algorithm framework. The classical register allocation for software pipelined single loops is subsumed by our method as a special case. The method has been implemented in the ORC compiler and produced code for the Itanium architecture. We report the effectiveness of our method on 134 loop nests with 348 loop levels. Several strategies for register allocation are compared and analyzed. Copyright 2005 ACM. . Keywords: Algorithms; Computational complexity; Iterative methods; Program compilers; Vectors; Register allocation; Software pipelining; Computer software
2005-01-01 - Scalable massively parallel artificial neural networks	There is renewed interest in computational intelligence, due to advances in algorithms, neuroscience, and computer hardware. In addition there is enormous interest in autonomous vehicles (air, ground, and sea) and robotics, which need significant onboard intelligence. Work in this area could not only lead to better understanding of the human brain but also very useful engineering applications. The functioning of the human brain is not well understood, but enormous progress has been made in understanding it and, in particular, the neocortex. There are many reasons to develop models of the brain. Artificial Neural Networks (ANN), one type of model, can be very effective for pattern recognition, function approximation, scientific classification, control, and the analysis of time series data. ANNs often use the back-propagation algorithm for training, and can require large training times especially for large networks, but there are many other types of ANNs. Once the network is trained for a particular problem, however, it can produce results in a very short time. Parallelization of ANNs could drastically reduce the training time. An object-oriented, massively-parallel ANN (Artificial Neural Network) software package SPANN (Scalable Parallel Artificial Neural Network) has been developed and is described here. MPI was used to parallelize the C++ code. Only the neurons on tlie edges of the domains were involved in communication, in order to reduce the communication costs and maintain scalability. The back-propagation algorithm was used to train the network. In preliminary tests, the software was used to identify character sets. The code correctly identified all the characters wlien adequate training was used in the network. The code was run on up to 500 Intel Itanium processors with 25,000 neurons and more than 2 billion neuron weights. Various comparisons in training time, forward propagation time, and error reduction were also made. . Keywords: Algorithms; Artificial intelligence; Brain; Computer programming languages; Computer software; Robotics; Autonomous vehicles; Communication costs; Error reduction; Forward propagation time; Neural networks
2005-01-01 - Seven-O'Clock: A new distributed GVT algorithm using network atomic operations	In this paper we introduce a new concept, network atomic operations (NAOs) to create a zero-cost consistent cut. Using NAOs, we define a wall-clock-time driven GVT algorithm called Seven O'Clock that is an extension of Fujimoto's shared memory GVT algorithm. Using this new GVT algorithm, we report good optimistic parallel performance on a cluster of state-of-the-art Itanium-II quad processor systems for both benchmark applications such as PHOLD and real-world applications such as a large-scale TCP/Internet model. In some cases, super-linear speedup is observed. © 2005 IEEE. . Keywords: Algorithms; Benchmarking; Computer science; Data storage equipment; Internet; Mathematical models; Performance; Program processors; GVT algorithm; Itanium-II quad processor systems; Network atomic operations (NAO); TCP/Internet model; Distributed computer systems
2005-01-01 - Some functions computable with a fused-mac	The fused multiply accumulate instruction (fused-mac) that is available on some current processors such as the Power PC or the Itanium eases some calculations. We give examples of some floating-point functions (such as ulp(x) or Nextafter(x, y)), or some useful tests, that are easily computable using a fused-mac. Then, we show that, with rounding to the nearest, the error of a fused-mac instruction is exactly representable as the sum of two floating-point numbers. We give an algorithm that computes that error. © 2005 IEEE. . Keywords: Error analysis; Functions; Number theory; Personal computers; Floating point numbers; Floating-point functions; Fused-mac interaction; Program processors
2005-01-01 - SWIFT: Software implemented fault tolerance	To improve performance and reduce power, processor designers employ advances that shrink feature sizes, lower voltage levels, reduce noise margins, and increase clock rates. However, these advances make processors more susceptible to transient faults that can affect correctness. While reliable systems typically employ hardware techniques to address soft-errors, software techniques can provide a lower-cost and more flexible alternative. This paper presents a novel, software-only, transient-fault-detection technique, called SWIFT. SWIFT efficiently manages redundancy by reclaiming unused instruction-level resources present during the execution of most programs. SWIFT also provides a high level of protection and performance with an enhanced control-flow checking mechanism. We evaluate an implementation of SWIFT on an Itanium 2 which demonstrates exceptional fault coverage with a reasonable performance cost. Compared to the best known single-threaded approach utilizing an ECC memory system, SWIFT demonstrates a 51% average speedup. . Keywords: Computer hardware; Computer software; Costs; Program processors; Spurious signal noise; Storage allocation (computer); Control-flow checking mechanism; SWIFT; Transient-fault-detection technique; Fault tolerant computer systems
2005-01-01 - The asynchronous 24MB on-chip level-3 cache for a dual-core itanium®-family processor	The 24MB level-3 cache on a dual-core Itanium® processor has more than 1.47G transistors. The cache uses an asynchronous design to reduce latency and power, and it includes other power saving and reliability improvement features. The 5-cycle array operates above 2GHz at 0.8V and 85°C while consuming less than 4.2W. © 2005 IEEE. . Keywords: Asynchronous machinery; Energy conservation; Program processors; Transistors; Asynchronous design; Dual-core; Power saving; Buffer storage
2005-01-01 - The asynchronous 24MB on-chip level-3 cache for a dual-core itanium®-family processor	[No abstract available] . Keywords:
2005-01-01 - The implementation of a 2-core multi-threaded itanium®-family processor	The next generation in the Itanium® processor family, code named Montecito, is introduced. Implemented in a 90nm 7M process, the processor has two dual-threaded cores integrated with 26.5MB of cache. Of the total of 1.72B transistors, 64M are dedicated to logic and the rest to cache. With both cores operating at full speed, the chip consumes 100W. © 2005 IEEE. . Keywords: Buffer storage; Formal logic; Integrated circuits; Transistors; Dual-threaded cores; Itanium processor family; Program processors
2005-01-01 - The implementation of a 2-core multi-threaded Itanium®-family processor	[No abstract available] . Keywords:
2005-01-01 - The implementation of a 2-core, multi-threaded itanium® family processor	[No abstract available] . Keywords:
2005-01-01 - The multi-threaded, parity-protected 128-word register files on a dual-core itanium®-family processor	[No abstract available] . Keywords: Electric charge; Error analysis; Integer programming; Natural frequencies; Integer register file; Parity; Thread switch; Microprocessor chips
2005-01-01 - The Open Runtime Platform: A flexible high-performance managed runtime environment	The Open Runtime Platform (ORP) is a high-performance managed runtime environment (MRTE) that features exact generational garbage collection, fast thread synchronization, and multiple coexisting just-in-time compilers (JITs). ORP was designed for flexibility in order to support experiments in dynamic compilation, garbage collection, synchronization, and other technologies. It can be built to run either Java or Common Language Infrastructure (CLI) applications, to run under the Windows or Linux operating systems, and to run on the IA-32 or Itanium processor family (IPF) architectures. Achieving high performance in a MRTE presents many challenges, particularly when flexibility is a major goal. First, to enable the use of different garbage collectors and JITs, each component must be isolated from the rest of the environment through a well-defined software interface. Without careful attention, this isolation could easily harm performance. Second, MRTEs have correctness and safety requirements that traditional languages such as C++ lack. These requirements, including null pointer checks, array bounds checks, and type checks, impose additional runtime overhead. Finally, the dynamic nature of MRTEs makes some traditional compiler optimizations, such as devirtualization of method calls, more difficult to implement or more limited in applicability. To get full performancě, JITs and the core virtual machine (VM) must cooperate to reduce or eliminate (where possible) these MRTE-specific overheads. In this paper, we describe the structure of ORP in detail, paying particular attention to how it supports flexibility while preserving high performance. We describe the interfaces between the garbage collector, the JIT, and the core VM; how these interfaces enable multiple garbage collectors and JITs without sacrificing performance; and how they allow the JIT and the core VM to reduce or eliminate MRTE-specific performance issues. Copyright © 2005 John Wiley & Sons, Ltd. . Keywords: Computer architecture; Computer operating systems; Computer software; Data storage equipment; Interfaces (computer); Java programming language; Network protocols; Optimization; Interface design; Management runtime environment (MRTE); Open runtime platform; Virtual machines; Program compilers
2005-01-01 - Three algorithms for improving the effectiveness of software pipelining: Comparison and combination	Software pipelining is a loop scheduling technique that extracts instruction level parallelism by overlapping the execution of several consecutive iterations. One of its drawbacks is the high register requirements, which may lead to software pipelining failure due to insufficient static general registers in Itanium. This paper evaluates the register requirements of software-pipelined loops and presents three new methods for software pipelining loops that require more static general registers than those available in Itanium processor. They reduce register pressure by either reducing instructions in the loop body or allocating stacked non-rotating registers or rotating register in register stack to serve as static registers. These methods are better than the existing techniques in that they further improve performance gain from software pipelining by increasing software-pipelined loops. These methods have been implemented in open research compiler (ORC) targeted for Itanium processor, and they perform well on loops of the programs in NAS Benchmarks. For some benchmarks, the performance is improved by more than 21%. . Keywords: Algorithms; Optimization; Resource allocation; Scheduling; Itanium processor; Loop unrolling; Register allocation; Software pipelining; Static register; Static variant; Parallel processing systems
2005-01-01 - Unpredication, unscheduling, unspeculation: Reverse engineering itanium executables	EPIC(Explicitly Parallel Instruction Computing) architectures, exemplified by the Intel Itanium, support a number of advanced architectural features, such as explicit instruction-level parallelism, instruction predication, and speculative loads from memory. However, compiler optimizations that take advantage of these features can profoundly restructure the program's code, making it potentially difficult to reconstruct the original program logic from an optimized Itanium executable. This paper describes techniques to undo some of the effects of such optimizations and thereby improve the quality of reverse engineering such executables. © 2005 IEEE. . Keywords: Algorithms; Computer software; Optimization; Parallel processing systems; Program compilers; Code optimization; Explicitly parallel instruction computing (EPIC) architectures; Instruction predication; Speculative loads; Reverse engineering
2006-01-01 - A 90-nm variable frequency clock system for a power-managed itanium architecture processor	An Itanium Architecture microprocessor in 90-nm CMOS with 1.7B transistors implements a dynamically-variable-frequency clock system. Variable frequency clocks support a power management scheme which maximizes processor performance within a configured power envelope. Core supply voltage and clock frequency are modulated dynamically in order to remain within the power envelope. The Foxton controller and dynamically-variable clock system reside on die while the variable voltage regulator and power measurement resistors reside off chip. In addition, high-bandwidth frequency adjustment allows the clock period to adapt during on-die supply transients, allowing higher frequency processor operation during transients than possible with a single-frequency clock system. . Keywords: Bandwidth; CMOS integrated circuits; Electric frequency control; Electric power measurement; Transistors; Voltage regulators; CMOS; Configured power envelope; Foxton controller; Frequency adjustment; Itanium architecture processor; Power management scheme; Power measurement resistors; Power-managed processor; Supply transients; Variable frequency clock; Microprocessor chips
2006-01-01 - A fast RSA implementation on itanium 2 processor	We show the fastest implementation result of RSA on Itanium 2. For realizing the fast implementation, we improved the implementation algorithm of Montgomery multiplication proposed by Itoh et al. By using our implementation algorithm, pilepine delay is decreased than previous one on Itanium 2. And we implemented this algorithm with highly optimized for parallel processing. Our code can execute 4 instructions per cycle (At maximum, 6 instructions are executed per cycle on Itanium 2), and its probability of pipeline stalling is just only 5%. Our RSA implementation using this code performs 32 times per second of 4096-bit RSA decryption with CRT on Itanium 2 at 900MHz. As a result, our implementation of RSA is the fastest on Itanium2. This is 3.1 times faster than IPP, a software library developed by Intel, in the best case. © Springer-Verlag Berlin Heidelberg 2006. . Keywords: Computer science; Computers; Fast implementation; Implementation algorithms; Instructions per cycles; Itanium; Itanium 2 processor; Montgomery multiplication; Parallel processing; Software implementation; Artificial intelligence
2006-01-01 - A Lifetime Optimal Algorithm for Speculative PRE	A lifetime optimal algorithm, called MC-PRE, is presented for the first time that performs speculative PRE based on edge profiles. In addition to being computationally optimal in the sense that the total number of dynamic computations for an expression in the transformed code is minimized, MC-PRE is also lifetime optimal since the lifetimes of introduced temporaries are also minimized. The key in achieving lifetime optimality lies not only in finding a unique minimum cut on a transformed graph of a given CFG, but also in performing a data-flow analysis directly on the CFG to avoid making unnecessary code insertions and deletions. The lifetime optimal results are rigorously proved. We evaluate our algorithm in GCC against three previously published PRE algorithms, namely,MC-PREcomp (Qiong and Xue’s computationally optimal version of MC-PRE), LCM (Knoop, R¨ uthing, and Steffen’s lifetime optimal algorithm for performing nonspeculative classic PRE), and CMP-PRE (Bodik, Gupta, and Soffa’s PRE algorithm based on code-motion preventing (CMP) regions, which is speculative but not computationally optimal). We report and analyze our experimental results, obtained from both actual program execution and instrumentation, for all 22 C, C++ and FORTRAN 77 benchmarks from SPECcpu2000 on an Itanium 2 computer system. Our results show that MC-PRE (or MC-PREcomp) is capable of eliminating more partial redundancies than both LCM and CMP-PRE (especially in functions with complex control flow), and, in addition, MC-PRE inserts temporaries with shorter lifetimes than MC-PREcomp. Each of both benefits has contributed to the performance improvements in benchmark programs at the costs of only small compile-time and code-size increases in some benchmarks. © 2006, ACM. All rights reserved. . Keywords:
2006-01-01 - A report on the progress of GNU Modula-2 and its potential integration into GCC	This paper reports on the status of the GNU Modula-2 front end to GCC and the extensions made to Modula-2 and gdb to ease its potential integration into the main GCC source tree. GNU Modula-2 (gm2) is maturing into a reliable tool and it now builds and passes its regression tests on the following platforms: x86, Opteron, Athlon 64, Alpha, Itanium processors running GNU/Linux, Sparc based Solaris, PowerPC MacOS, x86 Open Darwin and the x86 processor running FreeBSD. GNU Modula-2 currently conforms to all three Programming in Modula-2 dialects as defined by Wirth. The paper also describes the two categories of language extensions made. The first category follows the tradition of other GCC front ends by allowing the in-lining of assembly language, conditional compilation, procedure in-lining and allowing users to cleanly exploit the GCC library of built-in functions and constants. The second category provide easy access to C libraries. The work presented here discusses the portable implementation of open arrays, module priorities, coroutine primitives and multi-word sets. It also reports on many of the key design decisions taken during the construction of GNU Modula-2 and their various implications. . Keywords: Assembly language; Athlon 64; Built-in functions; Coroutine; Design decisions; FreeBSD; Front end; GNU/LINUX; Itanium processor; Language extensions; Multi-word; Open arrays; Opteron; Portable implementation; PowerPC; Regression tests; Solaris; Source tree; Modula (programming language)
2006-01-01 - Adapting EPIC architecture's register stack for virtual stack machines	The register stack (RS) is a major component of the Explicit Parallel Instruction Computer (EPIC) architecture. In this paper, our objective is to close the theoretical performance gap between EPIC and stack processors running virtual stack machines - using Forth, a simple and canonical stack machine. For this purpose, we first introduce a new calling mechanism using the RS to implement a software-only virtual stack machine. Based upon our performance measurements, we show that the new calling mechanism is a promising technique to improve the performance of stack-based interpretative virtual machines. But limitation in EPIC makes the need for hardware support to reach optimal performance. As a second step, we define an addition to Itanium 2 processor's instruction set to accommodate the new calling mechanism. As our third and last step, we describe a conservative architectural implementation of the extended instruction set. © 2006 IEEE. . Keywords: Computer hardware; Program processors; Register stack (RS); Virtual stack machines; Computer architecture
2006-01-01 - An efficient memory operations optimization technique for vector loops on Itanium 2 processors	To keep up with a large degree of instruction level parallelism (ILP), the Itanium 2 cache systems use a complex organization scheme: load/store queues, banking and interleaving. In this paper, we study the impact of these cache systems on memory instructions scheduling. We demonstrate that, if no care is taken at compile time, the non-precise memory disambiguation mechanism and the banking structure cause severe performance loss, even for very simple regular codes. We also show that grouping the memory operations in a pseudo-vectorized way enables the compiler to generate more effective code for the Itanium 2 processor. The impact of this code optimization technique on register pressure is analyzed for various valorization schemes. Copyright © 2006 John Wiley & Sons, Ltd. . Keywords: Codes (symbols); Optimization; Parallel processing systems; Performance; Program processors; Vectors; Bank conflicts; Cache optimization; Instruction level parallelism; Memory access optimization; Memory address disambiguation; Performance measurement; Buffer storage
2006-01-01 - Analysis and characterization of Intel® Itanium® instruction bundles for improving VLIW processor performance	In order to achieve high instruction level parallelism (ILP), designers are turning to very long instruction word (VLIW) based designs, in which different types of instructions are grouped together as bundles of 128 bits or longer. In VLIW, the added nops increase the code size, limit processor performance by the wider-utilization of functional units. In examining these performance issues of VLIW systems, we consider Intel first 64-bit architecture, the IA-64, and its first implementation, the Itanium, which employs Intel version of VLIW. We present a comprehensive analysis of the problem of wider-utilization due to nops and stops across a wide range of application domains through the use of three different benchmark suites: SPEC CPU 2000, MediaBench, and PacketBench. Our results show that, on average, nops create an under-utilization factor of 28.46% in the case of SPEC CPU. 32.27% in MediaBench, and 29.76% in PacketBench. We also analyze the characteristics of different instruction bundle formats, which we obtain by collecting statistics concerning the frequency of the bundle formats. © 2006 IEEE. . Keywords: Codes (symbols); Computer architecture; Integrated circuit layout; Benchmark suites; Instruction bundles; Microprocessor chips
2006-01-01 - Application of LADA for post-silicon test content and diagnostic tool validation	Embedded cache size has dramatically increased with the advent of Intel Hyper-Threading and Multi-Core Technology, making many of the existing cache test validation method less and less practical, if not obsolete. As a result, the effort to sustain and improve array test quality, which is ever so critical to achieve DPM goals, is becoming a formidable challenge. In this paper, we present a test content validation procedure through novel application of Laser Assisted Device Alteration (LADA), i.e. soft fault injection in state elements, which had proven itself in Itanium® 2 array test quality improvement. While the procedure was originally targeting cache test content, the underlying concept has been successfully deployed to expedite scan test content and fault isolation tool validation. Copyright © 2006 ASM International®. . Keywords: Cache memory; Equipment testing; Failure analysis; Silicon; Fault isolation tool validation; Laser Assisted Device Alteration (LADA); Soft fault injection; Test quality; Laser applications
2006-01-01 - Beating in-order stalls with "Flea-Flicker" two-pass pipelining	While compilers have generally proven adept at planning useful static instruction-level parallelism for in-order microarchitectures, the efficient accommodation of unanticipable latencies, like those of load instructions, remains a vexing problem. Traditional out-of-order execution hides some of these latencies, but repeats scheduling work already done by the compiler and adds additional pipeline overhead. Other techniques, such as prefetching and multithreading, can hide some anticipable, long-latency misses, but not the shorter, more diffuse stalls due to difficult-to-anticipate, first or second-level misses. Our work proposes a microarchitectural technique, two-pass pipelining, whereby the program executes on two in-order back-end pipelines coupled by a queue. The "advance" pipeline often defers instructions dispatching with unready operands rather than stalling. The "backup" pipeline allows concurrent resolution of instructions deferred by the first pipeline allowing overlapping of useful "advanced" execution with miss resolution. An accompanying compiler technique and instruction marking further enhance the handling of miss latencies. Applying our technique to an Itanium 2-like design achieves a speedup of 1.38x in mcf, the most memory-intensive SPECint2000 benchmark, and an average of 1.12x across other selected benchmarks, yielding between 32 percent and 67 percent of an idealized out-of-order design's speedup at a much lower design cost and complexity. © 2006 IEEE. . Keywords: Benchmarking; Computer architecture; Cost effectiveness; Problem solving; Program compilers; Cache-miss tolerance; Out-of-order execution; Prefetching; Runahead execution; Computer networks
2006-01-01 - Capturing register and control dependence in memory consistency models with applications to the itanium architecture	A complete framework for modelling memory consistency that includes register and control dependencies is presented. It allows us to determine whether or not a given computation could have arisen from a given program running on a given multiprocessor architecture. The framework is used to provide an exact description of the computations of (a subset of) the Itanium instruction set on an Itanium multiprocessor architecture. We show that capturing register and control dependencies is crucial: a producer/consumer problem is solvable without using strong synchronization primitives on Itanium multiprocessors, but is impossible without exploiting these dependencies. © Springer-Verlag Berlin Heidelberg 2006. . Keywords: Computation theory; Distributed computer systems; Program processors; Synchronization; Multiprocessor architecture; Multiprocessor memory consistency; Process coordination; Computer architecture
2006-01-01 - Customizing the redesignation process for individual platform interrupts on a soft-partitioned system	This disclosure relates to the field of operating systems. A technique is disclosed that allows the redesignation process that changes the designated partition on a soft-partitioned system to take actions that are specific to a particular platform interrupt source. A soft-partitioning product supports running multiple operating system instances in individual soft-partitions on a single, physical server The configuration and management of the individual soft-partitions is done by a soft-partition monitor that, on some platforms such as Itanium platforms, runs in an administrative soft-partition and launches individual soft partitions with support from firmware. On such a system, multiple operating system instances will share platform interrupt sources. The soft-partition monitor will allow only one operating system instance (denoted as the designated OS) to program the interrupt controller for a particular platform interrupt source. The designated OS will then handle all the hardware interrupts from that source The soft-partition monitor will manage all the book-keeping associated with tracking which OS instance is handling which interrupt source, and will forward interrupts to the appropriate soft-partition instances for handling by the associated driver's interrupt service routine. When the designated partition on a soft-partitioned system is no longer capable of handling interrupts (due to, for example, a crash, a reboot, a system shutdown, etc.), a new designated partition is chosen via a process known as redesignation According to the present disclosure, the redesignation process is coordinated by the soft-partition monitor. During redesignation, there may be specific tasks associated with each platform interrupt source that need to be performed by the soft-partition monitor. For example, one platform interrupt may require that the interrupt hardware be reset, while another may require certain corrective action specific to the interrupt source To implement this in the soft-partition monitor's redesignation code, each platform interrupt source is associated with a function pointer handle which is invoked when redesignation occurs The function pointer handle is provided to the soft-partition monitor by the interrupt handling subsystem associated with the platform interrupt source. When a redesignation event occurs for a particular interrupt source, the function pointer is used by the soft-partition monitor to invoke the corresponding interrupt handling subsystem In this manner, each platform interrupt source can manage its own specific redesignation-related tasks. As understood with reference to the Figure, in a method 1 of the present disclosure a soft-partition monitor is provided 2 A function handle pointer is associated 3 with each platform interrupt source After a redesignation event occurs 4, the soft-partition monitor invokes 5 the corresponding interrupt handling subsystem via the function pointer handle. Such a technique eliminates the need to provide extra platform interrupt controller hardware to support fine grained partitioning, and enables individual soft partitions to react to platform interrupts similarly to a regular physical server. . Keywords: Codes (symbols); Computer hardware; Data handling; Firmware; Function evaluation; Information management; Information retrieval systems; Configuration; Multiple operating systems; Soft-partitioning products; Computer operating systems
2006-01-01 - Distributed Computing - 20th International Symposium, DISC 2006, Proceedings	The proceedings contain 40 papers. The topics discussed include: renaming in message passing systems with Byzantine failures; built-in coloring for highly-concurrent doubly-linked lists; fault-tolerant and self-stabilizing mobile robots gathering; for computation by population protocols with a leader; on self-stabilizing search trees; efficient dynamic aggregation; groupings and pairings in anonymous networks; a new proof of the GHS minimum scanning tree algorithm; a knowledge-based analysis of global function computation; capturing register and control dependence in memory consistency models with applications to the Itanium architecture; less is more: consensus gaps between restricted and unrestricted objects; on consistency of encrypted files; agreeing to agree: conflict resolution for optimistically replicated data; and time and communication efficient consensus for crash failures. . Keywords: Adaptive algorithms; Artificial intelligence; Computational methods; Fault tolerant computer systems; Mobile robots; Trees (mathematics); Byzantine failures; Crash failures; Encrypted files; Computer science
2006-01-01 - Divining INTEL'S future	The business prospects of Intel Corp., the world's largest semiconductor company, are discussed. The company's core computer business is growing and its new business in communications are generating losses instead of growth. Chief Executive of the company is planning to erase $1 billion in expenses from the company's books, eliminating loss-making groups to focus on Intel's traditional X86 computing business. Intel is shopping its Xscale processor, which serves communications systems ranging from cell phones to Internet routers. Several analysts have suggested to sell or spin off the flash unit, retain Xscale, and transition out of Itanium as soon as possible to replace it with multithreaded X86 CPUs. It is also suggested to revamp the fab strategy to embrace foundry partners that would develop process technology and share capacity requirements. . Keywords: Communication systems; Microprocessor chips; Routers; Telephone sets; Foundry partners; Intel Corp (CO); Process technology; Share capacity; Industrial economics
2006-01-01 - Efficiently implementing a large number of LL/SC objects	Over the past decade, a pair of instructions called load-linked (LL) and store-conditional (SC) have emerged as the most suitable synchronization instructions for the design of lock-free algorithms. However, current architectures do not support these instructions; instead, they support either CAS (e.g., UltraSPARC, Itanium) or restricted versions of LL/SC (e.g., POWER4, MIPS, Alpha). Thus, there is a gap between what algorithm designers want (namely, LL/SC) and what multiprocessors actually support (namely, CAS or RLL/RSC). To bridge this gap, a flurry of algorithms that implement LL/SC from CAS have appeared in the literature. The two most recent algorithms are due to Doherty, Herlihy, Luchangco, and Moir (2004) and Michael (2004). To implement M LL/SC objects shared by N processes, Doherty et al.'s algorithm uses only 0(N + M) space, but is only non-blocking and not wait-free. Michael's algorithm, on the other hand, is wait-free, but uses O(N2 + M) space. The main drawback of his algorithm is the time complexity of the SC operation: although the expected amortized running time of SC is only O(1), the worst-case running time of SC is O(N2). The algorithm in this paper overcomes this drawback. Specifically, we design a waitfree algorithm that achieves a space complexity of O(N2 + M), while still maintaining the O(1) worst-case running time for LL and SC operations. © Springer-Verlag Berlin Heidelberg 2006. . Keywords: Design; Doherty; Itanium; Lock-free algorithms; Non-blocking; Running time; Space complexity; Time complexity; UltraSPARC; Wait-free algorithms; Algorithms
2006-01-01 - Evaluating recursive filters on distributed memory parallel computers	The aim of this paper is to show that the recently developed high performance divide and conquer algorithm for solving linear recurrence systems with constant coefficients together with the new BLAS-based algorithm for narrow-banded triangular Toeplitz matrix-vector multiplication, allow to evaluate linear recursive filters efficiently on distributed memory parallel computers. We apply the BSP model of parallel computing to predict the behaviour of the algorithm and to find the optimal values of the method's parameters. The results of experiments performed on a cluster of twelve dual-processor Itanium 2 computers and Cray X1 are also presented and discussed. The algorithm allows to utilize up to 30% of the peak performance of 24 Itanium processors, while a simple scalar algorithm can only utilize about 4% of the peak performance of a single processor. Copyright © 2006 John Wiley & Sons, Ltd. . Keywords: Algorithms; Linear systems; Parallel processing systems; Program processors; Recursive functions; Algorithms; Linear systems; Program processors; Recursive functions; Distributed memory parallel computers; Itanium processors; Linear recursive filters; Narrow-banded triangular Toeplitz matrix vector multiplication; Parallel processing systems
2006-01-01 - Fujitsu's vision for high performance computing	Demands for high performance computing are getting heavier every day. To meet these demands, Fujitsu continues to conduct ongoing research and development of new and more powerful servers, software, and solutions. Last July, Fujitsu launched a new HPC server model, the PRIMEQUEST 500 Series, which utilizes dual-core Intel Itanium 2 processors with Fujitsu's original chipset. The PRIMEQUEST 500 Series is a 64-way true SMP server with up to 2TBytes of memory. The largest configuration of the PRIMEQUEST 500 Series can consist of up to 256 nodes connected by high speed interconnect.Towards PetaScale Computing, Fujitsu is carrying out a feasibility study to deliver petascale computing in the 2010s. Fujitsu is also performing joint fundamental R&D activities with Kyushu University for petascale interconnect and system evaluation methodology as part of the next generation supercomputer national project of Japan. © 2006 IEEE. . Keywords: Industrial research; Interconnection networks; Mathematical models; Program processors; Project management; Servers; Fujitsu (CO); High performance computing; PetaScale Computing; Supercomputers
2006-01-01 - High throughput total order broadcast for cluster environments	Total order broadcast is a fundamental communication primitive that plays a central role in bringing cheap software-based high availability to a wide array of services. This paper studies the practical performance of such a primitive on a cluster of homogeneous machines. We present FSR, a (uniform) total order broadcast protocol that provides high throughput, regardless of message broadcast patterns. FSR is based on a ring topology, only relies on point-to-point inter-process communication, and has a linear latency with respect to the total number of processes in the system. Moreover, it is fair in the sense that each process has an equal opportunity of having its messages delivered by all processes. On a cluster of Itanium based machines, FSR achieves a throughput of 79 Mbit/s on a 100 Mbit/s switched Ethernet network. © 2006 IEEE. . Keywords: Carrier communication; Communication; Computer software; Data transfer; Local area networks; Network protocols; Telecommunication services; Broadcast protocols; Ethernet; Messages; Broadcasting
2006-01-01 - Implicit and explicit optimizations for stencil computations	Stencil-based kernels constitute the core of many scientific applications on block-structured grids. Unfortunately, these codes achieve a low fraction of peak performance, due primarily to the disparity between processor and main memory speeds. We examine several optimizations on both the conventional cache-based memory systems of the Itanium 2, Opteron, and Power5, as well as the heterogeneous multicore design of the Cell processor. The optimizations target cache reuse across stencil sweeps, including both an implicit cache oblivious approach and a cache-aware algorithm blocked to match the cache structure. Finally, we consider stencil computations on a machine with an explicitly-managed memory hierarchy, the Cell processor. Overall, results show that a cache-aware approach is significantly faster than a cache oblivious approach and that the explicitly managed memory on Cell is more efficient: Relative to the Power5, it has almost 2x more memory bandwidth and is 3.7x faster. Copyright 2006 ACM. . Keywords: Algorithms; Bandwidth; Computation theory; Data storage equipment; Hierarchical systems; Program processors; Memory bandwidth; Memory hierarchy; Memory systems; Grid computing
2006-01-01 - Improved Superblock optimization in GCC	Superblock scheduling is a common technique to increase the level of instruction level parallelism (ILP) in generated code. Compared to a basic block, the Superblock gives an optimizer or scheduler a longer range over which instructions can be moved. The bookkeeping necessary to execute that move is less than would be necessary inside an arbitrary trace region. Additionally, the process of forming Superblocks generates more instructions that are eligible for movement. These factors combine to produce a significant increase in the ILP in a section of code. By identifying the key feature of Superblock formation that allows this increase in ILP, we can generalize the concept to describe a class of similar optimizations. We refer to techniques in this class as structural techniques. Combining several optimizations in this class with aggressive classical optimization has been shown in the OpenIMPACT compiler to be particularly useful in developing ILP when compiling for the Itanium processor. As a motivation for our work, we present an investigation into the value of structural compilation in the OpenIMPACT compiler. In this domain, structural techniques have been credited with a 10% to 13% increase in code performance over a compiler that implements only classical optimizations. As a first step toward developing structural compilation techniques in GCC, we implemented Superblock formation at the Tree-SSA level. By performing structural transformations early, we give the compiler's high level optimizers an opportunity to specialize the transformed program, thereby cultivating higher levels of ILP. The early results of this modification are mixed, with some benchmarks improving and others slowing. In this paper, we present details on our implementation and study the effects of this structural transformation on later optimizations. Through this, we hope to motivate future work to implement and improve optimizations that can take advantage of the transformed control flow. . Keywords: Scheduling; Shape optimization; Basic blocks; Classical optimization; Code performance; Compilation techniques; Control flows; Instruction level parallelism; Itanium processor; Key feature; Optimizers; Structural transformation; Program compilers
2006-01-01 - Improving the compensated Horner scheme with a Fused Multiply and Add	Several different techniques and softwares intend to improve the accuracy of results computed in a fixed finite precision. Here we focus on a method to improve the accuracy of the polynomial evaluation. It is well known that the use of the Fused Multiply and Add operation available on some microprocessors like Intel Itanium improves slightly the accuracy of the Horner scheme. In this paper, we propose an accurate compensated Horner scheme specially designed to take advantage of the Fused Multiply and Add. We prove that the computed result is as accurate as if computed in twice the working precision. The algorithm we present is fast since it only requires well optimizable floating point operations, performed in the same working precision as the given data. Copyright 2006 ACM. . Keywords: Algorithms; Microprocessor chips; Optimization; Polynomials; Error-free transformations; Fused Multiply and Add; Horner scheme; IEEE-754 floating point arithmetic; Polynomial evaluation; Computer software
2006-01-01 - Inline analysis: Beyond selection heuristics	Research on procedure inlining has mainly focused on heuristics that decide whether inlining a particular call-site maximizes application performance. However, other equally important aspects of inline analysis such as call-site analysis order, indirect effects of inlining, and selection of the most profitable version of a procedure warrant more attention. This paper evaluates a number of different sequences in which call-sites are examined for inlining and shows that choosing the correct order is crucial to obtaining the best run-time performance. We then present a novel, work-list-based, and updated sequence that achieves the best results. While applying cross-module inline analysis on large applications with thousands of files and millions of lines of code, we separate the analysis from the transformation phase and allow the former to work solely on summary information in order to reduce compile-time and memory consumption. A focus of this paper is to enumerate the summaries that our compiler maintains, present a technique to compute the goodness factor on which the work-list sequence is based, and describe methods to continuously update the summaries as and when a call-site is accepted for inlining. We then introduce inline specialization, a new technique that facilitates inlining into call chains selectively. The power of inline specialization lies in its ability to choose the most profitable version of the called procedure without having to maintain multiple versions at any point of time. We discuss implementation of these techniques in the HPUX Itanium® production compiler and present experimental results showing that a dynamic work-list based analysis order, comprehensive summary updates, and inline specialization significantly improve performance of applications. © 2006 IEEE. . Keywords: Codes (symbols); Optimization; Profitability; Program compilers; Application performance; Compile time; Improve performance; Indirect effects; Inline analysis; Lines of code; Memory consumption; Run-time performance; Site selection
2006-01-01 - Issues and support for dynamic register allocation	Post-link and dynamic optimizations have become important to achieve program performance. A major challenge in post-link and dynamic optimizations is the acquisition of registers for inserting optimization code in the main program. It is difficult to achieve both correctness and transparency when software-only schemes for acquiring registers are used, as described in [1]. We propose an architecture feature that builds upon existing hardware for stacked register allocation on the Itanium processor. The hardware impact of this feature is minimal, while simultaneously allowing post-link and dynamic optimization systems to obtain registers for optimization in a "safe" manner, thus preserving the transparency and improving the performance of these systems. © Springer-Verlag Berlin Heidelberg 2006. . Keywords: Computer hardware; Optimization; Program processors; Resource allocation; Dynamic optimizations; Dynamic register allocation; Inserting optimization code; Computer architecture
2006-01-01 - Large scale Itanium® 2 processor OLTP workload characterization and optimization	Large scale OLTP workloads on modern database servers are well understood across the industry. Their runtime performance characterizations serve to drive both server side software features and processor specific design decisions but are not understood outside of the primary industry stakeholders. We provide a rare glimpse into the performance characterizations of processor and platform targeted software optimizations running on a large-scale 32 processor, Intel®1 Itanium® 21 based, ccNUMA platform. Copyright 2006 ACM. . Keywords: Cache memory; Data reduction; Database systems; Optimization; Program processors; Servers; Cache coherency; Data partitioning; Itanium; OLTP workload; Performance characterization; Profile guided optimization; Software optimization; Large scale systems
2006-01-01 - Multi-language programming: The challenge and promise of class-level interfacing	Many computer applications today involve modules written in different programming languages, and integrating these modules together is a delicate operation. This first requires the availability of formalisms to let programmers denote "foreign" entities like objects and subprograms as well as their associated types. Then, proper translation of what programmers express often calls for significant implementation effort, possibly down to the specification of very precise ABIs (Application Binary Interfaces). Meta-language based approaches ala CORBA/IDL are very powerful in this respect but typically aim at addressing distributed systems issues as well, hence entail support infrastructure that not every target environment needs or can afford. When component distribution over a network is not a concern, straight interfacing at the binary object level is much more efficient. It however relies on numerous low level details and in practice is most often only possible for a limited set of constructs. Binary level interaction between foreign modules is traditionally achieved through subprogram calls, exchanging simple data types and relying on the target environment's core ABI. Object Oriented features in modern languages motivate specific additional capabilities in this area, such as class-level interfacing to allow reuse and extension of class hierarchies across languages with minimal constraints. This paper describes work we have conducted in this context, allowing direct binding of Ada extensible tagged types with C++ classes. Motivated by extensions to the Ada typing system made as part of the very recent language standard revision, this work leverages the GCC multilanguage infrastructure and implementation of the Itanium C++ ABI. We will first survey the issues and mechanisms related to basic interlanguage operations, then present the interfacing challenges posed by modern object oriented features after a brief overview of the Ada, C++, and Java object models. We will continue with a description of our work on Ada/C++ classlevel interfacing facilities, illustrated by an example. . Keywords: Computer applications; High level languages; Application binary interfaces; Associated types; Binary objects; Class hierarchies; Component distributions; CORBA/IDL; Data type; Distributed systems; Itanium; Java objects; Language standards; Low level; Meta language; Modern languages; Object-oriented features; Subprograms; Tagged types; Typing systems; Ada (programming language)
2006-01-01 - New EAX crossover for large TSP instances	We propose an evolutionary algorithm (EA) that applies to the traveling salesman problem (TSP). The EA uses edge assembly crossover (EAX), which is known to be efficient and effective for solving TSPs. Recently, a fast implementation of EAX and an effective technique for preserving population diversity were proposed. This makes it possible to compare the EA with EAX comparable to state-of-the-art TSP heuristics based on Lin-Karnighan heuristics. We further improved the performance of EAs with EAX, especially for large instances of more than 10,000 cities. Our method can find optimal solutions for instances of up to 24978 cities within a day using a single Itanium 2 1.3-GHz processor. Moreover, our EA found three new best tours for unsolved national TSP instances in a reasonable computation time. © Springer-Verlag Berlin Heidelberg 2006. . Keywords: Evolutionary algorithms; Heuristic methods; Problem solving; Program processors; Computation time; Lin-Karnighan heuristics; Traveling salesman problem (TSP); Computation theory
2006-01-01 - On the single processor performance of simple lattice Boltzmann kernels	This report presents a comprehensive survey of the effect of different data layouts on the single processor performance characteristics for the lattice Boltzmann method both for commodity "off-the-shelf" (COTS) architectures and tailored HPC systems, such as vector computers. We cover modern 64-bit processors ranging from IA32 compatible (Intel Xeon/Nocona, AMD Opteron), superscalar RISC (IBM Power4), IA64 (Intel Itanium 2) to classical vector (NEC SX6+) and novel vector (Cray X1) architectures. Combining different data layouts with architecture dependent optimization strategies we demonstrate that the optimal implementation strongly depends on the architecture used. In particular, the correct choice of the data layout could supersede complex cache-blocking techniques in our kernels. Furthermore our results demonstrate that vector systems can outperform COTS architectures by one order of magnitude. © 2005 Elsevier Ltd. All rights reserved. . Keywords: Computer aided design; Computer architecture; Microprocessor chips; Numerical methods; Optimization; Vectors; Computer aided design; Microprocessor chips; Numerical methods; Optimization; Vectors; Cache-blocking techniques; Data layouts; Lattice Boltzmann kernels; Single processor performance; Computer architecture
2006-01-01 - Optimal register reassignment for register stack overflow minimization	Architectures with a register stack can implement efficient calling conventions. Using the overlapping of callers’ and callees’ registers, callers are able to pass parameters to callees without a memory stack. The most recent instance of a register stack can be found in the Intel Itanium architecture. A hardware component called the register stack engine (RSE) provides an illusion of an infinite-length register stack using a memory-backed process to handle overflow and underflow for a physically limited number of registers. Despite such hardware support, some applications suffer from the overhead required to handle register stack overflow and underflow. The memory latency associated with the overflow and underflow of a register stack can be reduced by generating multiple register allocation instructions within a procedure [Settle et al. 2003]. Live analysis is utilized to find a set of registers that are not required to keep their values across procedure boundaries. However, among those dead registers, only the registers that are consecutively located in a certain part of the register stack frame can be removed. We propose a compiler-supported register reassignment technique that reduces RSE overflow/underflow further. By reassigning registers based on live analysis, our technique forces as many dead registers to be removed as possible. We define the problem of optimal register reassignment, which minimizes interprocedural register stack heights considering multiple call sites within a procedure. We present how this problem is related to a path-finding problem in a graph called a sequence graph. We also propose an efficient heuristic algorithm for the problem. Finally, we present the measurement of effects of the proposed techniques on SPEC CINT2000 benchmark suite and the analysis of the results. The result shows that our approach reduces the RSE cycles by 6.4% and total cpu cycles by 1.7% on average. © 2006, ACM. All rights reserved. . Keywords:
2006-01-01 - Optimised external computation for the Euro50 MATLAB based integrated model	In previous work we have countered computational demands faced in integrated modelling by developing and using a parallel toolkit for MATLAB. However the use of an increasingly realistic model makes the computational requirements of the model much larger, particularly in wavefront sensing, reaching a point where simulations of several real time seconds were no longer practical taking up to 3 weeks per second. In response to this problem we have developed optimised C code to which MATLAB off loads computation. This code has numerous advantages over native MATLAB computation. It is portable, scaleable using OpenMP directives and can run remotely using Remote Procedure Calls (RPCs). It has opened up the possibility of exploiting high end Itanium and Opteron based shared memory systems, optimised 3rd party libraries and aggressive compiler optimisation. These factors combined with hand-tuning give a performance increase of the order of 100 times. The interface to the rest of the model remains the same so the overall structure is unchanged. In addition we have developed a similar system based on Message Passing Interface version 2, (MPI-2) which allows us to exploited clusters. Here we present an analysis of techniques used and gains obtained along with a brief discussion of future work. . Keywords: Computer simulation; Computer software; Mathematical models; Parallel processing systems; Problem solving; Real time systems; Requirements engineering; Euro50; Integrated Modelling; MATLAB; MPI; OpenMP; Remote Procedure Calls (RPC); Computation theory
2006-01-01 - Parallel simulation of three-dimensional bursting with MPI and OpenMP	This work presents a mathematical model and its parallel implementation via two parallel paradigms for the simulation of three-dimensional bursting phenomena. The mathematical model consists of four nonlinearly coupled partial differential equations and includes fast and slow subsystems. The differential equations have been discretized by means of a linearly-implicit finite difference method in equally-spaced grids. The resulting system of equations at each time level has been solved by means of an optimized Preconditioned Conjugate Gradient (PCG) method. The proposed mathematical model has been implemented via: (1) a message passing paradigm based on the standard MPI and (2) a shared address space paradigm based on SPMD OpenMP. The two implementations have been evaluated on two current parallel architectures, i.e., a cluster of biprocessors Xeon and an SGI Altix 3700 Bx2 based on Itanium. It is shown that better performance and scalability are obtained on the second platform. © Springer-Verlag Berlin Heidelberg 2006. . Keywords: Computer simulation; Finite difference method; Mathematical models; Parallel processing systems; Partial differential equations; Program processors; Biprocessors; Parallel architectures; Preconditioned Conjugate Gradient (PCG) method; Scalability; Interfaces (computer)
2006-01-01 - Partial dead code elimination on predicated code regions	This paper presents the design, implementation and experimental evaluation of a practical region-based partial dead code elimination (PDE) algorithm on predicated code in the Open Research Compiler framework. Existing PDE algorithms are not applicable on predicated code due to the existence of if-converted branches in the program. The proposed algorithm processes all PDE candidates in a worklist and considers their partial deadness using predicate partition graphs. Our algorithm operates uniformly on individual hyperblocks as well as regions comprising of basic blocks and hyperblocks. The result of applying our algorithm to a single-entry multiple-exit (SEME) region is optimal: partially dead code cannot be removed without changing the branching structure of the program or potentially introducing new predicate defining instructions. We present statistical evidence about the PDE opportunities in the 17 SPEC95 and SPEC00 integer benchmarks. Our algorithm achieves performance improvements in 12 out of the 17 benchmarks on an Itanium machine at small compilation overheads. Our results indicate that our algorithm can be used as a practical pass before instruction scheduling. Copyright © 2006 John Wiley & Sons, Ltd. . Keywords: Algorithms; Computer graphics; Integer programming; Optimization; Program compilers; Statistical methods; Code optimization; Hyperblocks; Partial dead code elimination (PDE); Performance evaluation; Predicate partition graphs; Predicated code regions; Block codes
2006-01-01 - Performance comparison of data-reordering algorithms for sparse matrix-vector multiplication in edge-based unstructured grid computations	Several performance improvements for finite-element edge-based sparse matrix-vector multiplication algorithms on unstructured grids are presented and tested. Edge data structures for tetrahedral meshes and triangular interface elements are treated, focusing on nodal and edges renumbering strategies for improving processor and memory hierarchy use. Benchmark computations on Intel Itanium 2 and Pentium IV processors are performed. The results show performance improvements in CPU time ranging from 2 to 3. Copyright © 2005 John Wiley & Sons, Ltd. . Keywords: Algorithms; Computational methods; Data structures; Finite element method; Matrix algebra; Vectors; Algorithms; Computational methods; Finite element method; Matrix algebra; Vectors; Cache-based machines; Data reordering; Edge-based finite elements; Unstructured grids; Data structures
2006-01-01 - Performance modeling using Monte Carlo simulation	Cycle accurate simulation has long been the primary tool for micro-architecture design and evaluation. Though accurate, the slow speed often imposes constraints on the extent of design exploration. In this work, we propose a fast, accurate Monte-Carlo based model for predicting processor performance. We apply this technique to predict the CPI of in-order architectures and validate it against the Itanium-2. The Monte Carlo model uses micro-architecture independent application characteristics, and cache, branch predictor statistics to predict CPI with an average error of less than 7%. Since prediction is achieved in a few seconds, the model can be used for fast design space exploration that can efficiently cull the space for cycle-accurate simulations. Besides accurately predicting CPI, the model also breaks down CPI into various components, where each component quantifies the effect of a particular stall condition (branch mis-prediction, cache miss, etc.) on overall CPI. Such a CPI decomposition can help processor designers quickly identify and resolve critical performance bottlenecks. . Keywords: Cache memory; Constraint theory; Error analysis; Mathematical models; Monte Carlo methods; Software architecture; Cycle accurate simulation; Performance modeling; Space exploration; Computer simulation
2006-01-01 - Performance tuning of matrix triple products based on matrix structure	Sparse matrix computations arise in many scientific and engineering applications, but their performance is limited by the growing gap between processor and memory speed. In this paper, we present a case study of an important sparse matrix triple product problem that commonly arises in primal-dual optimization method. Instead of a generic two-phase algorithm, we devise and implement a single pass algorithm that exploits the block diagonal structure of the matrix. Our algorithm uses fewer floating point operations and roughly half the memory of the two-phase algorithm. The speed-up of the one-phase scheme over the two-phase scheme is 2.04 on a 900 MHz Intel Itanium-2, 1.63 on an 1 GHz Power-4, and 1.99 on a 900 MHz Sun Ultra-3. © Springer-Verlag Berlin Heidelberg 2006. . Keywords: Algorithms; Computational complexity; Computer operating procedures; Computer science; Optimization; Parallel processing systems; Problem solving; Matrix structures; Matrix triple products; Performance tuning; Primal-dual optimization method; Matrix algebra
2006-01-01 - Piecewise cubic interpolation on distributed memory parallel computers and clusters of workstations	The aim of this paper is to present two new portable and high performance implementations of routines that can be used for piecewise cubic interpolation. The first one (sequential) is based on LAPACK routines, while the next, based on ScaLAPACK is designed for distributed memory parallel computers and clusters. The results of experiments performed on a cluster of twenty Itanium 2 processors and on Cray X1 are also presented and shortly discussed. © 2006 IEEE. . Keywords: Electrical engineering; Interpolation; Parallel architectures; Clusters of workstations; Cubic interpolation; Distributed-memory parallel computers; Itanium 2 processor; Performance implementation; Piece-wise; ScaLAPACK; Parallel processing systems
2006-01-01 - Power and temperature control on a 90-nm Itanium family processor	This paper describes the embedded feedback and control system on a 90-nm Itanium family processor, code-named Montecito, that maximizes performance while staying within a target power and temperature (PT) envelope. This system, referred to as Foxton Technology (FT), utilizes on-chip sensors and an embedded microcontroller to measure PT and modulate both voltage and frequency (VF) to optimize performance while meeting PT constraints. Changing both VF takes advantage of the cubic relationship of P/spl prop/CV/sup 2/F. We present measured results that show a 31% reduction in power for only a 10% drop in frequency. Montecito is able to implement FT using only 0.5% of the die area and 0.5% of the die power. . Keywords: Electric power measurement; Embedded systems; Frequency stability; Power control; Temperature control; Temperature measurement; Embedded feedback system; Embedded microcontroller; Foxton Technology; Frequency scaling; Itanium Family processor; Montecito processor; On-chip sensors; Power management; PT constraints; Temperature management; Microprocessor chips
2006-01-01 - Power struggle	Electricity consumption in the data processing sector that incorporate servers with smaller size, dense packing, is discussed. In large multi-megawatt data centers, annual power consumption exceeds £ 500,000. Server venders such as Sun, HP, and Dell have introduced their ultra-slim "blade severs". In recent years, energy efficiency in data centers has emerged as a top priority as new equipments are getting more and more efficient. Data conversion products used for large scale systems have become as efficient as 95 %. At 25 % load level, these equipments are capable of delivering 95 % efficiency. keeping the facilities cool in data centers is one of the problems that needs to be effectively solved. The physical design of many data centers is not adequate enough to involve any real solution for the power and cooling systems. Intel has come up with a solution by developing a new microprocessor architecture for the "Itanium" chips which the company claims to be power efficient. . Keywords: Cooling; Data acquisition; Data processing; Energy efficiency; Large scale systems; Microprocessor chips; Problem solving; Real time systems; Servers; Data conversion products; Electricity consumption; Multi-megawatt data centers; Venders; Power control
2006-01-01 - Predicate elimination technique in binary translation for Ia-64 architecture	EPIC (Explicitly Parallel Instruction Computing) architectures, such as the Intel IA-64 (Itanium), support novel features such as explicit instruction-level parallelism and predicated instructions. While these features promise to make code more efficient, the fact that these new architectural features means that EPIC code is more difficult to analyze than code for more traditional architectures. This paper describes a technique for removing predication instructions from optimized binary programs in a way that is guaranteed to preserve program semantics, and thereby improve the quality of binary translation for IA-64. ©2006 IEEE. . Keywords: Binary codes; Canning; Codes (standards); Codes (symbols); Information theory; Program translators; Translation (languages); Virtual reality; Architectural features; Binary programs; Binary translation; Explicitly parallel instruction computing; Instruction-level parallelism (ILP); international conferences; Itanium; program semantics; Computer architecture
2006-01-01 - Predicate removing technique in machine emulation for IA-64 aechitecture	EPIC (Explicitly Parallel Instruction Computing) architectures, such as the Intel IA-64 (Itanium), support novel features such as explicit instruction-level parallelism and predicated instructions. While these features promise to make code more efficient, the fact that these new architectural features means that EPIC code is more difficult to analyze than code for more traditional architectures. This paper describes a technique for removing predication instructions from optimized binary programs in a way that is guaranteed to preserve program semantics, thereby and thereby improve the quality of machine emulation for IA-64. . Keywords: Computer networks; Conceptual design; Industrial engineering; Information theory; Product design; Binary translation; IA-64; Industrial designs; Machine emulation; Predication; Design
2006-01-01 - Prematerialization: Reducing register pressure for free	Modern compiler transformations that eliminate redundant computations or reorder instructions, such as partial redundancy elimination and instruction scheduling, are very effective in improving application performance but tend to create longer and potentially more complex live ranges. Typically the task of dealing with the increased register pressure is left to the register allocator. To avoid introduction of spill code which can reduce or completely eliminate the benefit of earlier optimizations, researchers have developed techniques such as live range splitting and rematerialization. This paper describes prematerialization (PM), a novel method for reducing register pressure for VLIW architectures with nop instructions. PM and rematerialization both select "never killed" live ranges and break them up by introducing one or more definitions close to the uses. However, while rematerialization is applied to live ranges selected for spilling during register allocation, PM relies on the availability of nop instructions and occurs prior to register allocation. PM simplifies register allocation by creating live ranges that are easier to color and less likely to spill. We have implemented prematerialization in HP-UX production compilers for the Intel® Itanium® architecture. Performance evaluation indicates that the proposed technique is effective in reducing register pressure inherent in highly optimized code. Copyright 2006 ACM. . Keywords: Codes (symbols); Computational complexity; Computer architecture; Optimization; Resource allocation; Itanium; Register allocation; Register pressure; Rematerialization; VLIW; Program compilers
2006-01-01 - Pro oracle database 10g RAC on linux: Installation, administration, and performance	Real Application Clusters (RAC) and the Grid architecture are Oracle's strategy for scaling out enterprise systems to cope with bigger workloads and more users. Many books limit themselves by conceptualizing and theorizing about RAC technology, but this book is the first to portray implementing and administering an Oracle 10g RAC system in a Linux environment. This book features basic concepts underlying Linux and Oracle RAC, design strategies, hardware procurement and configuration, and many other topics. The RAC-specific technologies described include configuration of the interconnect, OCFS, ASM, Cluster Ready Services, and Grid Control. The Oracle features RMAN and Data Guard are also discussed, along with available hardware options. The authors include practical examples and configuration information, so that upon reading this book, you'll be armed with the information you need to build an Oracle RAC database on Linux, whether it is on a single laptop or a 64-node Itanium cluster. Copyright © 2006 by Julian Dyke and Steve Shaw All rights reserved. . Keywords:
2006-01-01 - Processing-in-memory technology for knowledge discovery algorithms	The goal of this work is to gain insight into whether processing-in-memory (PIM) technology can be used to accelerate the performance of link discovery algorithms, which represent an important class of emerging knowledge discovery techniques. PIM chips that integrate processor logic into memory devices offer a new opportunity for bridging the growing gap between processor and memory speeds, especially for applications with high memory-bandwidth requirements. As LD algorithms are data-intensive and highly parallel, involving read-only queries over large data sets, parallel computing power extremely close (physically) to the data has the potential of providing dramatic computing speedups. For this reason, we evaluated the mapping of LD algorithms to a processing-in-memory (PIM) workstation-class architecture, the DIVA / Godiva hardware testbeds developed by USC/ISI. Accounting for differences in clock speed and data scaling, our analysis shows a performance gain on a single PIM, with the potential for greater improvement when multiple PIMs are used. Measured speedups of 8x are shown on two additional bandwidth benchmarks, even though the Itanium-2 has a clock rate 6X faster. Copyright 2006 ACM. . Keywords: Algorithms; Bandwidth; Data storage equipment; Parallel processing systems; Program processors; Telecommunication links; Godiva hardware; Memory speeds; Processing in memory technology; Processor logic; Data mining
2006-01-01 - Programmer-centric conditions for itanium memory consistency	We formulate a programmer-centric description of the memory consistency model provided by the Itanium architecture. This allows reasoning about programs at a non-operational level in the natural way, not obscured by the implementation details of the underlying architecture. However, our definition is not tight. We provide two very similar definitions and show that the specification of the Itanium memory model lies between the two. These two definitions are motivated by slightly different implementations of load-acquire instructions. © Springer-Verlag Berlin Heidelberg 2006. . Keywords: Artificial intelligence; Itanium; Memory consistency; Memory consistency models; Memory modeling; Reasoning about programs; Computer science
2006-01-01 - Recovery code generation for general speculative optimizations	A general framework that integrates both control and data speculation using alias profiling and/or compiler heuristic rules has shown to improve CPU2000 performance on Itanium systems. However, speculative optimizations require check instructions and recovery code to ensure correct execution when speculation fails at runtime. How to generate check instructions and their associated recovery code efficiently and effectively is an issue yet to be well studied. It is also, very important that the recovery code generated in the earlier phases integrate gracefully in the later optimization phases. At the very least, it should not hinder later optimizations, thus, ensuring overall performance improvement. This paper proposes a framework that uses an if-block structure to facilitate check instructions and recovery code generation for general speculative optimizations. It allows speculative instructions and their recovery code generated in the early compiler optimization phases to be integrated effectively with the subsequent optimization phases. It also allows multilevel speculation for multilevel pointers and multilevel expression trees to be handled with no additional complexity. The proposed recovery code generation framework has been implemented and evaluated in the Open Research Compiler (ORC). © 2006, ACM. All rights reserved. . Keywords:
2006-01-01 - Refined cache/TLB profiling with IPF BTB usage	A method is disclosed for obtaining additional information during a cache/translation look-aside buffer (TLB) profiling of a software program, with the usage of the Itanium Processor Family (IPF) processors' Branch Trace Buffer (BTB). The usage of IPF Performance Monitoring Unit (PMU) hardware provides a reliable and efficient way to get vital information for cache/TLB miss events related performance tuning of a software module. BTB can record four to eight branches that have executed prior to the event occurrence in addition to registering the regular event collection/counting. The refined cache/TLB profiling provides faster identification and re solution of complex performance bottlenecks in common code. The method can also be used to understand the cache/TLB misses in a procedure with respect to the order of branches between the basic blocks in a given procedure. . Keywords: Buffer storage; Computer software; Program processors; Branch Trace Buffer (BTB); Look-aside buffer (TLB) profiling; Performance Monitoring Unit (PMU) hardware; Cache memory
2006-01-01 - Solution of large complex problems in computational electromagnetics using higher-order basis in MoM with out-of-core solvers	In a recent invited paper in the IEEE Antennas and Propagation Magazine, some of the challenging problems in computational electromagnetics were presented. One of the objectives of this note is to simply point out that challenging to one may be simple to another. This is demonstrated through an example cited in that article. The example chosen is a Vivaldi antenna array. What we discuss here also applies to the other examples presented in that article, but we have chosen the Vivaldi antenna array to help us make our point. It is shown in this short article that a higher-order basis using a surface integral equation a la a PMCHWT (Poggio-Miller-Chu-Harrington-Wu-Tsai) Method-of-Moments formulation may still be the best weapon that one have in today's arsenal to deal with challenging complex electromagnetic analysis problems. Here, we have used the commercially available code WIPL-D to carry out all the computations using laptop/desktop systems. The second objective of this paper is to present an out-of-Gore solver. The goal is to demonstrate that an out-of-core 32-bit-system-based solver can be as efficient as a 64-bit in-core solver. This is quite contrary to the popular belief that an out-of-core solver is generally much slower than an in-core solver. This can be significant, as the difference in the cost of a 32-bit system can be 1/30 of a 64-bit system of similar capabilities using current computer architectures. For the 32-bit system, we consider a Pentium 4 system, whereas for the 64-bit system, we consider an Itanium 2 system for comparison. The out-of-core solver can go beyond the 2 GB limitation for a 32-bit system and can be run on ordinary laptop/desktop; hence, we can simultaneously have a much lower hardware investment while better performance for a sophisticated and powerful electromagnetic solver. The system resources and the CPU times are also outlined. © 2006 IEEE. . Keywords: Computational methods; Electromagnetic wave propagation; Integral equations; Method of moments; Numerical analysis; Computational electromagnetics; Electromagnetic solver; Electromagnetics analysis; Vivaldi antenna array; Antenna arrays
2006-01-01 - The implementation of a 2-core, multi-threaded itanium family processor	The design of the high end server processor code named Montecito incorporated several ambitious goals requiring innovation. The most obvious being the incorporation of two legacy cores on-die and at the same time reducing power by 23%. This is an effective 325% increase in MIPS per watt which necessitated a holistic focus on power reduction and management. The next challenge in the implementation was to ensure robust and high frequency circuit operation in the 90-nm process generation which brings with it higher leakage and greater variability. Achieving this goal required new methodologies for design, a greatly improved and tunable clock system and a better understanding of our power grid behavior all of which required new circuits and capabilities. The final aspect of circuit design improvement involved the I/O design for our legacy multi-drop system bus. To properly feed the two high frequency cores with memory bandwidth we needed to ensure frequency headroom in the operation of the bus. This was achieved through several innovations in controllability and tuning of the I/O buffers which are discussed as well. . Keywords: Electric network analysis; Input output programs; Leakage currents; Multiprocessing systems; Networks (circuits); Power generation; Circuit design improvement; High frequency circuit operation; I/O design; Itanium family processor; Legacy cores; Montecito processor; Multi-threaded processor; Power management; Power reduction; Tunable clock system; Servers
2006-01-01 - The parity protected, multithreaded register files on the 90-nm itanium microprocessor	The integer and floating-point register files of the 90-nm generation Itanium Microprocessor are described. A pulsed, shared word line technique enables a 22 ported integer array with only 12 word lines per register. An in-register ripple parity system provides soft error detection with no impact to operand bypass or pipeline depth while keeping consuming less that 6% of the total register datapath area. The register file implements temporal multi-threading by multiplexing the read and write ports to two storage nodes enabling registers to write both foreground and background threads to the same register at the same time. Thread switching completes in one cycle. The register files are fabricated in a 7-layer 90-nm process and operate up to 2.0 GHz while consuming 400 mW per register array. . Keywords: Digital integrated circuits; Error detection; Integer programming; Linear equations; Switching; 400 mW; 90 nm; Itanium Microprocessor; Line techniques; Multithreaded register files; Parity protected register files; Ripple parity system; Soft error detection; Thread switching; Microprocessor chips
2006-01-01 - Translating between itanium and sparc memory consistency models	Our general goal is to port programs from one multiprocessor architecture to another, while ensuring that each program's semantics remains unchanged. This paper addresses a subset of the problem by determining the relationships between memory consistency models of three Sparc architectures, (TSO, PSO and RMO) and that of the Itanium architecture. First we consider Itanium programs that are constrained to have only one load-type of instruction in {load, load-acquire}, and one store-type of instruction in {store, store_release}. We prove that in three out of four cases, the set of computations of any such program is exactly the set of computations of the "same" program (using only load and store) on one Sparc architecture. In the remaining case the set is nested between two natural sets of Sparc computations. Real Itanium programs, however, use a mixture of load, load_acquire, store, store-release and memory fence instructions, and real Sparc programs use a variety of barrier instruction as well as load and store instructions. We next show that any mixture of the load-types or the store-types (in the case of Itanium) or any barrier instructions (in the case of Sparc) completely destroys the clean and simple similarities between the sets of computations of these systems. Thus (even without considering the additional complications due to register and control dependencies) transforming these more general programs in either direction requires constraining the transformed program substantially more than the original program in order to ensure that no erroneous computations can arise. Copyright 2006 ACM. . Keywords: Computer architecture; Computer programming; Mathematical models; Multiprocessing systems; Problem solving; Semantics; Barrier instructions; Multiprocessor architecture; Port programs; Sparc architecture; Storage allocation (computer)
2006-01-01 - Ultra-fast CPU performance prediction: Extending the Monte Carlo approach	Performance evaluation of contemporary processors is becoming increasingly difficult due to the lack of proper frameworks. Traditionally, cycle-accurate simulators have been extensively used due to their inherent accuracy and flexibility. However, the effort involved in building them, their slow speed, and their limited ability to provide insight often imposes constraints on the extent of design exploration. In this paper, we refine our earlier Monte Carlo based CPIprediction model [11] to include software assisted data-prefetching and an improved memory model. Softwarebased prefetching is becoming an increasingly important feature in modern processors but to the best of our knowledge, existing frameworks do not model it. Our model uses microarchitecture independent application characteristics to predict CPI with an average error of less than 10% when vaidated against the Itanium-2 processor. Besides accurate performance prediction, we illustrate the applications of the model to processor bottle-neck analysis, workload characterization and design space exploration. © 2006 IEEE. . Keywords: Computer simulation; Constraint theory; Model checking; Monte Carlo methods; Software design; Bottle-neck analysis; Design exploration; Prefetching; Workload characterization; Program processors
2006-01-01 - Using adaptive circuits to mitigate process variations in a microprocessor design	With each successive technology generation, process and environmental variations consume an increasingly large portion of the design envelope. To mitigate the impact of these variations, designs can incorporate adaptive techniques to reduce the impact. At the core of adaptability is the fundamental idea that each piece of silicon is different and will respond differently to stimuli. This poses a significant challenge in testing the product because testing relies on all parts behaving in a predictable manner every time they are tested. This article details adaptive techniques used on a dual-core, 90-nm Itanium microprocessor, and the issues and limitations encountered when testing this design. © 2006 IEEE. . Keywords: Application specific integrated circuits; Cache memory; Design for testability; Adaptive circuits; Cache safe technology; Itanium microprocessor; Process variation; Microprocessor chips
2006-01-01 - Ways to reduce the cost of runtime optimization	Runtime optimization analyzes the runtime information it collects, identifies hot spots, applies optimization on them, thus speedups the execution of the programs. However, the system itself may consume critical resources, which sometimes counteracts or even outweighs the benefit it gains, and leads to the failure of the optimization. This paper implements an adaptive binary optimization/compilation framework on SMP/IPF (Intel Itanium processor family)/Linux, among which runtime optimization is included. It also analyzes the stage and cost of runtime optimization, introduces the ideas and ways which are conducted to reduce them, during the implementation of the framework. . Keywords:
2006-01-01 - Whole-program optimization of global variable layout	On machines with high-performance processors, the memory system continues to be a performance bottleneck. Compilers insert prefetch operations and reorder data accesses to improve locality, but increasingly seek to modify an application's data layout to reduce cache miss and page fault penalties. In this paper we discuss Global Variable Layout (GVL), an optimization of the placement of entire static global data objects in the binary. We describe two practical methods for GVL in the HP-UX Integrity optimizing compiler for the Itanium © architecture. The first layout strategy relies on profile feedback, collaboratively employing the compiler, the linker and a pre-link tool to facilitate reordering. The second strategy uses whole-program analysis to drive data layout decisions, and does not require the use of a dynamic profile. We give a detailed description of our implementation and evaluate its performance for the SPEC integer benchmark programs, as well as for a large commercial database application. Copyright 2006 ACM. . Keywords: Buffer storage; Computer architecture; Data reduction; Database systems; Optimization; Program compilers; Compiler directed memory management; Data caches; Data layout; Global variable layout; Data storage equipment
2007-01-01 - A cross-platform parallel MoM code with ScaLAPACK solver	To make the MoM code more applicable to real world problems, an efficient load-balancing MPI based parallel MoM solver is presented here. An efficient matrix filling scheme is presented to make the parallelization more convenient and efficient. The code has been successfully compiled with Windows and Linux operating systems and run on different Platforms such as DELL 1855 Cluster and HP Itanium 2 Server. Several numerical results have been presented to demonstrate the high parallel efficiency and the portability of the code. © 2007 IEEE. . Keywords: Antenna accessories; Antennas; Chlorine compounds; Codes (symbols); Computer operating systems; Computer software; Crack propagation; Method of moments; Windows operating system; International symposium; Itanium; Linux - operating system; Load-balancing; Method of moments (MoM); MPI; Numerica l results; Parallel; Parallel efficiency; Parallelization; Real-world problems; ScaLAPACK; Codes (standards)
2007-01-01 - A magnetoelectronic register file cell for a self-checkpointing microprocessor	A self-checkpointing microprocessor periodically copies the state of the currently executing program to on-chip non-volatile storage, allowing it to resume execution of the program at the last checkpoint after a power supply interruption or shutdown. In this paper, we present a register file cell for a self-checkpointing microprocessor that integrates a magnetoelectronic non-volatile memory cell into a register cell similar to the one used in the Itanium 2 microprocessor. Our design allows the contents of each bit in the register file to be checkpointed simultaneously, reducing the time required to take a checkpoint to a few clock cycles, without compromising the performance of the register file during normal read and write operations. Because the area of the base register file cell is determined by the wiring tracks it requires, adding self-checkpointing to the cell only increases its area by 6%. Similarly, adding self-checkpointing has effectively no impact on the cell's performance, only increasing read and write times from 299 to 300 ps, even when the additional load on the bit lines from the larger cells is considered. Copyright © 2007 John Wiley & Sons, Ltd. . Keywords: Electric loads; Electric power distribution; Integrated circuit layout; Magnetoelectronics; Nonvolatile storage; Check pointing; File cell; Magnetic logic; Register file; Microprocessor chips
2007-01-01 - Abiego: A function outlining and partial inlining framework	Frequently invoked large functions are common in non-numeric applications. These large functions present challenges to modern compilers not only because they require more time and resources at compilation time, but also because they may prevent optimizations such as function inlining. Often large portions of the code in a hot function fhost are executed much less frequently than fhost itself. Partial inlining is a natural solution to the problems caused by including cold code segments that are seldom executed into hot functions that are frequently invoked. When applying partial inlining, a compiler outlines cold statements from a hot function fhost. After outlining, fhost becomes smaller and thus can be easily inlined. This paper presents Ablego, a framework for function outlining and partial inlining that includes several innovations: (1) an abstract-syntax-tree-based analysis and transformation to form cold regions for outlining; (2) a set of flexible heuristics to control the aggressiveness of function outlining; (3) several possible function outlining strategies; (4) explicit variable spilling, a new technique that overcomes negative side-effects of function outlining. With the proper strategy, partial inlining improves performance by up to 5.75%. A performance study also suggests that partial inlining's effect on enabling more aggressive inlining is limited. The performance improvement from partial inlining actually comes from better code placement and better code generation. Copyright © 2006 John Wiley & Sons, Ltd. . Keywords: Abstracting; Codes (symbols); Function evaluation; Heuristic methods; Optimization; Syntactics; Code locality; Compiler optimization; Function splitting; Itanium; Partial inlining; Numerical methods
2007-01-01 - Applying code specialization to FFT libraries for integral parameters	Code specialization is an approach that can be used to improve the sequence of optimizations to be performed by the compiler. The performance of code after specialization may vary, depending upon the structure of the application. For FFT libraries, the specialization of code with different parameters may cause an increase in code size, thereby impacting overall behavior of applications executing in environment with small instruction caches. In this article, we propose a new approach for specializing FFT code that can be effectively used to improve performance while limiting the code increase by incorporating dynamic specialization. Our approach makes use of a static compile time analysis and adapts a single version of code to multiple values through runtime specialization. This technique has been applied to different FFT libraries over Itanium IA-64 platform using ice compiler v 9.0. For highly efficient libraries, we are able to achieve speedup of more than 80% with small increase in code size. © Springer-Verlag Berlin Heidelberg 2007. . Keywords: Buffer storage; Digital libraries; Program compilers; Storage allocation (computer); Code size; Code specialization; Compile time analysis; Computer programming
2007-01-01 - Architecture-based optimization for mapping scientific applications to imagine	It is a challenging issue whether scientific applications are suitable for Imagine architecture. To address this problem, this paper presents a novel architecture-based optimization for the key techniques of mapping scientific applications to Imagine. Our specific contributions include that we achieve fine kernel granularity and choose necessary arrays to organize appropriate streams. Specially, we develop a new stream program generation algorithm based on the architecture-based optimization. We implement our algorithm to some representative scientific applications on ISIM simulation of Imagine, compared the corresponding FORTRAN programs running on Itanium 2. The experimental results show that the optimizing stream programs can efficiently improve computational intensiveness, enhance locality of LRF and SRF, avoid index stream overhead and enable parallelism to utilize ALUs. It is certain that Imagine is efficient for many scientific applications. © Springer-Verlag Berlin Heidelberg 2007. . Keywords: Mapping; Architecture-based; Computational intensiveness; FORTRAN programs; Imagine; Novel architecture; Program generation; Scientific applications; Stream projects; Application programs
2007-01-01 - Challenges for formal verification in industrial setting	Commercial competition is forcing computer companies to get better products to market more rapidly, and therefore the time for validation is shrinking relative to the complexity of microprocessor designs. Improving time-to-market performance cannot be solved by just growing the size of design and validation teams. Design process automation is increasing, and the adoption of more rigorous methods, including formal verification, is unavoidable because for achieving the quality demanded by the marketplace. Intel is one of the strongest promoters of the use of formal methods across all phases of the design development. Intel's design teams use high-level modeling of protocols and algorithms, formal verification of floatingpoint libraries, design exploration systems based on formal methods, full proofs and property verification of RTL specifications, and equivalence checking to verify that transistor-level schematics correspond to their RTL specifications. Even with the best effort to adopt the progress in formal methods quickly, there is a large gap between an idea published at a conference and a development of a tool that can be used on industrial-sized designs. These tools and methods need to scale well, be stable during a multi-year design effort, and be able to support efficient debugging. The use of formal methods on a live design must allow for ongoing changes in the specification and the design. The methodology must be flexible enough to permit new design features, such as scan and power-down logic, soft error detection, etc. In this paper, I will share my experience with the formal verification of the floating-point unit on an Itanium(R) microprocessor design and point out how it may influence future microprocessor-design projects. © Springer-Verlag Berlin Heidelberg 2007. . Keywords: Automation; Equivalence classes; Error detection; Formal methods; Microprocessor chips; Network protocols; Computer companies; Equivalence; Rigorous methods; Verification
2007-01-01 - Characterizing DSS workloads from the processor perspective	In this paper, we characterized the TPC-H benchmark on an Itanium II processor. Our experiment results clearly demonstrate: (1) On Itanium II processor, the memory stall time is dominanted by first level (Ll) instruction cache and third level (L3) data cache misses; (2) Index can reduces L3 data cache misses dramatically but increases a slightly more Condition Branch Instruction misprediction (BMP) rate. These revealed characteristics are expected to benefit database performance optimizations and database architecture design on next-generation processors. © Springer-Verlag Berlin Heidelberg 2007. . Keywords: Buffer storage; Data structures; Database systems; Query processing; Response time (computer systems); Branch Instruction misprediction (BMP) rate; Performance optimizations; Program processors
2007-01-01 - COBRA: An adaptive runtime binary optimization framework for multithreaded applications	This paper presents COBRA (Continuous Binary Re-Adaptation), a runtime binary optimization framework, for multithreaded applications. It is currently implemented on Itanium 2 based SMP and cc-NUMA systems. Using OpenMP NAS parallel benchmark, we show how COBRA can adaptively choose appropriate optimizations according to observed changing runtime program behavior. Coherent cache misses caused by true/false data sharing often limit the scalability of multithreaded applications. This paper shows that COBRA can significantly improve the performance of some applications parallelized with OpenMP, by reducing the aggressiveness of data prefetching and by using exclusive hints for prefetch instructions. For example, we show that COBRA can improve the performance of OpenMP NAS parallel benchmarks up to 68%, with an average of 17.5% on the SGI Altix cc-NUMA system. © 2007 IEEE. . Keywords: Benchmarking; Parallel processing systems; Rhenium; Shape memory effect; Binary optimizations; Cache misses; CC NUMA; Data prefetching; Data sharing; International conferences; Itanium; Multithreaded applications; NAS parallel benchmarks (NPB); Parallel processing; Prefetch; Program behaviors; Run time; Some applications; Optimization
2007-01-01 - Comparative characterization of SPEC CPU2000 and CPU2006 on Itanium ® architecture	Recently SPEC1 released the next generation of its CPU benchmark, widely used by compiler writers and architects for measuring processor performance. This calls for characterization of the applications in SPEC CPU2006 to guide the design of future microprocessors. In addition, it necessitates assessing the change in the characteristics of the applications from one suite to another. Although similar studies using the retired SPEC CPU benchmark suites have been done in the past, to the best of our knowledge, a thorough characterization of CPU2006 and its comparison with CPU2000 has not been done so far. In this paper, we present the above; specifically, we analyze IPC (instructions per cycle), L1, L2 data cache misses and branch prediction, especially in CPU2006. © Copyright 2007 ACM. . Keywords: Buffer storage; Microprocessor chips; Branch prediction; Performance evaluation; SPEC CPU benchmarks; Program compilers
2007-01-01 - Compiler optimizations for fault tolerance software checking	Dramatic increases in the number of transistors that can be integrated on a chip will make the hardware more susceptible to radiation-induced transient errors. High-end architectures like the IBM mainframes, HP NonStop or mission-critical computers are likely to include several hardware-intensive fault tolerance techniques. However, the commodity chips which are cost- and energy-constrained, will need a more flexible and inexpensive technology for error detection. Software approaches can play a major role for this sector of the market because they need little hardware modification and can be tailored to fit different requirements of reliability and performance. Current software approaches address the problem by replicating the instructions and adding checking instructions to compare the results [1, 2, 3, 4, 5]. These checking instructions account for a significant fraction of the added overhead. In this work we propose a set of compiler optimizations to identify and remove redundant checks from the replicated code. Two checks are considered redundant if they check the same variable. In this case, it is possible to remove the check that appears first during execution so that an error will be detected when the second check executes. However, determining how much a check can be delayed is tricky. If we delay it too little, there is little room for optimization. If we delay it too much, the errors will propagate to undesired places and result in segmentation faults, corrupted memory, wrong execution path, or undetected errors across checkpoints. We consider that how much the error detection can be delayed will depend on the recovery mechanism supported by the hardware or the system. As long as checks are not delayed beyond synchronization checkpoints, the system will be able to properly recover. With our techniques the user can define what are the synchronization checkpoints based on the hardware support for recovery. In this work we evaluate two levels of hardware or system support: memory without support for checkpointing and rollback, where memory is guaranteed to not be corrupted with wrong values and memory with low-cost support for checkpointing and rollback. We also consider the situation where register file is protected with parity or ECC, such as Intel Itanium, Sun UltraSPARC and IBM Power4-6 because software implementations can take advantage of this hardware feature and reduce some of the replicated instructions. We have evaluated our approach using LLVM as our compiler infrastructure and PIN for fault injection. Our experimental results with Spec benchmarks on a Pentium 4 show that in the case where memory is guaranteed not to be corrupted, performance improves by an average 6.2%. With more support for checkpoint performance improves by an average 14.7%. A software fault tolerant system that takes advantage of the register safe platforms improves by an average 16.0%. Fault injection experiments show that our techniques do not decrease fault coverage, although they slightly increase the number of segmentation faults. © 2007 IEEE. . Keywords: Codes (symbols); Computer hardware; Error detection; Errors; Fault tolerance; Fault tolerant computer systems; Hardware; Optimization; Parallel processing systems; Program compilers; Quality assurance; Security of data; Software reliability; Turbo codes; Check-pointing; Compilation techniques; Compiler optimizations; Cost support; Current software; Energy-constrained; Execution paths; Fault coverage; Fault tolerance techniques; Fault-injection; Hardware modifications; Hardware supports; International conferences; Itanium; Parallel architectures; Pentium 4; Recovery mechanisms; Register files; Software faults; Software implementations; System supports; Transient errors; UltraSPARC; Reliability
2007-01-01 - Computational Science IDE for HPCMP systems	Software engineering studies have shown that programmer productivity is improved through the use of computational science integrated development environments (or CSIDE, pronounced "sea side") such as MATLAB. ParaM is a CSIDE distribution which provides parallel execution of MATLAB scripts for high performance computing (HPC) systems. ParaM runs on a range of processor architectures (e.g., x86, x64, Itanium, PowerPC) and its Message Passing Interface (MPI) binding, known as bcMPI, supports a number of interconnect architectures (e.g., Myrinet and Infinband). In this paper, we describe our goals for the ParaM project, the current status of the project and report on successes and challenges of ParaM support on High Performance Computing Modernization Program (HPCMP) systems at the US Army Research Laboratory (ARL) and the Aeronautical Systems Center (ASC). © 2007 IEEE. . Keywords: Bridges; High performance liquid chromatography; Information management; MATLAB; Message passing; Modernization; Research; Research laboratories; Software engineering; Technology; Computational sciences; Current status; Department of Defense; High performance computing modernization program; High-performance computing systems; Integrated development environments; Interconnect architectures; Itanium; Matlab scripts; Message-passing interface; Myrinet; Parallel executions; PowerPC; Processor architectures; Programmer productivity; US Army; Computer systems
2007-01-01 - Computer systems	The performance goal of high-performance computing (HPC) has shifted from teraflops to petaflops. Sun Microsystems has declared that it will build a 1.7 petaflops machine using AMD Opteron quadcore chips, code-named Barcelona. IBM announced that its BlueGene/P would reach 3 petaflops, using quad-core PowerPC 450 chips. The Digital Virtual Aerodynamics Range developed at ARL uses coupled computational fluid dynamics and rigid body dynamics of complex guided projectiles. The Columbia cluster of Intel Itanium 2 processors at NASA Ames has been used to compute debris trajectories and possible impacts during space shuttle ascent and potential effects of tile damage during reentry. Sun's Niagara 2 chip also packs eight cores, and runs at 2.0 GHz or faster. . Keywords:
2007-01-01 - Density functional calculation of the electronic structure on insulin hexamer	An all-electron calculation on the non-symmetric insulin hexamer was successfully carried out using a precise initial guess of the combined quasi-canonical localized orbitals and the parallel density functional program employing the distributed matrices. The numbers of residues, atoms, and orbitals were 306, 4728, and 26 790, respectively. To our knowledge, insulin hexamer is the largest system calculated using the full-scale canonical density functional molecular orbital method. The calculation for the initial guess and the insulin hexamer took 26.5 h using 24 Xeon processors (3.2 GHz) and 65 h using 64 Itanium 2 processors (1.3 GHz), respectively. © 2006 Elsevier B.V. All rights reserved. . Keywords: Insulin; Molecular structure; Probability density function; Functional programs; Insulin hexamers; Orbitals; Xeon processors; Electronic structure
2007-01-01 - Developing a computational science IDE for HPC systems	Software engineering studies have shown that programmer productivity is improved through the use of computational science integrated development environments (or CSIDE, pronounced "sea side") such as MATLAB. ParaM is a CSIDE distribution which provides parallel execution of MATLAB scripts for HPC systems. ParaM runs on a range of processor architectures (e.g., x86, x64, Itanium, PowerPC) and its MPI binding, known as bcMPI, supports a number of interconnect architectures (e.g., Myrinet and Infinband). In this paper, we describe our goals for the ParaM project, the current status of the project and report on initial software engineering successes and challenges. © 2007 IEEE. . Keywords: Computer architecture; MATLAB; Parallel processing systems; Program processors; Computational science; Integrated development environments (IDE); Interconnect architectures; Software engineering
2007-01-01 - Development of parallel density functional program using distributed matrix to calculate all-electron canonical wavefunction of large molecules	We developed a new parallel density-functional canonical molecular-orbital program for large molecules based on the resolution of the identity method. In this study, all huge matrices were decomposed and saved to the distributed local memory. The routines of the analytical molecular integrals and numerical integrals of the exchange-correlation terms were parallelized using the single program multiple data method. A conventional linear algebra matrix library, ScaLAPACK, was used for matrix operations, such as diagonalization, multiplication, and inversion. Anderson's mixing method was adopted to accelerate the self-consistent field (SCF) convergence. Using this program, we calculated the canonical wavefunctions of a 306-residue protein, insulin hexamer (26,790 orbitals), and a 133-residue protein, interleukin (11,909 orbitals) by the direct-SCF method. In regard to insulin hexamer, the total parallelization efficiency of the first SCF iteration was estimated to be 82% using 64 Itanium 2 processors connected at 3.2 GB/s (SGI Altix3700), and the calculation successfully converged at the 17-th SCF iteration. By adopting the update method, the computational time of the first and the final SCF loops was 229 min and 156 min, respectively. The whole computational time including the calculation before the SCF loop was 2 days and 17 h. This study put the calculations of the canonical wavefunction of 30,000 orbitals to practical use. © 2007 Wiley Periodicals, Inc. . Keywords: Computational methods; Correlation methods; Linear algebra; Molecular structure; Numerical methods; Proteins; Distributed local memory; Distributed matrix; Large molecules; Parallelization; Density functional theory
2007-01-01 - Future fab	Intel's Automated Manufacturing Technology (AMT) is a software suite that monitors and controls the hundreds of steps wafers must pass through on their way to becoming Pentium, Itanium and Core 2 Dou processors, as well as other high-end Intel microprocessors. The software also plays a crucial role in Intel's multibillion-dollar-a-year research and development (R&D) operations. The suite has four major components: the Manufacturing Execution System, the Process Control Automation Framework, the Engineering Analysis Framework and the Material Handling and Tool Control. Each is composed of several programs and each of those programs controls a different part of the chip-making process. . Keywords: Automation; Computer software; Control systems; Manufacture; Research and development management; Technology; Automated manufacturing technology; Chip-making process; Microprocessor chips
2007-01-01 - High performance FFT on SGI Altix 3700	We have developed a high-performance FFT on SGI Altix 3700, improving the efficiency of the floating-point operations required to compute FFT by using a kind of loop fusion technique. As a result, we achieved a performance of 4.94 Gflops at 1-D FFT of length 4096 with an Itanium 21.3 GHz (95% of peak), and a performance of 28 Gflops at 2-D FFT of 40962 with 32 processors. Our FFT kernel outperformed the other existing libraries. © Springer-Verlag Berlin Heidelberg 2007. . Keywords: Digital arithmetic; Digital libraries; Floating-point operations; Loop fusion; Fast Fourier transforms
2007-01-01 - Implementation and evaluation of jacobi iteration on the imagine stream processor	In this paper, we explore an efficient streaming implementation of Jacobi iteration on the Imagine platform. Especially, we develop four programming optimizations according to different stream organizations, involving using SP, dot product, row product and multi-row product methods, each highlighting different aspects of the underlying architecture. The experimental results show that the multi-row product optimization of Jacobi iteration on Imagine achieves 2.27 speedup over the corresponding serial program running on Itanium 2. It is certain that Jacobi iteration can efficiently exploit the tremendous potential of Imagine stream processor through programming optimization. © Springer-Verlag Berlin Heidelberg 2007. . Keywords: Computational complexity; Iterative methods; Natural sciences computing; Optimization; Computational intensiveness; Imagine; Jacobi iteration; Matrix-vector multiplication; Jacobian matrices
2007-01-01 - Implementation and evaluation of matrix-matrix multiplication on FT64 stream processor	The FT64 stream processor is a 64-bit stream processor for scientific computing. It is necessary to research efficient implementation of scientific applications on FT64 to exploit the powerful ability. Matrix-matrix multiplication is an important kernel used In many scientific applications. In this paper, we develop two stream implementations of matrix-matrix multiplication on FT64, and optimize these versions by using stripmining technique. Our efforts aim at reducing memory access overhead and improving computational intensiveness. The experimental results show that the optimizing implementations on FT64 achieve high speedup over the corresponding fortran programs running on Itanium 2. It is certain that matrix-matrix multiplication can efficiently exploit the tremendous potential of FT64 processor through programming optimizations. © 2007 IEEE. . Keywords: Information technology; Optimization; Rivers; Technology; Bit stream; Computational intensiveness; Efficient implementation; Fortran programs; International conferences; Itanium; Matrix multiplications; Memory accesses; Scientific applications; Scientific computing; Stream processors; Two-stream; Matrix algebra
2007-01-01 - Iterative compilation with kernel exploration	The increasing complexity of hardware mechanisms for recent processors makes high performance code generation very challenging. One of the main issue for high performance is the optimization of memory accesses. General purpose compilers, with no knowledge of the application context and approximate memory model, seem inappropriate for this task. Combining application-dependent optimizations on the source code and exploration of optimization parameters as it is achieved with ATLAS, has been shown as one way to improve performance. Yet, hand-tuned codes such as in the MKL library still outperform ATLAS with an important speed-up and some effort has to be done in order to bridge the gap between performance obtained by automatic and manual optimizations. In this paper, a new iterative compilation approach for the generation of high performance codes is proposed. This approach is not applicationdependent, compared to ATLAS. The idea is to separate the memory optimization phase from the computation optimization phase. The first step automatically finds all possible decompositions of the code into kernels. With datasets that fit into the cache and simplified memory accesses, these kernels are simpler to optimize, either with the compiler, at source level, or with a dedicated code generator. The best decomposition is then found by a model-guided approach, performing on the source code the required memory optimizations. Exploration of optimization sequences and their parameters is achieved with a meta-compilation language, X language. The first results on linear algebra codes for Itanium show that the performance obtained reduce the gap with those of highly optimized hand-tuned codes. © Springer-Verlag Berlin Heidelberg 2007. . Keywords: Computer operating systems; Computer programming languages; Computer software portability; Iterative methods; Linear algebra; Storage allocation (computer); Kernel exploration; Memory optimization; Optimization sequences; Program compilers
2007-01-01 - JADAMILU: a software code for computing selected eigenvalues of large sparse symmetric matrices	A new software code for computing selected eigenvalues and associated eigenvectors of a real symmetric matrix is described. The eigenvalues are either the smallest or those closest to some specified target, which may be in the interior of the spectrum. The underlying algorithm combines the Jacobi-Davidson method with efficient multilevel incomplete LU (ILU) preconditioning. Key features are modest memory requirements and robust convergence to accurate solutions. Parameters needed for incomplete LU preconditioning are automatically computed and may be updated at run time depending on the convergence pattern. The software is easy to use by non-experts and its top level routines are written in FORTRAN 77. Its potentialities are demonstrated on a few applications taken from computational physics. Program summary: Program title: JADAMILU. Catalogue identifier: ADZT_v1_0. Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ADZT_v1_0.html. Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland. Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html. No. of lines in distributed program, including test data, etc.: 101 359. No. of bytes in distributed program, including test data, etc.: 7 493 144. Distribution format: tar.gz. Programming language: Fortran 77. Computer: Intel or AMD with g77 and pgf; Intel EM64T or Itanium with ifort; AMD Opteron with g77, pgf and ifort; Power (IBM) with xlf90. Operating system: Linux, AIX. RAM: problem dependent. Word size: real:8; integer: 4 or 8, according to user's choice. Classification: 4.8. Nature of problem: Any physical problem requiring the computation of a few eigenvalues of a symmetric matrix. Solution method: Jacobi-Davidson combined with multilevel ILU preconditioning. Additional comments: We supply binaries rather than source code because JADAMILU uses the following external packages:•MC64. This software is copyrighted software and not freely available. COPYRIGHT (c) 1999 Council for the Central Laboratory of the Research Councils.•AMD. Copyright (c) 2004-2006 by Timothy A. Davis, Patrick R. Amestoy, and Iain S. Duff. All Rights Reserved. Source code is distributed by the authors under the GNU LGPL licence.•BLAS. The reference BLAS is a freely-available software package. It is available from netlib via anonymous ftp and the World Wide Web.•LAPACK. The complete LAPACK package or individual routines from LAPACK are freely available on netlib and can be obtained via the World Wide Web or anonymous ftp.•For maximal benefit to the community, we added the sources we are proprietary of to the tar.gz file submitted for inclusion in the CPC library. However, as explained in the README file, users willing to compile the code instead of using binaries should first obtain the sources for the external packages mentioned above (email and/or web addresses are provided). Running time: Problem dependent; the test examples provided with the code only take a few seconds to run; timing results for large scale problems are given in Section 5. © 2007 Elsevier B.V. All rights reserved. . Keywords: Codes (symbols); Computer software; Convergence of numerical methods; Matrix algebra; Discrete energy states; Eigenvalue computation; Numerical software; Sparse matrices; Eigenvalues and eigenfunctions
2007-01-01 - Leveraging predicated execution for multimedia processing	Modern compression standards such as H.264, DivX, or VC-1 provide astonishing quality at the costs of steadily increasing processing requirements. Therefore, efficient solutions for mobile multimedia devices have to effectively leverage instruction level parallelism (ILP), which is often achieved by the deployment of EPIC (Explicitly Parallel Instruction Computing) architectures. A characteristical architectural feature to increase the available ILP in the presence of control flow is predicated execution. Compilers targeting those hardware platforms are responsible to carefully convert control flow into conditional/predicated instructions - a process called if-conversion. We describe an effective if-conversion algorithm for the CHILI - a novel hardware architecture specifically designed for digital video processing and mobile multimedia consumer electronic. Several architectural characteristics such as the lack of branch prediction units, large delay slots, and the provided predication model are significantly different from previous work, typically aiming mainstream architectures such as Intel Itanium. The algorithm has been implemented for an optimizing compiler based on LLVM. Experimental results using a cycle accurate simulator for the well known benchmark suite MiBench and several multimedia codecs show a speed improvement of about 18% on average. On the same programs, our compiler achieves a speedup of 21% in comparison to the existing code generator based on gcc. © 2007 IEEE. . Keywords: Benchmarking; Computer graphics; Computer programming languages; Embedded systems; Integrated circuits; Program compilers; Program processors; Real time systems; Technical presentations; Video signal processing; Architectural features; Benchmark suites; Branch predictions; Compression standards; Control flows; Conversion algorithms; Cycle accurate simulators; Digital video processing; Efficient solutions; Existing codes; Explicitly parallel instruction computing; Hardware architectures; Hardware platforms; Instruction level parallelisms; Itanium; Large delays; Mobile multimedias; Multimedia codecs; Multimedia processing; Optimizing compilers; Predicated executions; Predication models; Speed improvements; Speed-up; Multimedia systems
2007-01-01 - Loop optimization using hierarchical compilation and kernel decomposition	The increasing complexity of hardware features for recent processors makes high performance code generation very challenging. In particular, several optimization targets have to be pursued simultaneously (minimizing L1/L2/L3/TLB misses and maximizing instruction level parallelism). Very often, these optimization goals impose different and contradictory constraints on the transformations to be applied. We propose a new hierarchical compilation approach for the generation of high performance code relying on the use of state-of-the-art compilers. This approach is not application-dependent and do not require any assembly hand-coding. It relies on the decomposition of the original loop nest into simpler kernels, typically 1D to 2D loops, much simpler to optimize. We successfully applied this approach to optimize dense matrix muliply primitives (not only for the square case but to the more general rectangular cases) and convolution. The performance of the optimized codes on Itanium 2 and Pentium 4 architectures outperforms ATLAS and in most cases, matches hand-tuned vendor libraries (e.g. MKL). © 2007 IEEE. . Keywords: Hierarchical systems; Network architecture; Optimization; Program compilers; Matrix muliply; Optimization goals; Computer hardware
2007-01-01 - New data distribution for solving triangular systems on distributed memory machines	The aim is to present a new data distribution of triangular matrices that provides steady distribution of blocks among processes and reduces memory wasting compared to the standard block-cyclic data layout used in the ScaLAPACK library for dense matrix computations. A new algorithm for solving triangular systems of linear equations is also introduced. The results of experiments performed on a cluster of Itanium 2 processors and Cray X1 show that in some cases, the new method is faster than corresponding PBLAS routines PSTRSV and PSTRSM. © Springer-Verlag Berlin Heidelberg 2007. . Keywords: Block codes; Computation theory; Distributed computer systems; Linear equations; Matrix algebra; Problem solving; Data distribution; Dense matrix computations; Distributed memory machines; Data processing
2007-01-01 - OpenMP implementation of SPICE3 circuit simulator	In this paper, we describe our experience of creating an OpenMP implementation of the SPICE3 circuit simulator program. Given the irregular patterns of access to dynamic data structures in the SPICE code, a parallelization using current standard OpenMP directives is impossible without major rewriting of the original program. The aim of this work is to present a case study showing the development of a shared memory parallel code with minimum effort. We present two implementations, one with minimal code modification and one without modification to the original SPICE3 program using Intel's taskq construct. We also discuss the results of the case study in terms of what future compiler tools may be needed to help OpenMP application developers with similar porting goals. Our experiments using SPICE3, based on SRAM model simulation, were compiled by the SUN compiler running on a SunFire V880 UltraSPARC-III 750 MHz and by the Intel icc compiler running on both an IBM Itanium with four CPUs and Intel Xeon of two processors machines. The results are promising. © 2007 Springer Science+Business Media, LLC. . Keywords: Computer resource management; Data structures; Interfaces (computer); Parallel programming; SPICE; Storage allocation (computer); Code modification; OpenMP SPICE circuit simulators; Shared memory programming models; Circuit simulation
2007-01-01 - Optimal versus heuristic global code scheduling	We present a global instruction scheduler based on integer linear programming (ILP) that was implemented experimentally in the Intel Itanium® product compiler. It features virtually the full scale of known EPIC scheduling optimizations, more than its heuristic counterpart in the compiler, GCS, and in contrast to the latter it computes optimal solutions in the form of schedules with minimal length. Due to our highly efficient ILP model it can solve problem instances with 500-750 instructions, and in combination with region scheduling we are able to schedule routines of arbitrary size. In experiments on five SPEC® CPU2006 integer benchmarks, ILP-scheduled code exhibits a 32% schedule length advantage and a 10% runtime speedup over GCS-scheduled code, at the highest compiler optimization levels typically used for SPEC submissions. We further study the impact of different code motion classes, region sizes, and target microarchitectures, gaining insights into the nature of the global instruction scheduling problem. © 2007 IEEE. . Keywords: Codes (standards); Codes (symbols); Heuristic methods; Integer programming; Linear programming; Linearization; Optimization; Program processors; Scheduling; Arbitrary size; Code scheduling; Compiler optimizations; Full scale; Global instruction scheduling; Highly efficient; ILP models; Instruction schedulers; Integer linear programming (ILP); International symposium; Micro architectures; Micro-architectures; Motion classes; Optimal Solutions; Problem instances; Run time; Schedule length; Program compilers
2007-01-01 - Parallel simulation of missile flow field based on cluster system	A 18 nodes high performance computing cluster system which consists of 36 itanium 2 processors and infiniband network was constructed. The cluster s Unpack performance achieves 188.6 Giga floating-point operations per second (GFlops) after optimization. The cluster system was extended to a parallel CFD (Computational Fluid Dynamics) numerical simulation platform, then. A three-dimension flow field model of tactical missile which consists of 783 million structured grids was made, and the arithmetic of partitioning grids was used to transplant the model to the cluster system to measure its performance. The result shows that the CFD numerical simulation platform s hardware and software configuration is reasonable, and can meet the needs of medium-sized parallel CFD simulation. . Keywords: Computational fluid dynamics; Computer simulation; Flow fields; Optimization; Parallel processing systems; Transplants; Computational Fluid Dynamics (CFD); Numerical simulation; Parallel computing; Tactical missile; Missiles
2007-01-01 - Parallelization techniques for tabu search	Tabu search is a simple, yet powerful meta-heuristic based on local search. This paper presents current taxonomy of parallel tabu search algorithms and compares three parallel TS algorithms based on Tabucol, an algorithm for graph coloring. The experiments were performed on two different high performance architectures: an Itanium-based cluster and a shared-memory Sun server. The results are based on graphs available from the DIMACS benchmark suite. © Springer-Verlag Berlin Heidelberg 2007. . Keywords: Algorithms; Cluster analysis; Graph theory; Heuristic methods; Parallel processing systems; Graph coloring; Parallelization; Tabu search
2007-01-01 - Performance and security lessons learned from virtualizing the alpha processor	Virtualization has become much more important throughout the computer industry both to improve security and to support multiple workloads on the same hardware with effective isolation between those workloads. The most widely used chip architecture, the Intel and AMD x86 processors, have begun to support virtualization, but the initial implementations show some limitations. This paper examines the virtualization properties of the Alpha architecture with particular emphasis on features that improve performance and security. It shows how the Alpha's features of PALcode, address space numbers, software handling of translation buffer misses, lack of used and modified bits, and secure handling of unpredictable results all contribute to making virtualization of the Alpha particularly easy. The paper then compares the virtual architecture of the Alpha with Intel's and AMD's virtualization approaches for x86. It also comments briefly on Intel's virtualization technology for Itanium, IBM's zSeries and pSeries hypervisors and Sun's UltraSPARC virtualization. It particularly identifies some differences between translation buffers on x86 and translation buffers on VAX and Alpha that can have adverse performance consequences. Copyright 2007 ACM. . Keywords: Buffer storage; Computer architecture; Microprocessor chips; Program processors; Security of data; Chip architecture; Software handling; Virtual machine monitors; Virtualization; Virtual reality
2007-01-01 - Performance driven data cache prefetching in a dynamic software optimization system	Software or hardware data cache prefetching is an efficient way to hide cache miss latency. However effectiveness of the issued prefetches have to be monitored in order to maximize their positive impact while minimizing their negative impact on performance. In previous proposed dynamic frameworks, the monitoring scheme is either achieved using processor performance counters or using specific hardware. In this work, we propose a prefetching strategy which does not use any specific hardware component or processor performance counter. Our dynamic framework wants to be portable on any modern processor architecture providing at least a prefetch instruction. Opportunity and effectiveness of prefetching loads is simply guided by the time spent to effectively obtain the data. Every load of a program is monitored periodically and can be either associated to a dynamically inserted prefetch instruction or not. It can be associated to a prefetch instruction at some disjoint periods of the whole program run as soon as it is efficient. Our framework has been implemented for Itanium-2 machines. It involves several dynamic instrumentations of the binary code whose overhead is limited to only 4% on average. On a large set of benchmarks, our system is able to speed up some programs by 2% - 143%. Copyright 2007 ACM. . Keywords: Binary codes; Buffer storage; Computer software; Optimization; Resource allocation; Binary instrumentation; Data cache prefetching; Dynamic optimization; Data processing
2007-01-01 - Performance evaluation of the dual-core based SGI Altix 4700	The newest SGI Altix system is the 4000 series that uses dual-core Itanium-2 p9000 series processors. It differs from the Altix 3000 series in the processor architecture and in the type and number of the component modules. Here we compare and contrast between the Altix 4700 and Altix 3700/BX2 using a set of communication benchmarks, kernel benchmarks, and NSF application benchmarks. The communication benchmarks show that the 4700 has a higher effective bandwidth than the 3700/BX2 while the computation benchmarks show the two systems perform at comparable rates except where the application was able to take advantage of a larger L2 cache and faster memory bus on the 4700. © 2007 IEEE. . Keywords: Computer architecture; Computer science; Computer systems; Computers; High performance liquid chromatography; Component modules; Dual cores; Effective bandwidth; High performance computing (HiPC); International symposium; Itanium; L2 cache; Memory bus; Performance evaluation (PE); Processor architectures; Benchmarking
2007-01-01 - Propagation of UWB pulses on metal strips printed on semiconductor dielectrics	The interaction of ultra-wideband pulses propagating on a semiconductor substrate is analysed without solving the charge dynamic equations. This study demonstrates the systematic approach for selecting semiconductor parameters and their significance in the practical design of high speed microstrip interconnects. To our knowledge, this is the first time such a comprehensive and conceptual study of EMP interaction with semiconductor dielectrics has been performed. The comparison of waveforms on single and coupled lines elucidates the mechanisms of pulse distortion on semiconductor substrates. However, the realization of the dielectric relaxation is studied by changing the doping concentrations of the substrate. Moreover, the evolution of currents for different conductivities illustrates the realization of the doping concentration in a semiconductor. Furthermore, the study on layered substrates with different doping concentrations and relative permittivities demonstrates the fidelity of waveforms on the source line and significantly weak coupling to the victim line. This analysis is a first step towards the full-fledged self-consistent simulations of EMP interaction with the physical model of semiconductor devices. A 160 node Itanium II cluster processor has an average 5 hours simulation time in this work. © 2007 EuMA. . Keywords: Dielectric materials; Electric conductivity; Electromagnetic pulse; Flow interactions; Microwaves; Optical interconnects; Semiconductor device manufacture; Semiconductor device models; Semiconductor devices; Semiconductor materials; Strip metal; Telecommunication systems; Wireless telecommunication systems; Charge dynamics; Cluster processors; Conceptual studies; Coupled lines; Doping concentrations; High speeds; Itanium; Layered substrates; Metal strips; Microstrip interconnects; Physical models; Pulse distortions; Relative permittivities; Semi-conductors; Semiconductor parameters; Semiconductor substrates; Simulation times; Systematic approaches; UWB pulses; Weak couplings; Wideband pulses; Substrates
2007-01-01 - Rectangular full packed format for LAPACK algorithms timings on several computers	We describe a new data format for storing triangular and symmetric matrices called RFP (Rectangular Full Packed). The standard two dimensional arrays of Fortran and C (also known as full format) that are used to store triangular and symmetric matrices waste nearly half the storage space but provide high performance via the use of level 3 BLAS. Standard packed format arrays fully utilize storage (array space) but provide low performance as there are no level 3 packed BLAS. We combine the good features of packed and full storage using RFP format to obtain high performance using L3 (level 3) BLAS as RFP is full format. Also, RFP format requires exactly the same minimal storage as packed format. Each full and/or packed symmetric/triangular routine becomes a single new RFP routine. We present LAPACK routines for Cholesky factorization, inverse and solution computation in RFP format to illustrate this new work and to describe its performance on the IBM, Itanium, NEC, and SUN platforms. Performance of RFP versus LA-PACK full routines for both serial and SMP parallel processing is about the same while using half the storage, Performance is roughly one to a factor of 33 for serial and one to a factor of 100 for SMP parallel times faster than LAPACK packed routines. Existing LAPACK routines and vendor LAPACK routines were used in the serial and the SMP parallel study, respectively. In both studies vendor L3 BLAS were used. © Springer-Verlag Berlin Heidelberg 2007. . Keywords: Computer architecture; Factorization; Matrix algebra; Parallel processing systems; Two dimensional; Data formats; LAPACK packed routines; Symmetric matrices; Data structures
2007-01-01 - Research on the recovery technology of floating-point parameters in binary translation	The technology of parameters recovery is very important for researchers in the area of designing of binary translation systems. It has much to do with architecture, compiler, operating system, and many other aspects. The recovery of floating-point parameters is more complex, compared with the recovery of parameters in other types. This paper presents a new approach to floating-point parameters recovery based on the concept of parameter slot. It makes the recovery of parameters easy and especially precise to implement. This method has been used in the design of a binary translation system ITA, which is designed for a 64-bit architecture Itanium. Through the test using the programs in SPEC2000, this method is proved to be sound and effective in the ITA translation system. . Keywords: Computer science; Digital arithmetic; Program translators; 64-bit architectures; Binary translation; ITA; Itanium; New approaches; Parameter slot; Recovery technology; Translation systems; Recovery
2007-01-01 - Solution of large complex problems in computational electromagnetics using higher order basis in MOM with parallel solvers	In this paper, a general purpose parallel electromagnetic (EM) solver is presented for complicated EM scattering and radiation problems. The parallel algorithm has been designed for multiple platforms and is presented here on four typical operating systems with different hardware, which include IA32 with Windows, IA64 with Windows and EM64T with Linux and Windows operating systems and a DELL PowerEdge 1855 Blade Cluster and Itanium II server in hardware. Techniques have been successfully developed to solve the problem of access to more than 2 GB RAM per process. Since there is no commercial parallel ScaLAPACK solver available for Itanium 2 with Windows Operating System, we also developed ScaLAPACK solver which is presented in this paper. Numerical results shown in this paper indicate that the parallel solver can be run across multiple platforms to solve complicated EM problems very efficiently. ©2007 IEEE. . Keywords: Antenna accessories; Antennas; Chlorine compounds; Computer operating systems; Computer software; Crack propagation; Electromagnetism; Energy management; Magnetism; Parallel algorithms; Windows; Complex problems; Computational electromagnetics; Electrically large objects; Electromagnetic solver; Em scattering; Higher-order; International symposium; Itanium; Method of moments; Multiple platforms; Numerica l results; Operating systems; P arallel solvers; Parallel computation; Vivaldi array; Windows - operating systems; Windows operating system
2007-01-01 - The efficiency of application of dual-processor computers for the analysis of the three-dimensional second harmonic generation problem	The efficiency of application of dual-processor computers, various operating systems, and library functions adapted to the used processors for increasing the execution speed of a computer program used for solving a system of two 3D nonlinear Schrödinger equations is analyzed. The computation speeds obtained on computers with 32-bit and 64-bit processors are compared. It is shown that the use of all mentioned possibilities and the Intel Itanium 2 processor provides from to threefold to tenfold total saving of time, depending on the operating system and the computer type. Parallelization of the algorithm for different operating systems in combination with the use of a library adapted to the processor can increase the program's execution speed by a factor of 1.5-3. © 2007 Allerton Press, Inc. . Keywords:
2007-01-01 - Three-dimensional bursting and parallel computing	This work presents a mathematical model and its parallel implementation via two parallel paradigms for the simulation of three-dimensional bursting phenomena. The mathematical model consists of four nonlinearly coupled partial differential equations and includes fast and slow subsystems. The differential equations have been discretized by means of a linearly implicit finite difference method in equally spaced grids. The resulting system of equations at each time level has been solved by means of an optimized preconditioned conjugate gradient method. The proposed mathematical model has been implemented via (i) a message passing paradigm based on the standard MPI and (ii) a shared address space paradigm based on SPMD OpenMP. The two implementations have been evaluated on three current parallel architectures, namely, a cluster of dual Xeon, a SGI Altix 3700 Bx2 system based on Itanium, and a Sun Fire E15K. It is shown that for the conditions reported here, the nonlinear dynamics of the three-dimensional bursting phenomena exhibits four different regimes, charachterized by asynchronous, synchronous, and then asynchronous oscillations before a quiescent state is reached. In addition, the fast system reaches steady state in much less time than the slow variables. It is also shown that both parallel pradigms lead to similar scalability on all considered platforms. © 2007 by Begell House, Inc. . Keywords: Computer simulation; Conjugate gradient method; Finite difference method; Mathematical models; Message passing; Optimization; Partial differential equations; Address space paradigm; Asynchronous oscillations; MPI; Parallel processing systems
2007-01-01 - Transient analysis of the human body with a mobile phone subject to intense UWB pulses	Self-consistent 3-D FDTD analysis of a bounded-wave EMP simulator with a human model in the presence of a complex electrical environment is performed. The designed EMP simulator could be directly applied to the optimization of EMP for a desired level of the field intensity and risetime in UWB pulsed applications. Using a fine mesh and an Itanium II 160 processor cluster allowed the risetime of 1.4 ns - very close to the experimental design of 1 ns. This study is significantly important for investigating the effects of intense UWB pulses on biological cells. A detailed analysis from the pointof-view of physical understanding the effects of EMP on human tissue is presented. The biological cell of high conductivity i.e. small relaxation time inducing minimum field inside the human body is studied. A detailed picture of electric field and current evolution is demonstrated. © 2007 IEEE. . Keywords: Antenna accessories; Antennas; Behavioral research; Chlorine compounds; Crack propagation; Electric fields; Electromagnetic field theory; Electromagnetic fields; Electromagnetic pulse; Telecommunication; Telecommunication equipment; Telecommunication systems; Three dimensional; Applications.; Biological cells; EMP simulator; Experimental Design; FDTD analysis; Field intensity; High conductivity; Human bodies; Human modelling; Human tissues; International symposium; Itanium; Processor clusters; Rise time; UWB pulses; Transient analysis
2007-01-01 - What is Itanium Memory Consistency from the Programmer's Point of View?	A programmer-centric model describes the memory consistency rules of amultiprocessor as a collection, one for each processor, of 'views' of instructions and some agreements between these views. It also requires the natural notion of validity: the value read from a shared memory location is the one that was most recently stored, according to a given view. This allows reasoning about programs at a non-operational level in the natural way, not obscured by the implementation details of the underlying architecture. In this paper, we formulate a programmer-centric description of the memory consistency model provided by the Itanium architecture. However, our definition is not tight. We provide two very similar definitions and show that the specification of the Itanium memory model lies between the two. These two definitions are motivated by slightly different implementations of load-acquire instructions. A further entertainment of a handful of other load-acquire rules leads us to question whether the specification of the Itanium memory order [Intel Corporation. A formal specification of the intel itanium processor family memory ordering. http://www.intel.com/, Oct 2002] is indeed faithful to the Itanium architecture intentions. © 2007 Elsevier B.V. All rights reserved. . Keywords: Computer architecture; Mathematical models; Program processors; Storage allocation (computer); Itanium memory models; Itanium multiprocessors; Programmer centric memory consistency; Multiprocessing systems
2008-01-01 - A 65nm 2-billion-transistor quad-core Itanium® processor	An Itanium® processor implemented in 65nm CMOS with 8 layers of Cu contains 2.05B transisyors and measures 21.5×32.5mm. The processor has four dual-threaded cores, a system interface and 30MB of cache. High-speed links allow for peak processor-to-processor bandwidth of 96GB/s and peak memory bandwidth of 34GB/s. ©2008 IEEE. . Keywords: Networks (circuits); Solid state devices; 65nm cmos; Dual-threaded cores; High-speed links; Itanium; Memory bandwidths; System interfaces; Bandwidth
2008-01-01 - A hash-TLB approach for MMU virtualization in Xen/IA64	With advances in hardware-assisted full virtualization technologies, system virtualization based on the virtual machine monitor (VMM) has received much recent attention. Using the Xen/IA64 hardware virtual machine implemented on Intel® Virtualization Technology for Itanium® (VT-i), we investigate the design of a virtual software hash translation lookaside buffer (TLB) based on the virtual hash page table (VHPT). Experimental results show that the proposed design can significantly improve the performance of the hardware virtual machine of Xen/IA64. Our contributions are the following. First, we design and implement in the VMM a virtual hash TLB algorithm to optimize the system performance of VT-i guest virtual machines. Second, we quantify experimentally the performance benefits of the hash TLB for VT-i guest virtual machines and analyze the performance impact of the software VHPT walker with the hash TLB algorithm. Lastly, we present experiments to verify, in an SMP virtual machine system environment, the superior scalability of the hash TLB approach. ©2008 IEEE. . Keywords: Boolean functions; Computer networks; Distributed parameter networks; Shape memory effect; Virtual machine; Virtualization; Machine design
2008-01-01 - An effective automated approach to specialization of code	Application performance is heavily dependent on the compiler optimizations. Modern compilers rely largely on the information made available to them at the time of compilation. In this regard, specializing the code according to input values is an effective way to communicate necessary information to the compiler. However, the static specialization suffers from possible code explosion and dynamic specialization requires runtime compilation activities that may degrade the overall performance of the application. This article proposes an automated approach for specializing code that is able to address both the problems of code size increase and the overhead of runtime activities. We first obtain optimized code through specialization performed at static compile time and then generate a template that can work for a large set of values through runtime specialization. Our experiments show significant improvement for different SPEC benchmarks on Itanium-II(IA-64) and Pentium-IV processors using icc and gcc compilers. © 2008 Springer-Verlag Berlin Heidelberg. . Keywords: Benchmarking; Codes (symbols); Linguistics; Parallel processing systems; Technical presentations; Application performances; Automated approaches; Code sizes; Compile times; Compiler optimizations; Dynamic specializations; Input values; Itanium; Overall performances; Pentium; Runtime specializations; Program compilers
2008-01-01 - Analyzing mutual influences of high performance computing programs on SGI Altix 3700 and 4700 systems with PARbench	Nowadays, most high performance computing systems run in multiprogramming mode with several user programs simultaneously utilizing the available CPUs. Even though most current SMP systems are implemented as ccNUMA to reduce the bottleneck of main memory access, the user programs still compete in different ways for resources and influence the scheduler decisions with their generated load. This paper presents the investigation results of the SGI Altix 3700Bx2 of the TU-Dresden and its successor system the Altix 4700 with the PARbench system. The PARbench system is a multiprogramming and multithreading benchmark system, which enables the user to assess the system behaviour under typical production work load and identify bottlenecks and scheduling problems. The Altix 3700 and 4700 with their directory based ccNUMA architecture are the largest SMP systems on the market and promise a high scalability combined with a good isolation of the several system nodes. Several tests validates these promised features and analyzes the connection network and the utilized Intel Itanium 2 Madison (Altix 3700Bx2) and dual core Itanium 2 Montecito (Altix 4700) CPUs. The paper will also show practical problems of the shared system bus by two CPUs each in the 3700 system and compare these situation with the internally shared system bus of the dual core CPUs in the 4700 system. Further tests examine the load balancing behavior and it's consequences to OpenMP parallelized programs under overload. © 2008 The authors and IOS Press. All rights reserved. . Keywords:
2008-01-01 - Bayesian phylogenetic inference in the LIBI grid platform: A tool to explore large data sets	This works presents the porting on the Grid of a parallel application of phylogenetic inference, MrBayes, in the LIBI (International Laboratory of Bioinformatics) platform. Moreover a bioinformatic case study about the analysis of 11 phylogenies, including a series of large sequences data sets that represent the nascent barcode reference database for Lepidoptera is also presented. The case study has been tested on the platform and the speedup and efficiency on Itanium HP XC6000 (Intel Itanium 2 processors and Quadrics interconnects) of the SPACI consortium, University of Salento, are described. © 2008 IEEE. . Keywords: Bioinformatics; Distributed parameter networks; Barcode; Bayesian; Data sets; Grid platforms; Itanium; Itanium 2 processors; Large data sets; Lepidoptera; Parallel applications; Phylogenetic inferences; Reference database; Salento; Applications
2008-01-01 - Circuit design for voltage scaling and SER immunity on a quad-core Itanium® processor	A 700mm2 65nm Itanium® processor triples the logic circuitry of its predecessor. Voltage-frequency scaling to contain power consumption is improved by circuit changes that enable lower voltage operation. Furthermore, per-socket error rate is held constant with the use of SER-hardened latches and register files that reduce SER by 80 to 100x over unprotected structures. ©2008 IEEE. . Keywords: Dynamic frequency scaling; Integrated circuit design; Integrated circuit manufacture; Timing circuits; Voltage scaling; Circuit designs; Error rate; Itanium; Logic circuitry; Lower voltages; Register files; Unprotected structures; Voltage frequency; Computer circuits
2008-01-01 - Configurable Flow Models for FPGA Particle Graphics Engines	We describe the implementation of a hardwareaccelerated particle graphics engine on a reconfigurable computer. The engine incorporates a configurable flow model that enables the simulation of complex spatially-dependent particle graphics effects. The FPGA particle engine was designed using the Mitrion-C high-level language, and did not require detailed hardware design. The engine was implemented on a SGI Altix 350 with four Xilinx Virtex-4 LX200 FPGAs. The engine achieves speedups of 35 to 58 times over a 1.5 GHz Itanium 2 CPU when using one and four FPGAs respectively. © 2008 IEEE. . Keywords: Computers; Field programmable gate arrays (FPGA); Flow simulation; High level languages; Configurable; Flow models; Graphics engines; Hardware designs; Hardware-accelerated; Itanium; Reconfigurable computers; Engines
2008-01-01 - Construction of speculative optimization algorithms	In modern processors, instructions to perform operations are often produced before it becomes known that this is required. Such an expedient, which is called speculative execution, helps to reveal parallelism at the instruction level. In the EPIC architectures, the speculative execution is completely controlled by the compiler, which makes it possible to avoid using complex hardware mechanisms for supporting speculative instruction production. Moreover, the idea of the speculative execution can be used by the compiler in machine-independent optimizations. The paper describes a scheme of construction of the speculative optimization that is based on the selection of properties of the control flow and data flow that are important from the optimization standpoint and on the estimation of the probabilities of their fulfillment. The probabilities found are used for searching and constructing advantageous speculative and bookkeeping transformations. For optimizations that include only speculative movements of instructions upwards along the control flow graph, on the basis of the suggested scheme, a method has been developed that includes algorithms for finding probabilities of data and control dependences, for estimating benefit of speculative movements, and for constructing a recovery code. On the basis of this method, an algorithm for the speculative scheduling of instructions for the Intel Itanium architecture has been developed and implemented. Specific features of its implementation and experimental results are described. © 2008 MAIK Nauka. . Keywords: Computer hardware; Optimization; Parallel processing systems; Program processors; Optimization algorithms; Speculative instruction production; Algorithms
2008-01-01 - Correctly rounded multiplication by arbitrary precision constants	We introduce an algorithm for multiplying a floating-point number χ by a constant C that is not exactly representable in floating-point arithmetic. Our algorithm uses a multiplication and a fused multiply and add instruction. Such instructions are available in some modern processors such as the IBM Power PC and the Intel/HP Itanium. We give three methods for checking whether, for a given value of C and a given floating-point format, our algorithm returns a correctly rounded result for any χ. When it does not, some of our methods return all the values χ for which the algorithm fails. We generalize our study to the case where a wider internal format is used for the intermediate calculations, which gives a fourth method. Our programs and some additional information (such as the case where an arbitrary nonbinary even radix is used), as well as examples of runs of our programs can be downloaded from http://perso.ens-lyon.fr/jean-michel.muller/ MultConstant.html. © 2008 IEEE. . Keywords: Multiplying circuits; Number theory; Program processors; Computer arithmetic; Floating-point arithmetic; Digital arithmetic
2008-01-01 - Design and implement of parallel CFD numerical simulating platform	Aiming at the high performance computing (HPC) needs of computational fluid dynamics (CFD), a cluster system consisting of 36 Itanium 2 processors and Infiniband network was constructed. The linpack value achieved 188.6 GFlops after optimization. The cluster was extended to a parallel CFD numerical simulation platform. A three-dimension flow field model of a tactical missile was built. The performance and precision of the parallel platform were measured. Results show that the hardware and software configuration of the parallel platform is appropriate, and that it can meet the needs of medium and large sized CFD applications. . Keywords:
2008-01-01 - Design and implementation of hardware-assisted virtualization on itanium processor family	With the rapid development of virtualization technology, system virtualization becomes a new hotspot in today's computer system research domain. In addition to the deep analysis of Intel? Virtualization Technology for IA-64 and Xen Virtual Machine Monitor architecture, the design and implementation of Hardware-Assisted VMM for Itanium Processor Family, named as Xen/VT-i, was unveiled, on which unmodified Operating Systems could run with high efficiency and security. In the meantime, the key technologies implemented were proposed, focusing on processor virtualization, memory virtualization, device virtualization and time virtualization. Finally performance on Xen/VT-i Hardware-Assisted virtual machine was analyzed comparing with native system, and prospect on Hardware-Assisted virtualization technology was proposed in addition. . Keywords:
2008-01-01 - Efficient sorting algorithms for the cell broadband engine	The problem of sorting has been studied extensively and many algorithms have been suggested in the literature for the problem. Literature on parallel sorting is abundant. Many of the algorithms proposed, though being theoretically important, may not perform satisfactorily in practice owing to large constants in their time bounds. The algorithms presented in this paper have the potential of being practical. We suggest some novel sorting mechanisms specific to the Cell Broadband engine. We try to utilize the specifics of its architecture in order to get the optimum performance. As part of our comparative analysis we juxtapose these algorithms with similar ones implemented on Itanium 2 processor as well as the Pentium 4 processor. © 2008 IEEE. . Keywords: Arsenic; Ketones; Telecommunication systems; Cell broadband engines; Comparative analysis; Itanium; Its architectures; Optimum performances; Parallel sorting; Pentium 4 processors; Sorting algorithms; Time bounds; Algorithms
2008-01-01 - Evaluating linear recursive filters using novel data formats for dense matrices	The aim of this contribution is to show that the performance of the recently developed high performance algorithm for evaluating linear recursive filters can be increased by using new generalized data structures for dense matrices introduced by F. G. Gustavson. The new implementation is based on vectorized algorithms for banded triangular Toeplitz matrix - vector multiplication and the algorithm for solving linear recurrence systems with constant coefficients. The results of experiments performed on Intel Itanium 2 and Cray X1 are also presented and discussed. © 2008 Springer-Verlag Berlin Heidelberg. . Keywords: Algorithms; Boolean functions; Data structures; Eigenvalues and eigenfunctions; Evolutionary algorithms; File organization; Multitasking; Recursive functions; Scheduling algorithms; Statistics; Trees (mathematics); Wave filters; Applied mathematics; Constant coefficients; Data formatting; Dense matrices; Heidelberg (CO); High performance; international conferences; Itanium; Linear recurrences; parallel processing; Recursive filtering; Springer (CO); Toeplitz matrices; Matrix algebra
2008-01-01 - Fast, frequency-based, integrated register allocation and instruction scheduling	Instruction scheduling and register allocation are two of the most important optimization phases in modern compilers as they have a significant impact on the quality of the generated code. Unfortunately, the objectives of these two optimizations are in conflict with one another. The instruction scheduler attempts to exploit instruction-level parallelism and requires many operands to be available in registers. On the other hand, the register allocator wants register pressure to be kept low so that the amount of spill code can be minimized. Currently these two phases are done separately, typically in three passes: prepass scheduling, register allocation and postpass scheduling. But this separation can lead to poor results. Previous works attempted to solve the phase-ordering problem by combining the instruction scheduler with graph-coloring-based register allocators. The latter tend to be computationally expensive. Linear-scan register allocators, on the other hand, are simple, fast and efficient. In this paper, we describe our effort to integrate instruction scheduling with a linear-scan allocator. Furthermore, our integrated optimizer is able to take advantage of execution frequencies obtained through profiling. Our integrated register allocator and instruction scheduler achieved good code quality with significantly reduced compilation times. On the SPEC2000 benchmarks running on a 900 MHz Itanium II, compared with Open IMPACT, we halved the time spent in instruction scheduling and register allocation with negligible impact on execution times. Copyright © 2007 John Wiley & Sons, Ltd. . Keywords: Benchmarking; Codes (standards); Codes (symbols); Frequency allocation; Optimization; Separation; 900 MHz; Allocator; Allocators; Code generation; Compilers; Computationally expensive; Execution times; Good codes; Instruction schedulers; Instruction scheduling; Instruction-level parallelism; Itanium; Optimizer; Register allocation; Register pressure; Spill code; Two phases; Scheduling
2008-01-01 - From speculation to security: Practical and efficient information flow tracking using speculative hardware	Dynamic information flow tracking (also known as taint tracking) is an appealing approach to combat various security attacks. However, the performance of applications can severely degrade without hardware support for tracking taints. This paper observes that information flow tracking can be efficiently emulated using deferred exception tracking in microprocessors supporting speculative execution. Based on this observation, we propose SHIFT, a low-overhead, software-based dynamic information flow tracking system to detect a wide range of attacks. The key idea is to treat tainted state (describing untrusted data) as speculative state (describing deferred exceptions). SHIFT leverages existing architectural support for speculative execution to track tainted state in registers and needs to instrument only load and store instructions to track tainted state in memory using a bitmap, which results in significant performance advantages. Moreover, by decoupling mechanisms for taint tracking from security policies, SHIFT can detect a wide range of exploits, including high-level semantic attacks. We have implemented SHIFT using the Itanium processor, which has support for deferred exceptions, and by modifying GCC to instrument loads and stores. A security assessment shows that SHIFT can detect both low-level memory corruption exploits as well as high-level semantic attacks with no false positives. Performance measurements show that SHIFT incurs about 1% overhead for server applications. The performance slowdown for SPEC-INT2000 is 2.81X and 2.27X for tracking at byte-level and word-level respectively. Minor architectural improvements to the Itanium processor (adding three simple instructions) can reduce the performance slowdown down to 2.32X and 1.8X for byte-level and word-level tracking, respectively. © 2008 IEEE. . Keywords: Computer science; Computer systems; Computers; Dynamic programming; Industrial management; Information theory; Retail stores; Semantics; Architectural improvements; Architectural support; Decoupling mechanisms; Dynamic information flow tracking; False positives; Hardware supports; Information flows; International symposium; Itanium processor; Memory corruption; Performance measurements; Security assessment; Security attacks; Security policies; Server applications; Software-based; Speculative execution; Computer architecture
2008-01-01 - Implementation and optimization of dense LU decomposition on the stream processor	Developing scientific computing applications on the stream processor has absorbed a lot of researchers attention. In this paper, we implement and optimize dense LU decomposition on the stream processor. Different from other existing parallel algorithms for LU decomposition, StreamLUD algorithm aims at exploiting producer-consumer locality and at overlapping chip-off memory access with kernel execution. Simulation results show that dealing with matrices of different sizes, compared with LUD of HPL on an Itanium 2 processor, StreamLUD we implement and optimize gets a speedup from 2.56 to 3.64 ultimately. © 2008 Springer-Verlag Berlin Heidelberg. . Keywords: Aerospace applications; Algorithms; Computational fluid dynamics; Data storage equipment; Evolutionary algorithms; Multitasking; Natural sciences computing; Optimization; Paper; Parallel algorithms; Statistics; Trees (mathematics); Applied mathematics; Different sizes; Heidelberg (CO); Implementation and optimization; international conferences; Itanium 2 processor; L U decomposition; memory accesses; parallel processing; Scientific computing applications; simulation results; Springer (CO); stream processors; Integer programming
2008-01-01 - Interprocedural speculative optimization of memory accesses to global variables	The discrepancy between processor and memory speed, also known as memory gap, is steadily increasing. This means that execution speed is more and more dominated by memory accesses. We investigate the use of globals, which reside inherently in memory, in standard applications and present an approach to reduce the number of memory accesses, thereby reducing the effect of the memory gap. Our approach can explicitly deal with uncertain information and, hence, optimize more aggressively with the help of speculative techniques while not changing the semantics of the optimized programs. We present an implementation of the proposed optimization in our compiler framework for the Intel Itanium and show that our techniques lead to an increased performance for the SPEC CPU2006 benchmarks, thus showing that the impact of the memory gap can be effectively mitigated with advanced speculative optimization. © 2008 Springer-Verlag Berlin Heidelberg. . Keywords: Benchmarking; Gallium alloys; Global optimization; Information theory; Offshore drilling; Optimization; Program compilers; Real time systems; Speed; Standards; Execution speed; Global variables; Itanium; Memory accesses; Memory speeds; Parallel processing; Speculative optimization; Uncertain informations; Data storage equipment
2008-01-01 - Latency-tolerant software pipelining in a production compiler	In this paper we investigate the benefit of scheduling non-critical loads for a higher latency during software pipelining "Non-critical" denotes those loads that have sufficient slack in the cyclic data dependence graph so that increasing the scheduling distance to their first use can only increase the number of stages of the software pipeline, but should not increase the lengths of the individual stages, the initiation interval (II). The associated cost is in many cases negligible, but the memory stall reduction due to improved latency coverage and load clustering in the schedule can be considerable. We first analyze benefit and cost in theory and then present how we have implemented latency-tolerant pipelining experimentally in the Intel Itanium® product compiler. A key component of the technique is the preselection of likely long-latency loads that is integrated into prefetching heuristics in the high-level optimizer. Only when applied selectively based on these prefetcher hints, the optimization gives the full benefit also without trip-count information from dynamic profiles. Experimental results show gains of up to 14%, with an average of 2.2%, in a wide range of SPEC® CPU2000 and CPU2006 benchmarks. These gains were realized on top of best-performing compiler options typically used for SPEC submissions. Copyright 2008 ACM. . Keywords: Algorithms; Data storage equipment; Graph theory; Optimization; Program compilers; High-level optimizer; Memory stall reduction; Computer software
2008-01-01 - Mapping table based register file design and compiler optimization	Register file design is very important in high performance processor design. Register Stack and Register Stack Engine are effective ways to improve performance. Compiler optimizations are often driven by specific assumptions about the underlying architecture and implementation of the target machine. In this paper, we present our efforts to design and implement register file management mechanism-MTRM (Mapping Table-based Register Management) on EDSMT, which is a kind of SMT architecture based on IA-64. MTRM assigns a Mapping Table for each thread to mapping their logic registers to physic registers, which adds a middle level into Itanium's original rename mechanism. MTRM focused on supporting the effective sharing of registers in an EDSMT processor, using register renaming to permit multiple threads to share a single global register file. Existing hardware is effective at allocating physical registers;it has only limited ability to identify register deallocation points. Compile optimization is considered to deallocate dead registers, while Special Bit and Special Instruction are used as two effective ways. Simulation results indicate that these mechanisms can reduce register deallocation ineffciencies;in particular, on small register files, the best of the schemes attains speedups of up to 2.2 for some applications, and 1.8 on average. . Keywords: Microprocessor chips; Optimization; Explicitly parallel instruction computing (EPIC); Mapping table based register management; Register file; Simultaneous multithreading (SMT); Program compilers
2008-01-01 - MV-FT: Efficient implementation for matrix-vector multiplication on FT64 stream processor	In this paper, we present a detailed case study of the optimizing implementation of a fundamental scientific kernel, matrix-vector multiplication, on FT64, which is the first 64-bit stream processor designed for scientific computing. The major novelties of our study are as follows. First, we develop four stream programs according to different stream organizations, involving dot product, row product, multi-dot product and multirow product approaches. Second the optimal strip size for partitioning the large matrix is put forward based on a practical parameter model. Finally loop unrolling and software pipelining are used to hide the communications with the computations. The experimental results show that the optimizing implementations on FT64 achieve high speedup over the corresponding Fortran programs running on Itanium 2. It is certain that matrix-vector multiplication can efficiently exploit the tremendous potential of FT64 stream processor through programming optimizations. © 2008 IEEE. . Keywords: Digital arithmetic; Matrix algebra; Optimization; Vectors; Bit stream (BS); Case studies; Digital society; Efficient implementation; Fortran programs; International conferences; Itanium; Loop unrolling; Matrix vector multiplication (MVM); Parameter modelling; Software pipelining; Stream processors; Rivers
2008-01-01 - OLTP workloads on modern processor: Characterization and analysis	This paper analysis of how OLTP workloads interact with modern processors and caches behavior. First, we extend TPC-C, the OLTP-oriented benchmark, to ETPC-C benchmark, for measuring the performance of main-memory database (MMDBMS) more precisely. As the performance of MMDBMS is not affected by disk I/O, it is more sensitive to cache usage. Then using ETPC-C benchmark, we investigated the behavior of caches and processors extensively. We find that the miss stall time is mostly spent on on-CPU-chip caches, that is, the first and second level cache misses are dominant. Furthermore, we find instruction cache (I-cache) stall time of on-CPU-chip is a major component to the memory stall time. The smaller the emulated users, the more proportion the I-cache stall time of on-CPU-chip contributes to the memory stall time. However, if employing index, the system under test (SUT) has more total I-cache stall time than the SUT without index at the same number of emulated users and data population. Another observation is that the SUT with index has a little more branch misprediction rate than the SUT without index in average. Finally, we find only the third level (L3) D-cache stall time rate increases with the number of users. This is because L3 D-cache miss incremental rate is the largest. Under TPC-and ETPC-evaluation, we find that for optimized database performance on modern computers, reducing instruction miss penalty is equally important to reducing data miss penalty; since they are conflict efforts, the best way is to have them balanced. . Keywords: Benchmarking; Cache memory; Calculations; Characterization; Computer architecture; Database systems; Indexing (of information); Magnetic disk storage; Optimization; Data miss; Instruction caches; Instruction miss; Itanium II processor; Modern computers; OLTP Workload; On CPU chip caches; Stall time; Program processors
2008-01-01 - OpenMP parallelization of the METRAS meteorology model: Application to the America's cup	We describe the parallelization of the meteorology model METRAS (MEsoscale TRAnsport and Stream) in the context of the America's Cup 2007 for the South African sailing yacht Shosholoza. METRAS is a community model of the atmosphere whose development is coordinated at the Meteorological Institute, ZMAW, University of Hamburg. The parallelization which is based OpenMP was done at the Steinbuch Centre for Computing (SCC) of the University of Karlsruhe and took advantage of the specific features of the Itanium-2 processors available on the local parallel computer HP XC6000. In this paper, we report on the parallelization of the meteorology model METRAS as well as describe how this parallelized version is being used in the highly challenging context of the America's Cup. © 2008 Springer-Verlag. . Keywords:
2008-01-01 - Optimising a 3D multigrid algorithm for the IA-64 architecture	Multigrid methods are amongst the most efficient algorithms to numerically solve partial differential equations. However, standard implementations usually cannot exploit the potential of modern processors. The IA-64 architecture transferes most complexity to the software side to provide a highly superscalar design with large caches, leading to unique control over the actual execution. Exemplified on a simple multigrid solver equation in 3D and the Itanium 2 processor, we present how known performance optimisation techniques can be successfully combined. While implementation details are specific, the optimisation concept should be applicable for a wide range of numerical algorithm and CPUs. © 2008, Inderscience Publishers. . Keywords: Program processors; Cache blocking; IA-64; Multi-grid; Performance optimisation; Streaming execution; Computer architecture
2008-01-01 - Optimizing code through iterative specialization	Code specialization is a way to obtain significant improvement in the performance of an application. It works by exposing values of different parameters in source code. The availability of these specialized values enables the compilers to generate better optimized code. Although most of the efficient source code implementations contain specialized code to benefit from these optimizations, the real impact of specialization may however vary depending upon the value of the specializing parameter. In this paper, we suggest the specialization of code to acquire an iterative approach. For some specialized code, wo search for a better version of code by re-specializing the code, followed by a low-level code analysis. The specialized versions fulfilling the required criteria arc then transformed to generate another equivalent version of the original specialized code. The approach has been tested on Itanium-II architecture using ice compiler. The results show significant improvement in the performance of different benchmarks. Copyright 2008 ACM. . Keywords: Benchmarking; Computer software; Computers; Linguistics; Optimization; Program compilers; Analysis and transformation; Code analysis; Code specializations; Compiling techniques; Itanium; Iterative approaches; Program optimization and specialization; Programming languages implementation; Source code implementations; Source codes; Codes (symbols)
2008-01-01 - Optimizing scientific application loops on stream processors	This paper describes a graph coloring compiler framework to allocate on-chip SRF (Stream Register File) storage for optimizing scientific applications on stream processors. Our framework consists of first applying enabling optimizations such as loop unrolling to expose stream reuse and opportunities for maximizing parallelism, i.e., overlapping kernel execution and memory transfers. Then the three SRF management tasks are solved in a unified manner via graph coloring: (1) placing streams in the SRF, (2) exploiting stream use, and (3) maximizing parallelism. We evaluate the performance of our compiler framework by actually running nine representative scientific computing kernels on our FT64 stream processor. Our preliminary results show that compiler management achieves an average speedup of 2.3x compared to First- Fit allocation. In comparison with the performance results obtained from running these benchmarks on Itanium 2, an average speedup of 2.1x is observed. Copyright © 2008 ACM. . Keywords: Acoustic streaming; Computer programming languages; Embedded systems; Graph theory; Optimization; Parallel processing systems; Data reuse; Graph colorings; Loop optimizations; Prefetching; Software-managed cache; Stream processor; Program compilers
2008-01-01 - Optimizing scientific application loops on stream processors	This paper describes a graph coloring compiler framework to allocate on-chip SRF (Stream Register File) storage for optimizing scientific applications on stream processors. Our framework consists of first applying enabling optimizations such as loop unrolling to expose stream reuse and opportunities for maximizing parallelism, i.e., overlapping kernel execution and memory transfers. Then the three SRF management tasks are solved in a unified manner via graph coloring: (1) placing streams in the SRF, (2) exploiting stream use, and (3) maximizing parallelism. We evaluate the performance of our compiler framework by actually running nine representative scientific computing kernels on our FT64 stream processor. Our preliminary results show that compiler management achieves an average speedup of 2.3x compared to First-Fit allocation. In comparison with the performance results obtained from running these benchmarks on Itanium 2, an average speedup of 2.1x is observed. © 2008 ACM. . Keywords: Acoustic streaming; Benchmarking; Computer software reusability; Digital storage; Graph theory; Parallel processing systems; Data reuse; Graph colorings; Loop optimizations; Prefetching; Stream processor; Program compilers
2008-01-01 - Partitioning technologies and dynamic reconfiguration of mission-critical IA server PRIMEQUEST	The PRIMEQUEST series of mission-critical IA servers provides the high performance, reliability, availability, and flexibility required by mission-critical systems on an open platform. In particular, these servers adopt Fujitsu-specific Dual Synchronous System Architecture (DSSA) of the complete independent recovery type, to afford mainframe-class availability. The series also adopts open-standard operating systems (like Linux and Windows), open-standard processors (such as Intel Itanium 2), and optimizers for data centers (to achieve flexible partitioning, multi-OS scaling out, and scaling up). This paper describes some basic technologies of the PRIMEQUEST series, and then explains Extended Partitioning (XPAR) and Dynamic Reconfiguration (DR). XPAR provides up to 16 partitions operating as independent systems by separately allocating the PRIMEQUEST resources using DR as the mechanism for reconfiguring these partitions without having to reset the system. . Keywords: Computer operating systems; Data storage equipment; Program processors; Software architecture; Dual Synchronous System Architecture (DSSA); Mission-critical systems; Servers
2008-01-01 - Perfmon2: A leap forward in performance monitoring	This paper describes the software component, perfmon2, that is about to be added to the Linux kernel as the standard interface to the Performance Monitoring Unit (PMU) on common processors, including x86 (AMD and Intel), Sun SPARC, MIPS, IBM Power and Intel Itanium. It also describes a set of tools for doing performance monitoring in practice and details how the CERN openlab team has participated in the testing and development of these tools. © 2008 IOP Publishing Ltd. . Keywords:
2008-01-01 - Performance evaluation of NSF application benchmarks on parallel systems	The National Science Foundation (NSF) recently released a set of application benchmarks that would be a key factor in selecting the next-generation high-performance computing environment. These benchmarks are designed to capture the salient attributes of those science and engineering applications placing the most stringent demands on the system to be provisioned. The application benchmarks consist of six codes that require large amount of memory and work with large data sets. In this work, we study the complexity, performance, and scalability of these codes on four machines: a 512-processor SGI Altix 3700, a 512-processor SGI Altix 3700/BX2, a 512-processor dual-core based SGI Altix 4700, and a 128-processor Cray Opteron cluster interconnected by the Myrinet network. We evaluated these codes for two different problem sizes using different numbers of processors. Our results show that per processor the SGI machines, using the Intel Itanium-2 processor, are faster than the Cray cluster, using the AMD Opteron processor, by a factor of up to three. Also, we found out that some of these codes scale up very well as we increase the number of processors while others scaled up poorly. In addition, one of the codes achieved about 2/3 of the peak rate of an SGI Altix processor. Moreover, the dual-core based system achieved comparable performance results to the single-core based system. Finally, we provide some limitations and concluding remarks. ©2008 IEEE. . Keywords: Applications; Chlorine compounds; Computer networks; Computer systems; Data storage equipment; Distributed parameter networks; Parallel processing systems; Dual cores; Parallel and distributed processing; SGI Altix 3700; Benchmarking
2008-01-01 - Pre-virtualization: Soft layering for virtual machines	Despite its current popularity, para-virtualization has an enormous cost. Its deviation from the platform architecture abandons many of the benefits of traditional virtualization: stable and well-defined platform interfaces, hypervisor neutrality, operating system neutrality, and upgrade neutrality - in sum, modularity. Additionally, para-virtualization has a significant engineering cost. These limitations are accepted as inevitable for significantly better performance, and for the ability to provide virtualization-like behavior on non-virtualizable hardware such as x86. Virtualization and its modularity solve many systems problems, and when combined with the performance of para-virtualization become even more compelling. We show how to achieve both together. We still modify the guest operating system, but according to a set of design principles that avoids lock-in, which we call soft layering. Additionally, our approach is highly automated and thus reduces the implementation and maintenance burden of para-virtualization, which is especially useful for enabling obso-leted operating systems. We demonstrate soft layering on x86 and Itanium: we can load a single Linux binary on a variety of hypervisors (and thus substitute virtual machine environments and their enhancements), while achieving essentially the same performance as para-virtualization with less effort. ©2008 IEEE. . Keywords: Computer operating systems; Computer systems; Modernization; Design principles; Engineering costs; Hypervisor; Itanium; Operating systems; Platform architectures; Platform interfaces; Virtual machine environments; Virtual machines; Virtualization; Computer architecture
2008-01-01 - Reconstructing control flow in modulo scheduled loops	Software pipelining is a loop optimization technique used to exploit instruction level parallelism in the loop. EPIC architectures, such as Intel IA-64 (Itanium) provide extensive hardware support for software pipelining to generate compact and highly parallel code. However it transforms explicit conditional branches into implicit control flow based on the information of the guard registers. It is difficult to reconstruct precise control flow from the optimized code. This paper describes an approach to reconstruct implicit control flow in modulo scheduled loops and thereby improve the quality of reverse engineering optimized executables. We also demonstrate the effectiveness of this approach through experiment results. © 2008 IEEE. . Keywords: Codes (symbols); Communication; Computer networks; Cybernetics; Information science; Reengineering; Reverse engineering; Conditional branches; Decompilation; Modulo scheduling; Predication execution; Register rotation; Optimization
2008-01-01 - Scientific computing applications on a stream processor	Stream processors, developed for the stream programming model, perform well on media applications. In this paper we examine the applicability of a stream processor to scientific computing applications. Eight scientific applications, each having different performance characteristics, are mapped to a stream processor. Due to the novelty of the stream programming model, we show how to map programs in a traditional language, such as FORTRAN. In a stream processor system, the management of system resources is the programmers' responsibility. We present several optimizations, which enable mapped programs to exploit various aspects of the stream processor architecture. Finally, we analyze the performance of the stream processor and the presented optimizations on a set of scientific computing applications. The stream programs are from 1.67 to 32.5 times faster than the corresponding FORTRAN programs on an Itanium 2 processor, with the optimizations playing an important role in realizing the performance improvement. © 2008 IEEE. . Keywords: Codes (symbols); Computer programming languages; Computer systems; FORTRAN (programming language); Optimization; Rivers; Technical presentations; FORTRAN language; Fortran programs; International symposium; Itanium 2 processor; Media applications; Performance analyses; Performance characteristics; Performance improvements; Scientific applications; Scientific computing applications; Stream processors; Stream programming; System resources; Systems-and-software; Computer hardware description languages
2008-01-01 - Shell projects dual-core assisted performance gains of up to 800 percent	Computational imaging and modeling tools are critical to discovering and developing reserves as well as to increasing production from existing fields. The Shell E&P division uses a proprietary reservoir modeling application called Dynamo. The software gives analyzes seismic data, which are gathered from sound waves reflected by subsurface strata, to provide detailed images of belowground features. Shell opted to test the software's performance on a server using dual-core Intel Itanium processors for a performance boost. Shell E&P worked closely with Intel to optimize Dynamo for the Itanium-based platform, recompiling the application for Itanium processors using compilers specifically developed for EPIC architecture. They also employed Intel VTune Performance Analyzer, which runs natively on Itanium-based systems using the Linux operating system. Compared to a reduced instruction set computer (RISC)-based platform, the dual-core Itanium processor also had a superior price-to-performance ratio. . Keywords: Computer operating systems; Electric generators; Interactive devices; Microprocessor chips; Program compilers; Seismology; Shells (structures); Computational imaging; Dual-core; EPIC architecture; Increasing production; Itanium; Itanium processor; LINUX- operating system; Modeling tool; Performance Gain; Performance ratio; Recompiling; Reduced instruction set computers; Reservoir modeling; Seismic datas; Sound waves; Computer architecture
2008-01-01 - Solving linear-quadratic optimal control problems on parallel computers	We discuss a parallel library of efficient algorithms for the solution of linear-quadratic optimal control problems involving large-scale systems with state-space dimension up to O(104). We survey the numerical algorithms underlying the implementation of the chosen optimal control methods. The approaches considered here are based on invariant and deflating subspace techniques, and avoid the explicit solution of the associated algebraic Riccati equations in case of possible ill-conditioning. Still, our algorithms can also optionally compute the Riccati solution. The major computational task of finding spectral projectors onto the required invariant or deflating subspaces is implemented using iterative schemes for the sign and disk functions. Experimental results report the numerical accuracy and the parallel performance of our approach on a cluster of Intel Itanium-2 processors. . Keywords: Differential equations; Disks (structural components); Intersections; Large scale systems; Linear equations; Numerical methods; Optimal control systems; Probability density function; Quadratic programming; Riccati equations; State space methods; Algebraic Riccati equation; Algebraic riccati equations; Computational tasks; Disk function; Efficient algorithms; Explicit solutions; Itanium; Iterative schemes; Linear-quadratic optimal control; Numerical accuracies; Numerical algorithms; Optimal control methods; Parallel computers; Parallel libraries; Parallel performances; Quadratic optimal controls; Results reports; Riccati solutions; Sign function; Space dimensions; Spectral; Subspace techniques; Parallel algorithms
2009-01-01 - 2DRMP: A suite of two-dimensional R-matrix propagation codes	The R-matrix method has proved to be a remarkably stable, robust and efficient technique for solving the close-coupling equations that arise in electron and photon collisions with atoms, ions and molecules. During the last thirty-four years a series of related R-matrix program packages have been published periodically in CPC. These packages are primarily concerned with low-energy scattering where the incident energy is insufficient to ionise the target. In this paper we describe 2DRMP, a suite of two-dimensional R-matrix propagation programs aimed at creating virtual experiments on high performance and grid architectures to enable the study of electron scattering from H-like atoms and ions at intermediate energies. Program summary: Program title: 2DRMP. Catalogue identifier: AEEA_v1_0. Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEEA_v1_0.html. Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland. Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html. No. of lines in distributed program, including test data, etc.: 196 717. No. of bytes in distributed program, including test data, etc.: 3 819 727. Distribution format: tar.gz. Programming language: Fortran 95, MPI. Computer: Tested on CRAY XT4 [1]; IBM eServer 575 [2]; Itanium II cluster [3]. Operating system: Tested on UNICOS/lc [1]; IBM AIX [2]; Red Hat Linux Enterprise AS [3]. Has the code been vectorised or parallelised?: Yes. 16 cores were used for small test run. Classification: 2.4. External routines: BLAS, LAPACK, PBLAS, ScaLAPACK. Subprograms used: ADAZ_v1_1. Nature of problem: 2DRMP is a suite of programs aimed at creating virtual experiments on high performance architectures to enable the study of electron scattering from H-like atoms and ions at intermediate energies. Solution method: Two-dimensional R-matrix propagation theory. The (r1, r2) space of the internal region is subdivided into a number of subregions. Local R-matrices are constructed within each subregion and used to propagate a global R-matrix, ℜ, across the internal region. On the boundary of the internal region ℜ is transformed onto the IERM target state basis. Thus, the two-dimensional R-matrix propagation technique transforms an intractable problem into a series of tractable problems enabling the internal region to be extended far beyond that which is possible with the standard one-sector codes. A distinctive feature of the method is that both electrons are treated identically and the R-matrix basis states are constructed to allow for both electrons to be in the continuum. The subregion size is flexible and can be adjusted to accommodate the number of cores available. Restrictions: The implementation is currently restricted to electron scattering from H-like atoms and ions. Additional comments: The programs have been designed to operate on serial computers and to exploit the distributed memory parallelism found on tightly coupled high performance clusters and supercomputers. 2DRMP has been systematically and comprehensively documented using ROBODoc [4] which is an API documentation tool that works by extracting specially formatted headers from the program source code and writing them to documentation files. Running time: The wall clock running time for the small test run using 16 cores and performed on [3] is as follows: bp (7 s); rint2 (34 s); newrd (32 s); diag (21 s); amps (11 s); prop (24 s). References: [1]HECToR, CRAY XT4 running UNICOS/lc, http://www.hector.ac.uk/, accessed 22 July, 2009.[2]HPCx, IBM eServer 575 running IBM AIX, http://www.hpcx.ac.uk/, accessed 22 July, 2009.[3]HP Cluster, Itanium II cluster running Red Hat Linux Enterprise AS, Queen s University Belfast, http://www.qub.ac.uk/directorates/InformationServices/Research/HighPerformanceComputing/Services/Hardware/HPResearch/, accessed 22 July, 2009.[4]Automating Software Documentation with ROBODoc, http://www.xs4all.nl/~rfsber/Robo/, accessed 22 July, 2009. © 2009 Elsevier B.V. All rights reserved. . Keywords: Application programming interfaces (API); Atoms; Computer operating systems; Corrosion prevention; Electron energy loss spectroscopy; Electron scattering; Electrons; Experiments; FORTRAN (programming language); Impact ionization; Ions; Problem oriented languages; Program documentation; Software packages; Solar concentrators; Supercomputers; Test facilities; Two dimensional; Atomic collision processes; Electron impact excitation; Electron impact ionisation; IERM; Intermediate energies; R-matrix; RMPS; Two-dimensional R-matrix propagation; Matrix algebra
2009-01-01 - 3-D brain MRI tissue classification on FPGAs	Many automatic algorithms have been proposed for analyzing magnetic resonance imaging (MRI) data sets. With the increasingly large data sets being used in brain mapping, there has been a significant rise in the need for accelerating these algorithms. Partial volume estimation (PVE), a brain tissue classification algorithm for MRI, was implemented on a field-programmable gate array (FPGA)-based high performance reconfigurable computer using the Mitrion-C high-level language (HLL). This work develops on prior work in which we conducted initial studies on accelerating the prior information estimation algorithm. In this paper, we extend the work to include probability density estimation and present new results and additional analysis. We used several simulated and real human brain MR images to evaluate the accuracy and performance improvement of the proposed algorithm. The FPGA-based probability density estimation and prior information estimation implementation achieved an average speedup over an Itanium 2 CPU of 2.5 × and 9.4 ×, respectively. The overall performance improvement of the FPGA-based PVE algorithm was 5.1 × with four FPGAs. © 2009 IEEE. . Keywords: Algorithms; Brain; Brain Mapping; Computer Simulation; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Markov Chains; Phantoms, Imaging; Reproducibility of Results; Sensitivity and Specificity; Signal Processing, Computer-Assisted; Time Factors; Algorithms; Brain; C (programming language); Classification (of information); Estimation; Field programmable gate arrays (FPGA); Medical imaging; Probability density function; Reconfigurable hardware; Resonance; Tissue; Automatic algorithms; Brain mapping; Brain MRI; Brain tissue; Classification; Data sets; Human brain; Itanium; Large datasets; New results; Partial volumes; Performance improvements; Prior information; Probability density estimation; Reconfigurable computer; Reconfigurable computing; Tissue classification; algorithm; article; brain; brain mapping; computer simulation; histology; human; image processing; image quality; methodology; nuclear magnetic resonance imaging; physiology; probability; reproducibility; sensitivity and specificity; signal processing; time; Magnetic resonance imaging
2009-01-01 - A 65 nm 2-billion transistor quad-core itanium processor	This paper describes an Itanium processor implemented in 65 nm process with 8 layers of Cu interconnect. The 21.5 mm by 32.5 mm die has 2.05B transistors. The processor has four dual-threaded cores, 30 MB of cache, and a system interface that operates at 2.4 GHz at 105$\,{C}}$. High speed serial interconnects allow for peak processor-to-processor bandwidth of 96 GB/s and peak memory bandwidth of 34 GB/s. © 2006 IEEE. . Keywords: Computers; Copper; Dies; Electric clocks; Integrated circuit manufacture; Microprocessor chips; Optical interconnects; Telecommunication systems; Transistors; 65-nm process technology; Circuit design; Clock distribution; Microprocessor; On-die cache; Voltage domains; Computer architecture
2009-01-01 - A computational science IDE for HPC systems: Design and applications	Software engineering studies have shown that programmer productivity is improved through the use of computational science integrated development environments (or CSIDE, pronounced "sea side") such as MATLAB. Scientists often desire to use high-performance computing (HPC) systems to run their existing CSIDE scripts with large data sets. ParaM is a CSIDE distribution that provides parallel execution of MATLAB scripts on HPC systems at large shared computer centers. ParaM runs on a range of processor architectures (e.g., x86, x64, Itanium, PowerPC) and its MPI binding, known as bcMPI, supports a number of interconnect architectures (e.g., Myrinet and InfiniBand). On a cluster at Ohio Supercomputer Center, bcMPI with blocking communication has achieved 60% of the bandwidth of an equivalent C/MPI benchmark. In this paper, we describe goals and status for the ParaM project and the development of applications in signal and image processing that use ParaM. © 2008 Springer Science+Business Media, LLC. . Keywords: Benchmarking; Computer architecture; High performance liquid chromatography; Image processing; Linguistics; MATLAB; Software engineering; Supercomputers; Computational science; High performance computing; High-level language; Infiniband; Integrated development environments; Interconnect architectures; Itanium; Large data sets; MPI; Myrinet; Parallel executions; PGAS; PowerPC; Processor architectures; Programmer productivities; Signal and image processing; High level languages
2009-01-01 - A practical approach to hardware performance monitoring based dynamic optimizations in a production JVM	While the concept of online profile directed dynamic optimizations using hardware performance monitoring unit (PMU) data is not new, it has seen fairly limited or no use in commercial JVMs. The main reason behind this fact is the set of significant challenges involved in (1) obtaining low overhead and usable profiling support from the underlying platform (2) the complexity of filtering, interpreting and using precise PMU events online in a JVM environment (3) demonstrating the total runtime benefit of PMU data based optimizations above and beyond regular online profile based optimizations. In this paper we address all three challenges by presenting a practical framework for PMU data collection and use within a high performance product JVM on a highly scalable server platform. Our experiments with JavaTM workloads using the SunTM HotspotTM JDK 1.6 JVM on the Intel ® Itanium ® platform indicate that the hardware data collection overhead (less than 0.5%) is not as significant as the challenge of extracting the precise information for optimization purposes. We demonstrate the feasibility of mapping the instruction IP address based hardware event information to the runtime components as well as the JIT server compiler internal data structures for use in optimizations within a dynamic environment. We also evaluated the additional performance potential of optimizations such as object co-location during garbage collection and global instruction scheduling in the JIT compiler with the use of PMU generated load latency information. Experimental results show performance improvements of up to 14% with an average of 2.2% across select Java server benchmarks such as SPECjbb2005[16], SPECjvm2008[17] and Dacapo[18]. These benefits were observed over and above those provided by profile guided server JVM optimizations in the absence of hardware PMU data. © 2009 IEEE. . Keywords: Computer hardware; Data acquisition; Data structures; Hardware; Just in time production; Network components; Program compilers; Refuse collection; Waste disposal; Colocation; Data collection; Data-based optimization; Dynamic environments; Dynamic optimization; Garbage collection; Global instruction scheduling; Hardware performance; Hardware performance monitoring; High performance products; IP address; Itanium; Java; Java servers; JIT compiler; Low overhead; Performance improvements; Performance potentials; Runtime; Server platform; Structural optimization
2009-01-01 - Building routes to customers: Proven strategies for profitable growth	Building Routes to Customers explains a powerful approach to maximizing your organization's success by getting the right products and services to the right customers through the right channels at the right time. World-class organizations and fledgling startups alike have employed these strategies and tactics to achieve profitable growth in volatile markets. Through in-depth analysis and dozens of illustrative examples, the authors show you how to employ the "Routes-to-Market" methodology to optimize the productivity of marketing, sales and customer service in your organization. "A key challenge in dynamic and fast changing markets is getting marketing and sales aligned. This book shows how to do this effectively and drive tactical execution better to achieve a dramatic increase in marketing and sales productivity."-Ravi Venkatesan, Chairman of Microsoft Corporation (India) "Routes-to-Market came as a breakthrough for IBM at a very challenging time in our industry. It had a big impact on our bottom line by enabling us to grow sales with a much more cost-effective mix of selling resources. Many companies need to solve that challenge today, before their competitors do. This book shows how to do it."-Ned Lautenbach, Partner, Clayton, Dubilier & Rice, formerly Senior Vice President-Worldwide Sales & Services, IBM "At Adobe we spent millions of dollars with consultants, both large and small. In most cases, a few months later, you couldn't remember the work they did. RTM was simple yet powerful and had lasting value to the company. It made it possible for each product manager to apply the correct resources and achieve an excellent ROI. Companies without this kind of methodology are flying blind."-Kyle Mashima, VP of Strategic Development, Visible Measures Incorperated, formerly VP of Strategic Development, Adobe Systems Incorporated. "Technology innovation is not limited to the lab or the manufacturing process. Successfully marketing new technologies is about understanding change and helping customers adopt a new technology to create significant business value. RTM is a practical roadmap for maximizing revenue and profitability throughout the entire product life cycle. This book is a must read for anyone looking to drive technology adoption in today's evolving markets."-Joan Jacobs, Executive Director of Itanium Solutions Alliance, formerly Global Alliance Director, Hewlett-Packard. © 2009 Peter Raulerson, Jean-Claude Malraison and Antoine Leboyer. All rights reserved. . Keywords:
2009-01-01 - Computer Performance Evalution and Benchmarking-SPEC Benchmark workshop 2009,Proceeding	The proceedings contain 9 papers. The topics discussed include: SPECrate2006: alternatives considered, lessons learned; SPECjvm2008 performance characterization; performance characterization of Itanium® 2-based montecito processor; a tale of two Processors: revisiting the RISC-CISC debate; investigating cache parameters of x86 family processors; the next frontier for power/performance benchmarking: energy efficiency of storage subsystems; thermal design space exploration of 3D die stacked multi-core processors using geospatial-based predictive models; Generation, validation and analysis of SPEC CPU2006 simulation points based on branch, memory and TLB characteristics; and a note on the effects of service time distribution in the M/G/1 queue. . Keywords:
2009-01-01 - Control flow obfuscation with information flow tracking	Recent micro-architectural research has proposed various schemes to enhance processors with additional tags to track various properties of a program. Such a technique, which is usually referred to as information flow tracking, has been widely applied to secure software execution (e.g., taint tracking), protect software privacy and improve performance (e.g., control speculation). In this paper, we propose a novel use of information flow tracking to obfuscate the whole control flow of a program with only modest performance degradation, to defeat malicious code injection, discourage software piracy and impede malware analysis. Specifically, we exploit two common features in information flow tracking: the architectural support for automatic propagation of tags and violation handling of tag misuses. Unlike other schemes that use tags as oracles to catch attacks (e.g., taint tracking) or speculation failures, we use the tags as flow-sensitive predicates to hide normal control flow transfers: the tags are used as predicates for control flow transfers to the violation handler, where the real control flow transfer happens. We have implemented a working prototype based on Itanium processors, by leveraging the hardware support for control speculation. Experimental results show that BOSH can obfuscate the whole control flow with only a mean of 26.7% (ranging from 4% to 59%) overhead on SPECINT2006. The increase in code size and compilation time is also modest. Copyright 2009 ACM. . Keywords: Computer crime; Crime; Security of data; Architectural research; Architectural support; Code size; Common features; Control flow obfuscation; Control flows; Hardware supports; Information flows; Itanium processor; Malicious codes; Malwares; Normal controls; Performance degradation; Secure software; Program processors
2009-01-01 - Developing energy efficient filtering systems	Processing large volumes of information generally requires massive amounts of computational power, which consumes a significant amount of energy. An emerging challenge is the development of "environmentally friendly" systems that are not only efficient in terms of time, but also energy efficient. In this poster, we outline our initial efforts at developing greener filtering systems by employing Field Programmable Gate Arrays (FPGA) to perform the core information processing task. FPGAs enable code to be executed in parallel at a chip level, while consuming only a fraction of the power of a standard (von Neuman style) processor. On a number of test collections, we demonstrate that the FPGA filtering system performs 10-20 times faster than the Itanium based implementation, resulting in considerable energy savings. . Keywords: Data processing; Energy conservation; Energy efficiency; Information retrieval; Information services; Chip-level; Computational power; Energy efficient; Energy saving; Environmentally-friendly; Filtering systems; Information processing; Itanium; Test Collection; Field programmable gate arrays (FPGA)
2009-01-01 - Development of polyaniline nanocomposite thin film micro-gas sensor array for the qualitative analysis of inorganic vapor	A novel chemical micro-gas sensor array composed of polyaniline (PANI) and its nanocomposite thin film was fabricated for NH3, CO and H 2 gas classification. Itanium dioxide (TiO2), indium oxide (In2O3) and multi-walled carbon nanotube (PANI/MWNT) combined with PANI were chosen as the sensing materials by the step-clustering analysis. The normalization method was developed to eliminate the dispersion of the array data. Probabilistic neuron network (PNN) was designed and trained with the processed data, and the different discrimination results were discussed under the different spread constant (SP). The accurate classification result was achieved when SP was set as 0.05 or 0.1. ©2009 IEEE. . Keywords: Carbon nanotubes; Chemical sensors; Cluster analysis; Computational fluid dynamics; Gas detectors; Gases; Multiwalled carbon nanotubes (MWCN); Nanocomposites; Neural networks; Polyaniline; Sensor arrays; Thin film devices; Thin films; Classification results; Clustering analysis; Constant-step; Gas classification; Indium oxide; Inorganic vapors; Itanium; Micro-gas sensor array; Nanocomposite thin films; Normalization methods; Polyaniline nanocomposites; Probabilistic neuron networks; Qualitative analysis; Sensing material; TiO; Quality control
2009-01-01 - Dynamic frequency-switching clock system on a quad-core itanium® processor	[No abstract available] . Keywords:
2009-01-01 - Enabling data structure oriented performance analysis with hardware performance counter support	An interesting and as of yet under-represented aspect of program development and optimization are data structures. Instead of analyzing data with respect to code regions, the objective here is to see how performance metrics are related to data structures. With the advanced performance monitoring unit of Intel's Itanium processor series such an analysis becomes possible. This paper describes how the hardware features of the Itanium 2 processor are exploited by the perfmon and PAPI performance monitoring APIs and how PAPI's support for address range restrictions has been integrated into an existing profiling tool to achieve the goal of data structure oriented profiling in the context of OpenMP applications. © Springer-Verlag Berlin Heidelberg 2009. . Keywords: Computer hardware; Data structures; Technical presentations; Hardware features; Hardware performance counters; Itanium 2 processor; Itanium processor; OpenMP applications; Performance analysis; Performance metrics; Performance monitoring; Profiling tools; Program development; Under-represented; Structural optimization
2009-01-01 - Enabling high-productivity SIP application development: Modeling and simulation of superconducting quantum interference filters	The inherent complexity in utilizing and programming high performance computing (HPC) systems is the main obstacle to widespread exploitation of HPC resources and technologies in the Department of Defense (DoD). Consequently, there is the persistent need to simplify the programming interface for the generic user. This need is particularly acute in the Signal/Image Processing (SIP), Integrated Modeling and Test Environments (IMT), and related DoD communities where typical users have heterogeneous unconsolidated needs. Mastering the complexity of traditional programming tools (C, MPI, etc.) is often seen as a diversion of energy that could be applied to the study of the given scientific domain. Many SIP users instead prefer high-level languages (HLLs) within integrated development environments, such as MATLAB. We report on our collaborative effort to use a HLL distribution for HPC systems called ParaM to optimize and parallelize a compute-intensive Superconducting Quantum Interference Filter (SQIF) application provided by the Navy SPAWAR Systems Center in San Diego, CA. ParaM is an open-source HLL distribution developed at the Ohio Supercomputer Center (OSC), and includes support for processor architectures not supported by MATLAB (e.g., Itanium and POWER5) as well as support for high-speed interconnects (e.g., InfiniBand and Myrinet). We make use of ParaM installations available at the Army Research Laboratory (ARL) DoD Supercomputing Resource Center (DSRC) and OSC to perform a successful optimization/parallelization of the SQIF application. This optimization/ parallelization may be used to assess the feasibility of using SQIF devices as extremely sensitive detectors for electromagnetic radiation which is of great importance to the Navy and DoD in general. © 2010 IEEE. . Keywords: Computer architecture; Computer simulation; Computer software selection and evaluation; Distributed computer systems; Electromagnetic waves; Internet protocols; Quantum interference devices; Radiation detectors; Research laboratories; Supercomputers; Superconductivity; Army research laboratories; Collaborative efforts; Department of Defense; Electromagnetic radiation; High performance computing systems; High-speed interconnects; Infiniband; Inherent complexity; Integrated development environment; Integrated modeling; Itanium; Modeling and simulation; Myrinet; Open-source; Parallelizations; Processor architectures; Programming interface; Programming tools; Resource center; San Diego , CA; Sensitive detector; SIP application; Superconducting quantum interference filters; Test Environment; High level languages
2009-01-01 - FARM_2DRMP: A version of FARM for use with 2DRMP	To complete the 2DRMP package an asymptotic program, such as FARM, is needed. The original version of FARM is designed to construct the physical R-matrix, R, from surface amplitudes contained in the H-file. However, in 2DRMP, R has already been constructed for each scattering energy during propagation. Therefore, this modified version of FARM, known as FARM_2DRMP, has been developed solely for use with 2DRMP. New version program summary: Program title: FARM_2DRMP. Catalogue identifier: ADAZ_v1_1. Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ADAZ_v1_1.html. Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland. Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html. No. of lines in distributed program, including test data, etc.: 13 806. No. of bytes in distributed program, including test data, etc.: 134 462. Distribution format: tar.gz. Programming language: Fortran 95 and MPI. Computer: Tested on CRAY XT4 [1]; IBM eServer 575 [2]; Itanium II cluster [3]. Operating system: Tested on UNICOS/lc [1]; IBM AIX [2]; Red Hat Linux Enterprise AS [3]. Has the code been vectorized or parallelized?: Yes. 16 cores were used for the small test run. Classification: 2.4. External routines: BLAS, LAPACK. Does the new version supersede the previous version?: No. Nature of problem: The program solves the scattering problem in the asymptotic region of R-matrix theory where exchange is negligible. Solution method: A radius is determined at which the wave function, calculated as a Gailitis expansion [4] with accelerated summing [5] over terms, converges. The R-matrix is propagated from the boundary of the internal region to this radius and the K-matrix calculated. Collision strengths or cross sections may be calculated. Reasons for new version: To complete the 2DRMP package [6] an asymptotic program, such as FARM [7], is needed. The original version of FARM is designed to construct the physical R-matrix, R, from surface amplitudes contained in the H-file. However, in 2DRMP, R, has already been constructed for each scattering energy during propagation and each R is stored in one of the RmatT files described in Fig. 8 of [6]. Therefore, this modified version of FARM, known as FARM_2DRMP, has been developed solely for use with 2DRMP. Instructions on its use and corresponding test data is provided with 2DRMP [6]. Summary of revisions: FARM_2DRMP contains two codes, farm.f and farm_par.f90. The former is a serial code while the latter is a parallel F95 code that employs an MPI harness to enable the nenergy energies to be computed simultaneously across ncore cores, with each core processing either ⌊ nenergy / ncore ⌋ or ⌈ nenergy / ncore ⌉ energies. The input files, input.d and H, and the output file farm.out are as described in [7]. Both codes read R directly from RmatT. Restrictions: FARM_2DRMP is for use solely with 2DRMP and for a specified L, S and Π combination. The energy range specified in input.d must match that specified in energies.data. Running time: The wall clock running time for the small test run using 16 cores and performed on [3] is 9 secs. References: [1]HECToR, CRAY XT4 running UNICOS/lc, http://www.hector.ac.uk/, visited 22 July, 2009.[2]HPCx, IBM eServer 575 running IBM AIX, http://www.hpcx.ac.uk/, visited 22 July, 2009.[3]HP Cluster, Itanium II cluster running Red Hat Linux Enterprise AS, Queen's University Belfast, http://www.qub.ac.uk/directorates/InformationServices/Research/HighPerformanceComputing/Services/Hardware/HPResearch/, visited 22 July, 2009.[4]M. Gailitis, J. Phys. B 9 (1976) 843.[5]C.J. Noble, R.K. Nesbet, Comput. Phys. Comm. 33 (1984) 399.[6]N.S. Scott, M.P. Scott, P.G. Burke, T. Stitt, V. Faro-Maza, C. Denis, A. Maniopoulou, Comput. Phys. Comm. 180 (12) (2009) 2424-2449, this issue.[7]V.M. Burke, C.J. Noble, Comput. Phys. Comm. 85 (1995) 471. © 2009 Elsevier B.V. All rights reserved. . Keywords: Asymptotic analysis; Computer operating systems; Corrosion prevention; Electrons; Farms; FORTRAN (programming language); Matrix algebra; Problem oriented languages; Scattering; Solar concentrators; Surface chemistry; Test facilities; Testing; Wave functions; 2DRMP; Atomic collision processes; Electron impact excitation; Electron impact ionization; Intermediate energies; R-matrix; Impact ionization
2009-01-01 - FPC: A high-speed compressor for double-precision floating-point data	Many scientific programs exchange large quantities of double-precision data between processing nodes and with mass storage devices. Data compression can reduce the number of bytes that need to be transferred and stored. However, compression is only likely to be employed in high-end computing environments if it does not impede the throughput. This paper describes and evaluates FPC, a fast lossless compression algorithm for linear streams of 64-bit floating-point data. FPC works well on hard-to-compress scientific datasets and meets the throughput demands of high-performance systems. A comparison with five lossless compression schemes, BZIP2, DFCM, FSD, GZIP, and PLMI, on four architectures and thirteen datasets shows that FPC compresses and decompresses one to two orders of magnitude faster than the other algorithms at the same geometric-mean compression ratio. Moreover, FPC provides a guaranteed throughput as long as the prediction tables fit into the L1 data cache. For example, on a 1.6 GHz Itanium 2 server, the throughput is 670 megabytes per second regardless of what data are being compressed. © 2008 IEEE. . Keywords: Compression ratio (machinery); Computer systems; Data processing; Data storage equipment; Forecasting; Image compression; Linear systems; Throughput; Compression ratios; Computing environments; Data caches; Data models; Data sets; Floating-point compression; Guaranteed throughputs; Itanium; Linear streams; Lossless compression algorithms; Lossless compressions; Mass storage devices; Orders of magnitudes; Other algorithms; Performance systems; Point datums; Precision floating; Prediction methods; Prediction tables; Processing nodes; Scientific datasets; Scientific programs; Data compression
2009-01-01 - Implementing fast packet filters by software pipelining on x86 processors	Packet filters are essential for network traffic/security management on the Internet. Filters implemented by software on general-purpose CPUs are very flexible but occasionally suffer from poor performance. In order to address this problem, we have investigated software pipelining techniques for loops with a number of conditional branches for use in software-based fast packet filters. Based on our previous researches, we herein apply the software pipelining approach in an attempt to increase the filter performance for large filter rules. We validate the effectiveness of the proposed approach on Intel x86-32/64 series, as well as Intel Itanium 2 processors, which speaks to the generality and practicality of the proposed approach. The software pipelined program codes on x86-64 processors are 2.2 times faster than C-compiler-based codes and 1.8 times faster than carefully optimized hand-compiled codes. In addition, the performance of the pipelined codes we obtained on x86-64 processors is comparable to that on Itanium 2 processors with predicate registers. © 2009 Springer. . Keywords: Computer software; Program compilers; Conditional branch; Filter performance; Itanium 2 processor; Packet filters; Poor performance; Program code; Software pipelining; Software-based; Pipeline processing systems
2009-01-01 - Improving performance of optimized kernels through fast instantiations of templates	To fully exploit the instruction-level parallelism offered by modern processors, compilers need the necessary information available during the execution of the program. This advocates for iterative or dynamic compilation. Unfortunately, dynamic compilation is suitable only for applications where the cost of compilation may be amortized by multiple invocations of the same code. Similarly, the cost of iterative compilation makes it impractical to be widely used for performance improvement. In this article, we suggest a novel approach for improving the performance of mathematical kernels through fast instantiations of templates. Optimized templates are generated at static compile time with a limited number of compilations. The initial instantiations of these templates are performed at static compile time, and the runtime instantiations are performed with a very small overhead through specialized data, requiring no computations at runtime. It represents an effective solution in terms of reduced overhead incurring at static compile time and dynamic compile time. The experiments have been performed on an Itanium-II architecture using highly optimized kernels of ATLAS and FFTW with ice and gcc compilers. Copyright © 2008 John Wiley & Sons, Ltd. . Keywords: Iterative methods; Optimization; Code specialization; Compilation techniques; Dynamic compilation; Effective solution; Improving performance; Instruction level parallelism; Iterative compilation; Modern processors; Program compilers
2009-01-01 - Legacy computer systems migration	Many legacy systems designed more than ten years ago are performing their functions more than adequately, however, the hardware/software platforms may no longer be available or supported. The initial cost that would be incurred to re-design these systems to work on the standard platforms of today cannot be cost justified. More importantly, many of these systems did not perform adequately after their initial implementation and companies invested a substantial amount of time and effort into the evolution of these systems, making them as robust and reliable as they are today. Much of these ongoing software maintenance costs, operational downtime, and poor quality issues would again be incurred after a re-design effort. The impact of these issues can be greatly reduced by porting to compatible systems that are available and supported today. . Keywords: Computer software; Computer software maintenance; Iron; Management information systems; Steel; Systems engineering; Alpha; Hewlett packard; Itanium; Migration; OpenVMS; Software; VAX; VMS; Legacy systems
2009-01-01 - Matrix-based streamization approach for improving locality and parallelism on FT64 stream processor	FT64 is the first 64-bit stream processor designed for scientific computing. It is critical to exploit optimizing streamization approaches for scientific applications on FT64 due to the inefficiency of direct streamization approach. In this paper, we propose a novel matrix-based streamization approach for improving locality and parallelism of scientific applications on FT64. First, a Data&Computation Matrix is built to abstract the relationship between loops and arrays of the original programs, and it is helpful for formulating the streamization problem. Second, three key techniques for optimizing streamization approach are proposed based on the transformations of the matrix, i.e., coarse-grained program transformations, fine-grained program transformations, and stream organization optimizations. Finally, we apply our approach to ten typical scientific application kernels on FT64. The experimental results show that the matrix-based streamization approach achieves an average speedup of 2.76 over the direct streamization approach, and performs equally to or better than the corresponding Fortran programs on Itanium 2 except CG. It is certain that the matrix-based streamization approach is a promising and practical solution to efficiently exploit the tremendous potential of FT64. © 2008 Springer Science+Business Media, LLC. . Keywords: Applications; Fourier transforms; Natural sciences computing; Systems analysis; Bit streams; Coarse-grained; Fortrans; FT64; Itanium; Key techniques; matrixes; Practical solutions; Program transformation; Scientific applications; Scientific computing; Stream organization; Stream processors; Streamization; Structured programming
2009-01-01 - Parallel coupling of heterogeneous domains with KOP3D using PACX-MPI	This paper outlines how coupled heterogeneous domains can be distributed in a supercomputing environment using PACX-MPI. KOP3D is a coupling program in the field of aero acoustics, a typical multi-scale application, since on the one hand it has to account for the small vortical structures as the source of the noise and on the other for the long wave length of the acoustic waves. The amount of energy, and with that the pressure, in the sound generating flow is by orders of magnitude higher than the amount of energy carried by the acoustic waves. The acoustical wavelength is much larger than the diameter of the noise-generating vortices, and even the computational domains may vary in a wide range from the geometry which is in the order of meters for an airfoil to the distance between a starting or landing plan and the observer on the ground which is in the order of about 1 km. Through the use of heterogeneous domain decomposition it is possible to reduce the computation time needed to simulate large flow fields by adaptation of the equations, the discretization, the mesh and the time step to the local requirements of the flow simulation within each sub-domain. These locally adapted computations may result in different requirements on the computer architecture. On the other side, in a supercomputing network there are generally different computer architectures available. By matching the sub-domains to the best suited available architectures an even shorter time to solution may be gained. The parallel version of the KOP3D coupling scheme is shown and the benefits of running the simulation distributed on the vector machine NEC-SX8 and on the scalar Itanium II (IA64) machine are demonstrated. A 2D and a 3D testcase are presented. © 2009 Springer-Verlag Berlin Heidelberg. . Keywords: Acoustic waves; Acoustics; Airfoils; Computational geometry; Computer architecture; Computer networks; Computers; Couplings; Domain decomposition methods; Flow simulation; Fluid dynamics; Grid computing; Aeroacoustics; Computation time; Computational domains; Coupling scheme; Discretizations; Heterogeneous domains; Itanium; Long waves; Multiscales; Orders of magnitude; Parallel coupling; Parallel version; Sub-domains; Subdomain; Time step; Time-to-solution; Vector machines; Vortical structures; Computational fluid dynamics
2009-01-01 - Performance characterization of itanium®2-based montecito processor	This paper presents the performance characteristics of the Intel &reg2 Itanium&reg2 2-based Montecito processor and compares its performance to the previous generation Madison processor. Measurements on both are done using the industry-standard SPEC CPU2006 benchmarks. The benchmarks were compiled using the Intel Fortran/C++ optimizing compiler and run using the reference data sets. We analyze a large set of processor parameters such as cache misses, TLB misses, branch prediction, bus transactions, resource and data stalls and instruction frequencies. Montecito achieves 1.14× and 1.16× higher (geometric mean) IPC on integer and floating-point applications. We believe that the results and analysis presented in this paper can potentially guide future IA-64 compiler and architectural research. ©Springer-Verlag Berlin Heidelberg 2009. . Keywords: Benchmarking; Architectural research; Branch prediction; Cache Miss; Geometric mean; Itanium; Optimizing compilers; Performance characteristics; Performance characterization; Reference data; Program compilers
2009-01-01 - Reconfigurable sparse/dense matrix-vector multiplier	We propose an ANSI/IEEE-754 double precision floating-point matrix-vector multiplier. Its main feature is the capability to process efficiently both Dense Matrix-Vector Multiplications (DMVM) and Sparse Matrix-Vector Multiplications (SMVM). The design is composed of multiple processing elements (PE) and is optimized for FPGAs. We investigate theoretically the boundary conditions when the DMVM equals the SMVM performance with respect to the matrix sparsity. Thus, we can determine the most efficient processing mode configuration with respect to the input data sparsity. Furthermore, we evaluate our design both with simulations and on real hardware. We experimented on an Altix 450 machine using the SGI Reconfigurable Application Specific Computing (RASC) services, which couple Dual-Core Itanium-2 processors with Virtex-4 LX200 FPGAs. Our design has been routed and executed on the Altix 450 machine at 100 MHz. Experimental results suggest that only two PEs suffice to outperform the pure software SMVM execution. The performance improvement at the kernel level scales near linearly to the number of configured PEs both for the SMVM and DMVM. Compared to related work, the design does not indicate any performance degradation and performs equally or better than designs optimized either for SMVM or DMVM alone. © 2009 IEEE. . Keywords: Field programmable gate arrays (FPGA); Frequency multiplying circuits; Machine design; Vectors; ANSI/IEEE; Application specific; Dense matrices; Double precision; Dual-core; Input datas; Itanium; Matrix sparsity; Matrix-vector multipliers; Multiple processing; Performance degradation; Performance improvements; Point matrices; Re-configurable; Sparse matrix-vector multiplication; Matrix algebra
2009-01-01 - Seven-O'clock: A new distributed GVT algorithm using network atomic operations	In this paper we introduce a new concept, Network Atomic Operations (NAOs) to create a zero-cost consistent cut. Using NAOs, we define a wall clock time driven Global Virtual Time (GVT) algorithm called The Seven O'clock GVT algorithm that is an extension of Fujimoto's shared memory GVT algorithm. Using this new GVT algorithm, we report good optimistic parallel performance on a cluster of Itanium-II quad processor systems as well as a dated cluster of 40 dual Pentium III systems for both benchmark applications such as PHOLD and real-world applications such as a large-scale TCP/IP internet model. In some cases, super-linear speedup is observed. The Seven O'clock GVT algorithm greatly simplifies processor synchronisation by creating a zero-cost 'consistent cut' across the distributed simulation. Copyright © 2009, Inderscience Publishers. . Keywords: Benchmarking; Clocks; Parallel processing systems; Distributed simulations; Global virtual time; Global virtual time algorithms; High performance computing; Itanium-II quad processor systems; Network atomic operations; Optimistic simulation; Parallel simulations; Clustering algorithms
2009-01-01 - Vine - A numerical code for simulating astrophysical systems using particles. I. Description of the physics and the numerical methods	We present a numerical code for simulating the evolution of astrophysical systems using particles to represent the underlying fluid flow. The code is written in Fortran 95 and is designed to be versatile, flexible, and extensible, with modular options that can be selected either at the time the code is compiled or at run time through a text input file. We include a number of general purpose modules describing a variety of physical processes commonly required in the astrophysical community and we expect that the effort required to integrate additional or alternate modules into the code will be small. In its simplest form the code can evolve the dynamical trajectories of a set of particles in two or three dimensions using a module which implements either a Leapfrog or Runge-Kutta-Fehlberg integrator, selected by the user at compile time. The user may choose to allow the integrator to evolve the system using individual time steps for each particle or with a single, global time step for all. Particles may interact gravitationally as N-body particles, and all or any subset may also interact hydrodynamically, using the smoothed particle hydrodynamic (SPH) method by selecting the SPH module. A third particle species can be included with a module to model massive point particles which may accrete nearby SPH or N-body particles. Such particles may be used to model, e.g., stars in a molecular cloud. Free boundary conditions are implemented by default, and a module may be selected to include periodic boundary conditions. We use a binary "Press" tree to organize particles for rapid access in gravity and SPH calculations. Modules implementing an interface with special purpose "GRAPE" hardware may also be selected to accelerate the gravity calculations. If available, forces obtained from the GRAPE coprocessors may be transparently substituted for those obtained from the tree, or both tree and GRAPE may be used as a combination GRAPE/tree code. The code may be run without modification on single processors or in parallel using OpenMP compiler directives on large-scale, shared memory parallel machines. We present simulations of several test problems, including a merger simulation of two elliptical galaxies with 800,000 particles. In comparison to the Gadget-2 code of Springel, the gravitational force calculation, which is the most costly part of any simulation including self-gravity, is ∼4.6-4.9 times faster with VINE when tested on different snapshots of the elliptical galaxy merger simulation when run on an Itanium 2 processor in an SGI Altix. A full simulation of the same setup with eight processors is a factor of 2.91 faster with VINE. The code is available to the public under the terms of the Gnu General Public License. © 2009. The American Astronomical Society. All rights reserved. . Keywords:
2010-01-01 - A method for debugging of pipelined processors in formal verification by correspondence checking	Presented is a method for debugging of pipelined processors in their formal verification with the highly automatic and scalable approach of Correspondence Checking, where a pipelined/superscalar/VLIW implementation is compared against a non-pipelined specification via an inductive correctness criterion based on symbolic simulation in a way that guarantees the correctness of the implementation for all possible execution scenarios. The benefit from the proposed method increases with the complexity of the processor under formal verification. For a 12-stage VLIW processor that imitates the Intel Itanium in many features, the method reduced the size of the EUFM correctness formulas from buggy processors by up to an order of magnitude, the number of Boolean variables in the equivalent propositional correctness formulas and the number of 1s in the counterexample traces by up to 2 orders of magnitude, and resulted in an average speedup in detecting the bugs of 2 orders of magnitude, thus increasing the productivity of the processor designers. . Keywords: Computer aided design; Digital integrated circuits; Nanotechnology; Very long instruction word architecture; Boolean variables; Correctness criterion; Execution scenario; Formal verifications; Itanium; Order of magnitude; Orders of magnitude; Pipelined processor; Processor designers; Scalable approach; Symbolic simulation; VLIW processor; Pipeline processing systems
2010-01-01 - Exploring the performance of massively multithreaded architectures	We present a new scheme for evaluating the performance of multithreaded computers and demonstrate its application to the Cray MTA-2 and XMT supercomputers. Our scheme is based on the concept of clock cycles per element,C, plotted against both problem size and the number of processors. This scheme clearly shows if an implementation has achieved its asymptotic efficiency and is more general than (but includes) the commonly used speedup metric. It permits the discovery of any imperfections in both the software as well as the hardware, and is expected to permit a unified comparison of many different parallel architectures. Measurements on a number of well-known parallel algorithms, ranging from matrix multiply to quicksort, are presented for the MTA-2 and XMT and highlight some interesting differences between these machines. The performance of sequence alignment using dynamic programming is evaluated on the MTA-2, XMT, IBM x3755 and SGI Altix 350 and provides a useful comparison of the capabilities of the Cray machines with more conventional shared memory architectures. . Keywords: Dynamic programming; Multitasking; Parallel algorithms; Parallel architectures; Parallel processing systems; Supercomputers; Cray MTA; Cray XMT; IBM x3755; Itanium; Multi core; Multi-threading; Opteron; SGI Altix; Shared memory; Memory architecture
2010-01-01 - Extending the Monte Carlo processor modeling technique: Statistical performance models of the Niagara 2 processor	With the complexity of contemporary single- and multi-core, multi-threaded processors comes a greater need for faster methods of performance analysis and design. It is no longer practical to use only cycle-accurate processor simulators for design space analysis of modern processors and systems. Therefore, we propose a statistical processor modeling method that is based on Monte Carlo techniques. In this paper, we present new details of the methodology and the recent extensions that we have made to it, including the capability to model multi-core processors.We detail the steps to develop a new model and then present statistical performance models of the Sun Niagara 2 processor micro-architecture that, together with a previously published Itanium 2 Monte Carlo model, demonstrates the validity of the technique and its new capabilities. We show that we can accurately predict single and multi-core performance within 7% of actual on average, and we can use the models to quickly pinpoint performance problems at various components. © 2010 IEEE. . Keywords: Computer architecture; Cycle accurate; Design spaces; Itanium; Micro architectures; Modeling method; Modeling technique; Modern processors; MONTE CARLO; Monte Carlo model; Monte Carlo techniques; Multi core; Multithreaded processors; New model; Performance analysis; Performance problems; Statistical performance; Monte Carlo methods
2010-01-01 - Feedback-directed specialization of code	Based on feedback information, a large number of optimizations can be performed by the compiler. This information actually indicates the changing behavior of the applications and can be used to specialize code accordingly. Code specialization is a way to facilitate the compiler to perform optimizations by providing the information regarding variables in the code. It is however difficult to select the variables which maximize the benefit of specialization. Also the overhead of specialization and code size increase are the main issues while specializing code. This paper suggests a novel method for improving the performance using specialization based on feedback information and analysis. The code is iteratively specialized after selecting candidate variables by using a heuristic, followed by generation of optimized templates. These templates require a limited set of instructions to be specialized at runtime and are valid for a large number of values. The overhead of runtime specialization is further minimized through optimal software cache of template clones whose instantiation can be performed at static compile time. The experiments have been performed on Itanium-II(IA-64) and Pentium-IV processors using icc and gcc compilers. A significant improvement in terms of execution speed and reduction of code size has been achieved for SPEC and FFTW benchmarks. © 2009 Elsevier Ltd. All rights reserved. . Keywords: Computer software; Linguistics; Network components; Code size; Code specialization; Compile time; Compilers; Dynamic code generation; Execution speed; Feed back information; Itanium; Novel methods; Pentium; Programming languages; Runtime specialization; Runtimes; Specialization; Program compilers
2010-01-01 - Formal verification of object layout for C++ multiple inheritance	Object layout - the concrete in-memory representation of objects - raises many delicate issues in the case of the C++ language, owing in particular to multiple inheritance, C compatibility and separate compilation. This paper formalizes a family of C++ object layout schemes and mechanically proves their correctness against the operational semantics for multiple inheritance of Wasserrab et al. This formalization is flexible enough to account for space-saving techniques such as empty base class optimization and tail-padding optimization. As an application, we obtain the first formal correctness proofs for realistic, optimized object layout algorithms, including one based on the popular "common vendor" Itanium C++ application binary interface. This work provides semantic foundations to discover and justify new layout optimizations; it is also a first step towards the verification of a C++ compiler frontend. Copyright © 2011 ACM. . Keywords: Computer applications; Optimization; Semantics; Application binary interfaces; Base class; C++ language; Formal correctness; Formal verifications; Itanium; Languages; Layout algorithms; Layout optimization; Layout schemes; Multiple inheritance; Operational semantics; Semantic foundation; Separate compilation; High level languages
2010-01-01 - Intel® QuickPath Interconnect architectural features supporting scalable system architectures	Single processor performance has exhibited substantial growth over the last three decades [1] as shown in Figure 1. What is also desired are techniques which enable connecting together multiple processors in order to create scalable, modular and resilient multiprocessor systems. Beginning with the production of the Intel® Xeon® processor 5500 series, (previously codenamed "Nehalem-EP"), the Intel® Xeon® processor 7500 series (previously codenamed "Nehalem-EX"), and the Intel® Itanium™ processor 9300 series (previously codenamed "Tukwila- MC"), Intel Corporation has introduced a series of multi-core processors that can be easily interconnected to create server systems scaling from 2 to 8 sockets. In addition, OEM platforms are currently available that extend this up to 256-socket server designs1. This scalable system architecture is built upon the foundation of the Intel® QuickPath Interconnect (Intel QPI). These Intel micro-architectures provide multiple high-speed (currently up to 25.6 GB/s), point-to-point connections between processors, I/O hubs and third party node controllers. The interconnect features, as well as the capabilities built into the processor's system interconnect logic (also known as "uncore"), work together to deliver the performance, scalability, and reliability demanded in larger scale systems. © 2010 IEEE. . Keywords: Architecture; Intel; Nehalem; QuickPath; Scalable; Xeon; Computer architecture
2010-01-01 - Lessons from at-speed scan deployment on an Intel® Itanium® microprocessor	Lessons learnt during the deployment of transition scan content on an Intel® Itanium® server microprocessor design and its use for electrical debug and defect screening in highvolume manufacturing are described. While many publications in the area of transition scan show it being practiced as an efficient defect screening tool, only a minority of these designs were high-performance microprocessor designs. This work illustrates the benefits of such techniques on complex microprocessors. © 2010 IEEE. . Keywords: At-speed; High-performance microprocessors; High-volume manufacturing; Itanium; Screening tool; Server microprocessor; Defects
2010-01-01 - Newton-Raphson algorithms for floating-point division using an FMA	Since the introduction of the Fused Multiply and Add (FMA) in the IEEE-754-2008 standard [6] for floating-point arithmetic, division based on Newton-Raphson's iterations becomes a viable alternative to SRT-based divisions. The Newton-Raphson iterations were already used in some architecture prior to the revision of the IEEE-754 norm. For example, Itanium architecture already used this kind of iterations [8]. Unfortunately, the proofs of the correctness of binary algorithms do not extend to the case of decimal floating-point arithmetic. In this paper, we present general methods to prove the correct rounding of division algorithms using Newton-Raphson's iterations in software, for radix 2 and radix 10 floating-point arithmetic. © 2010 IEEE. . Keywords: Digital arithmetic; Newton-Raphson method; Binary algorithms; Decimal floating points; Division algorithms; Floating point divisions; General method; Newton Raphson iteration; Newton-Raphson algorithm; Radix-10 floating-point arithmetics; Computer architecture
2010-01-01 - OpenMP parallelization of a mickens time-integration scheme for a mixed-culture biofilm model and its performance on multi-core and multi-processor computers	We document and compare the performance of an OpenMP parallelized simulation code for a mixed-culture biofilm model on a desktop workstation with two quad core Xeon processors, and on SGI Altix Systems with single core and dual core Itanium processors. The underlying model is a parabolic system of highly non-linear partial differential equations, which is discretized in time using a non-local Mickens scheme, and in space using a standard finite difference method. © 2010 Springer-Verlag. . Keywords: Biofilms; Differential equations; Biofilm models; Desktop workstation; Dual core; Itanium processor; Multi core; Multi-processors; Nonlinear partial differential equations; Nonlocal; OpenMP-parallelization; Parabolic system; Simulation code; Time-integration scheme; Xeon processors; Computer simulation
2010-01-01 - Radiation hardened by design SRAM strategies for TID and SEE mitigation	Static random access memory (SRAM) is ubiquitous in modern system-on-a-chip (SOC) integrated circuits (ICs). Due to its value in programmable systems by providing fast scratchpad memory in embedded and real-time applications as well as space for large working sets in microprocessor designs, IC SRAM content continues to grow. As ICs surpass 1 billion transistors, and given the high relative design and power efficiency of memory arrays compared with random logic, SRAM is projected to comprise 90% of the total die area by 2013 [1]. For instance, the Itanium processor has progressed from 6 MB and 9 MB L3 caches on 130 nm fabrication processes to 24 MB caches on the 65 nm technology generation [2-4]. The Xeon processors include 16 MB caches [5]. Consequently, ICs designed for space and other radiation environments require robust SRAM designs if they are to track the size and performance of commercial ICs. © 2011 by Taylor & Francis Group, LLC. . Keywords:
2010-01-01 - Software-implemented fault injection at firmware level	Software-implemented fault injection is an established method to emulate hardware faults in computer systems. Existing approaches typically extend the operating system by special drivers or change the application under test. We propose a novel approach where fault injection capabilities are added to the computer firmware. This approach can work without any modification to operating system and / or applications, and can support a larger variety of fault locations. We discuss four different strategies in X86/X64 and Itanium systems. Our analysis shows that such an approach can increase portability, the non-intrusiveness of the injector implementation, and the number of supported fault locations. Firmware-level fault injection paves the way for new research directions, such as virtual machine monitor fault injection or the investigation of certified operating systems. © 2010 IEEE. . Keywords: Computer operating systems; Application under tests; Extensible firmware interfaces; Fault injection; Fault injection capability; Fault location; Hardware faults; Itanium; Operating systems; Research directions; System management mode; Virtual machine monitors; X86; Firmware
2010-01-01 - Strategies for predicate-aware register allocation	For predicated code a number of predicate analysis systems have been developed like PHG, PQA or PAS. In optimizing compilers for (fully) predicated architectures like the Itanium® 2 processor, the primary application for such systems is global register allocation. This paper classifies predicated live ranges into four types, develops strategies based on classical dataflow analysis to allocate register candidates for all classes efficiently, and shows that the simplest strategy can achieve the performance potential provided by a PQS-based implementation. The gain achieved in the Intel® production compiler for the CINT2006 integer benchmarks is up to 37.6% and 4.48% in the geomean. © 2010 Springer-Verlag. . Keywords: Computer architecture; Computer software; Data flow analysis; Nanotechnology; Analysis system; Itanium; Itanium processor; Optimizing compilers; Performance potentials; Register allocation; Program compilers
2010-01-01 - Synthesis of a unified unit for evaluating an application-specific set of elementary functions	Fast and accurate evaluation of elementary functions (e.g. 1/x , log, 3√x, and 1/√x) is vital in many computation intensive applications. This paper presents a synthesis system of a unified hardware unit for evaluating a custom subset of elementary functions. Based on some latest algorithms of software library and new design principles that exploit the parallelism of reconfigurable hardware, fast and compact datapath is automatically generated. The proposed system also enables rapid design space exploration by allowing designers to modify the constraints on each function (e.g. precision, delay, area, etc.). For an arbitrary set of logarithm functions and power series, accuracy around 0.6 ulp (unit of last place) is achieved at double precision, which is 0.3 ulp more accurate than Pentium®, and the speed is 30% faster than Itanium®. The result is simulated by 80nm process. © 2010 IEEE. . Keywords: Algebra; Design; Reconfigurable hardware; Space research; Application-Specific; Automatically generated; Data paths; Double precision; Elementary function; Itanium; Logarithm function; New design; Pentium; Power series; Rapid design; Software libraries; Function evaluation
2011-01-01 - A 32nm 3.1 billion transistor 12-wide-issue Itanium® processor for mission-critical servers	The next generation in the Intel® Itanium® processor family, code named Poulson, has eight multi-threaded 64 bit cores. Poulson is socket compatible with the current Intel® Itanium® Processor 9300 series (Tukwila) [1]. The new design integrates a ring-based system interface derived from portions of previous Xeon® and Itanium® processors, and includes 32MB of Last Level Cache (LLC). The processor is designed in Intel®'s 32nm CMOS technology utilizing high-K dielectric metal gate transistors [2] combined with nine layers of copper interconnect. The 18.2×29.9mm2 die contains 3.1 billion transistors, with 720 million allocated to the eight cores (Fig. 4.8.1). A total of 54MB of on die cache is distributed throughout the core and system interface. Poulson implements twice as many cores as Tukwila while lowering the thermal design power (TDP) by 15W to 170W and increases the top frequency of the I/O and memory interfaces by 50% to 6.4GT/s. © 2011 IEEE. . Keywords: High-k dielectric; CMOS technology; Copper interconnects; Lastlevel caches (LLC); Memory interface; Metal gate transistors; Mission critical; System interfaces; Thermal designs; Integrated circuit design
2011-01-01 - Data cache prefetching with dynamic adaptation	Modern processors based on VLIW architecture rely heavily on software cache prefetching incorporated by the compiler. For accurate prefetching different factors such as latencies of the loop iterations need to be taken into account, which cannot be determined at (static) compile time. Consequently, the compilers either produce inaccurate prefetches or resort to producing code without prefetching. Many applications with complex code are therefore unable to perform very well on the modern processors. In this paper, we present an approach that is able to generate accurate prefetch instructions by exploiting information available at runtime. The code is instrumented with prefetches having offsets which may be adapted at runtime through a dynamic code specializer. Such cache prefetching with dynamic adaptation results in better performance of the applications. The runtime code generation activity is highly efficient and incurs a very small overhead. The experiments performed on Itanium-II architecture using icc and gcc compilers produce an average speedup of 2.51% and 2.56%, respectively. © The Author 2010. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved. . Keywords: Optimization; Query languages; Very long instruction word architecture; Cache prefetching; Code optimization; dynamic compilation; memory hierarchy optimization; programming languages; Program compilers
2011-01-01 - Energy-aware partitioned fixed-priority scheduling for chip multi-processors	Energy management is becoming an increasingly important problem in application domains ranging from embedded devices to data centers. In many such sys- tems, multi-core processors are projected as a promis- ing technology to achieve improved performance with a lower power envelope. Managing the application power consumption under timing constraints poses significant challenges in these emerging platforms. In this paper, we study the energy-efficient scheduling of periodic real- time tasks with implicit deadlines on chip multi-core processors (CMPs). We specifically consider processors with a single voltage and clock frequency domain, such as the state-of-the-art embedded multi-core NVIDIA Tegra 2 processor and enterprise-class processors such as Intel's Itanium 2, i5, i7 and IBM's Power 6 and Power 7 series. The major contributions of this work are (i) we prove that Worst-Fit-Decreasing (WFD) task parti- tioning when Rate-Monotonic Scheduling (RMS) is used has an approximation ratio of 1.71 for the problem of minimizing the schedulable operating frequency with partitioned fixed-priority scheduling, (ii) we illustrate the major shortcoming of WFD with RMS resulting from not considering task periods during allocation, and (iii) we propose a Single-clock domain multi-processor Frequency Assignment Algorithm (SFAA) that determines a globally energy-efficient frequency while including task period relationships. Our evaluation results show that SFAA provides significant energy gains when compared to WFD. In fact SFAA is shown to save up to 55% more power compared to WFD for an octa-core processor. © 2011 IEEE. . Keywords: Approximation algorithms; Embedded systems; Energy management; Information management; Information technology; Microprocessor chips; Multiprocessing systems; Timing circuits; Application domains; Approximation ratios; Clock frequency; Data centers; Embedded device; Energy aware; Energy efficient; Energy gain; Energy-Efficient Scheduling; Evaluation results; Frequency assignments; Itanium; Multi core; Multi-core processor; Multi-processors; On chips; Operating frequency; Rate-monotonic scheduling; Timing constraints; Energy efficiency
2011-01-01 - Exact and approximated error of the FMA	The fused multiply accumulate-add (FMA) instruction, specified by the IEEE 754-2008 Standard for Floating-Point Arithmetic, eases some calculations, and is already available on some current processors such as the Power PC or the Itanium. We first extend an earlier work on the computation of the exact error of an FMA (by giving more general conditions and providing a formal proof). Then, we present a new algorithm that computes an approximation to the error of an FMA, and provide error bounds and a formal proof for that algorithm. © 2006 IEEE. . Keywords: Approximation algorithms; Digital arithmetic; Error analysis; Computer arithmetic; Current processors; Error bound; Floating-point arithmetic; FMA; Formal proofs; Fused multiply-add; IEEE-754; Itanium; Multiply accumulate; Error compensation
2011-01-01 - Formal verification of object layout for C++ multiple inheritance	Object layout-the concrete in-memory representation of objects-raises many delicate issues in the case of the C++ language, owing in particular to multiple inheritance, C compatibility and separate compilation. This paper formalizes a family of C++ object layout schemes and mechanically proves their correctness against the operational semantics for multiple inheritance of Wasserrab et al. This formalization is flexible enough to account for spacesaving techniques such as empty base class optimization and tailpadding optimization. As an application, we obtain the first formal correctness proofs for realistic, optimized object layout algorithms, including one based on the popular "common vendor" Itanium C++ application binary interface. This work provides semantic foundations to discover and justify new layout optimizations; it is also a first step towards the verification of a C++ compiler frontend. © 2011 ACM. . Keywords: Computer applications; Computer programming languages; Semantics; Application binary interfaces; Base class; C++ language; Formal correctness; Formal verifications; Itanium; Layout algorithms; Layout optimization; Layout schemes; Multiple inheritance; Operational semantics; Semantic foundation; Separate compilation; Optimization
2011-01-01 - Hardware accelerated design of millimeter wave antireflective surfaces: A comparison of field-programmable gate array (FPGA) and graphics processing unit (GPU) implementations	Engineered materials that demonstrate a specific response to electromagnetic energy incident on them in antenna and radio frequency component design applications are in high demand due to both military and commercial needs. The design of such engineered materials typically requires numerically intensive computations to simulate their behavior as they may have electrically small features on a large area or often the overall system performance is required, which means modeling the entire integrated system. Furthermore, to achieve an optimal performance these simulations need to be run many times until a desired solution is achieved, presenting a major hindrance in arriving at a feasible solution in a reasonable amount of time. One example of such applications is the design of antireflective (AR) surfaces at millimeter wave frequencies, which often involves sub-wavelength gratings in an electrically large multilayer structure. This paper investigates the use of field-programmable gate arrays (FPGAs) and graphics processing units (GPUs) as coprocessors to the CPU in order to expedite the computation time. Preliminary results show that the hardware implementation (100 MHz) on Xilinx Virtex4LX200 FPGA is able to outperform a single-thread software implementation on Intel Itanium 2 processor (1.66 GHz) by 20 folds. However, the performance of the FPGA implementation lags behind the single-thread implementation on a modern Xeon (2.26 GHz) by 3.6×. On the other hand, modern GPUs demonstrate an evident advantage over both CPU and FPGA by achieving 20× speedup than the Xeon processor. © 2011 ACES. . Keywords: Computer graphics equipment; Design; Hardware; Image coding; Logic gates; Mathematical models; Microprocessor chips; Military applications; Millimeter waves; Program processors; Antireflective; Co-processors; Commercial needs; Computation time; Electrically large; Engineered materials; Feasible solution; FPGA implementations; Graphics Processing Unit; Graphics processing units; Hardware accelerated design; Hardware implementations; High demand; Integrated systems; Itanium 2 processor; Millimeter wave frequencies; Multilayer structures; Optimal performance; Radio frequencies; Small features; Software implementation; Sub-wave length grating; Xeon processors; Field programmable gate arrays (FPGA)
2011-01-01 - Hardware hooks for transition scan characterization	Comprehensive transition scan content was deployed on an Intel ® Itanium ® server microprocessor design, including full coverage patterns for all core logic blocks. Since this was the first time at-speed scan patterns were being planned as a manufacturing screen on an Intel ® CPU core design, the test deployment team needed to ensure that all concerns of over-and under-testing were systematically addressed. A few innovative and novel DFT solutions were deployed to ensure that the post-silicon team had the adequate tools to fully analyze the at-speed scan content. This paper describes these DFT solutions that proved invaluable during this process. © 2011 IEEE. . Keywords: At-speed; Coverage patterns; CPU cores; Itanium; Logic blocks; Post-silicon; Scan patterns; Server microprocessor; Microprocessor chips
2011-01-01 - Improving performance through deep value profiling and specialization with code transformation	Specialization of code is used to improve the performance of the applications. However, specialization based on ineffective profiles deteriorates the performance. Existing value profiling algorithms are not yet able to address the issue of code size explosion incurred due to specialization of code. This problem can be mitigated by capturing data through profiling that would be useful for specialization of code with minimum code size. In this article, we present an approach to optimize code through value profiling and specialization with code transformation. The values of the parameters selected through an analysis of code are captured in the intervals which are automatically adapted to dynamic behavior of the application. The code is then specialized based on value profiles. The specialized code contains optimizations and may be converted back to the generalized code through a transformation. This approach facilitates the code to obtain optimizations through specialization with minimum size, and no runtime overhead. Using this approach, the experiments performed on Itanium-II (IA-64) architecture with icc compiler v 9.0 show a significant improvement in the performance of the SPEC 2000 benchmarks. © 2011 Elsevier Ltd. All rights reserved. . Keywords: Algorithms; Computer architecture; Cosine transforms; Capturing data; Code optimization; Code size; Code specialization; Code transformation; Dynamic behaviors; Improving performance; Profiling; Runtime overheads; Software performance; Optimization
2011-01-01 - Test access and the testability features of the poulson multi-core intel Itanium® processor	This paper presents the "t-Ring" based DFX access architecture and the testability features of Intel's latest multi-core Itanium® processor. The architecture solves many common challenges of testing a multi-core CPU using distinctive and innovative solutions. At the core of the architecture is a hierarchical and scalable test access mechanism design providing flexible access for a variety of use models in high volume manufacturing test and debug platforms. © 2011 IEEE. . Keywords: Architecture; Integrated circuit testing; Machine design; High volume manufacturing; Innovative solutions; Itanium; Multi core; Test access; Test access mechanism; Testability; Use-model; Microprocessor chips
2011-01-01 - The design and implementation of fault-injection platform for Itanium architecture	Fault injection (FI) is an effective method for the evaluation of system availability. Research on injecting faults into the Itanium architecture system is limited, so a fault injection platform for Itanium architecture which can simulate fault injection using various methods is presented. The platform is high scalable since it develops a generic component design standard that assists the user when adapting the platform to new fault injection tools. A fault injection testing automation system is also developed based on software testing automation framework (STAF). The effectiveness of developed platform has been verified through tests combining workload and fault-load on the target server HP RX6600. . Keywords: Automation; Computer software selection and evaluation; Fault injection; Fault injection testing; Generic components; Itanium; System availability; Testing automation; Software testing
2011-01-01 - Variation tolerant digitally assisted high-speed IO PHY	Technology scaling leads to reduction of supply voltage and increase in random device variability and thus creates new challenges for analog design. A complete overhaul of the design approach at system architecture and circuit topology levels is necessary to keep the link robust and tolerant to low supply voltage and random variability challenges. This paper presents key analog circuit architecture techniques employed to implement 6.4GT/s per lane, 14mW/Gbps analog front end high-speed IO interfaces on Poulson - a 32nm next generation Intel Itanium microprocessor [1]. © 2011 IEEE. . Keywords: Electric network topology; Microprocessor chips; Analog design; Analog front end; Circuit architectures; Design approaches; High-speed; IO interfaces; Itanium; Low supply voltages; Random variability; Supply voltages; System architectures; Technology scaling; Analog circuits
2012-01-01 - A 32 nm, 3.1 billion transistor, 12 wide issue itanium® processor for mission-critical servers	An Itanium® processor implemented in 32 nm CMOS with nine layers of Cu contains 3.1 billion transistors. The die measures 18.2 mm by 29.9 mm. The processor has eight multi-threaded cores, a ring based system interface and combined cache on the die is 50 MB. High-speed links allow for peak processor-to-processor bandwidth of up to 128 GB/s and memory bandwidth of up to 45 GB/s. © 1966-2012 IEEE. . Keywords: Error correction; Inductive logic programming (ILP); Microprocessor chips; Program processors; Clock buffer; Data caches; Data paths; double error correction, triple error detection (DECTED); Execution units; first level data (FLD); First level instruction (FLI); Home agents; instruction buffer logic (IBL); Instruction caches; Itanium processor family; Lastlevel caches (LLC); Memory controller; Memory ordering; ordering CZQueue (OZQ); Parrallelism; quick path interconnect (QPI); Random logic; Register files; Scalable memory; Second level; Signal array; Single error corrections; Thermal designs; Cache memory
2012-01-01 - Hardware counters based analysis of memory accesses in SMPs	Modern microprocessors incorporate Hardware Counters (HC) that provide useful information with low overhead. HC are not commonly used because of the lack of tools to get their information in an easy way. In this paper, a set of tools to simplify the accessing and programming of Intel Itanium 2 TMEARs (Event Address Registers) is presented. The aim of these tools is to characterise the memory accesses of parallel codes, in multicore systems, in which the cache hierarchy can greatly influence the performance. The first tool allows the user to insert in the code, in a simple and transparent way, the instructions needed to monitor and manage hardware counters. Two versions of this tool have been implemented. The first one is a command line tool that takes as input a C source file with appropriate directives and outputs it with the monitoring code added. The other one is a graphical interface that allows the user to select the parts of the code to analise. The second tool takes the information gathered by the monitored parallel code provided by the hardware counters and displays it graphically. This tool shows the information in a comprehensive but simple way, allowing the user to adjust the level of detail. These tools were used to carry out a study of parallel irregular codes. Although this study has been made in a specific environment, the tools here presented can be used in any system as long as it is based on hardware counters present in current processors. © 2012 IEEE. . Keywords: Distributed parameter networks; Hardware; Monitoring; Parallel processing systems; Shape memory effect; Cache hierarchies; Command line; Current processors; Graphical interface; Hardware counters; Irregular codes; Itanium; Level of detail; Low overhead; Memory access; Memory hierarchy; Modern microprocessor; Monitoring code; Multi-core systems; Parallel code; Source files; Cache memory
2012-01-01 - SMARQ: Software-managed alias register queue for dynamic optimizations	Traditional alias analysis is expensive and ineffective for dynamic optimizations. In practice, dynamic optimization systems perform memory optimizations speculatively, and rely on hardware, such as alias registers, to detect memory aliases at runtime. Existing hardware alias detection schemes either cannot scale up to a large number of alias registers or may introduce false positives. Order-based alias detection overcomes the limitations. However, it brings considerable challenges as how software can efficiently manage the alias register queue and impose restrictions on optimizations. In this paper, we present SMARQ, a Software-Managed Alias Register Queue, which manages the alias register queue efficiently and supports more aggressive speculative optimizations. We conducted experiments with a dynamic optimization system on a VLIW processor that has 64 alias registers. The experiments on a suite of SPECFP2000 benchmarks show that SMARQ improves the overall performance by 39% as compared to the case without hardware alias detection. By scaling up to a large number (from 16 to 64) of alias registers, SMARQ improves performance by 10%. Compared to a technique with false positives (similar to Itanium), SMARQ improves performance by 13%. To reduce the chance of alias register overflow, the novel alias register allocation algorithm in SMARQ reduces the alias register working set by 74% as compared to a straightforward alias register allocation based on program order. © 2012 IEEE. . Keywords: Benchmarking; Computer architecture; Experiments; Hardware; Queueing theory; Alias detection; alias register; Dynamic optimization; False positive; Memory optimization; Register allocation; speculation; Speculative optimization; Optimization
2013-01-01 - Dynamic reduction of voltage margins by leveraging on-chip ECC in itanium II processors	Lowering supply voltage is one of the most effective approaches for improving the energy efficiency of microprocessors. Unfortunately, technology limitations, such as process variability and circuit aging, are forcing microprocessor designers to add larger voltage guardbands to their chips. This makes supply voltage increasingly difficult to scale with technology. This paper presents a new mechanism for dynamically reducing voltage margins while maintaining the chip operating frequency constant. Unlike previous approaches that rely on special hardware to detect and recover from timing violations caused by low-voltage execution, our solution is firmware-based and does not require additional hardware. Instead, it relies on error correction mechanisms already built into modern processors. The system dynamically reduces voltage margins and uses correctable error reports raised by the hardware to identify the lowest, safe operating voltage. The solution adapts to core-to-core variability by tailoring supply voltage to each core's safe operating level. In addition, it exploits variability in workload vulnerability to low voltage execution. The system was prototyped on an HP Integrity Server that uses Intel's Itanium 9560 processors. Evaluation using SPECjbb2005 and SPEC CPU2000 workloads shows core power savings ranging from 18% to 23%, with minimal performance impact. Copyright 2013 ACM. . Keywords: Computer architecture; Energy efficiency; Error correction; Firmware; Hardware; Microprocessor chips; Correctable errors; Correction mechanism; Effective approaches; Modern processors; Operating frequency; Performance impact; Process Variability; Technology limitations; Computer hardware
2013-01-01 - Novel on chip-interconnection structures for giga-scale integration VLSI ICS	Based on the guidelines of International Technology Roadmap for Semiconductors (ITRS) Intel has already designed and manufactured the next generation product of the Itanium family containing 1.72 billion transistors. In each new technology due to scaling, individual transistors are becoming smaller and faster, and are dissipating low power. The main challenge with these systems is wiring of these billion transistors since wire length interconnect scaling increases the distributed resistance-capacitance product. In addition, high clock frequencies necessitate reverse scaling of global and semi-global interconnects so that they satisfy the timing constraints. Hence, the performances of future GSI systems will be severely restricted by interconnect performance. It is therefore essential to look at interconnect design techniques that will reduce the impact of interconnect networks on the power, performance and cost of the entire system. In this paper a new routing technique called Wave-Pipelined Multiplexed (WPM) Routing similar to Time Division Multiple Access (TDMA) is discussed. This technique is highly useful for the current high density CMOS VLSI ICs. The major advantages of WPM routing technique are flexible, robust, simple to implement, and realized with low area, low power and performance overhead requirements. Copyright © 2013 SPIE. . Keywords: Communication; Interconnection networks; Systems analysis; Time division multiple access; Area performance; Distributed resistance; Interconnect structures; Power; Routing techniques; Timing constraints; Intelligent control
2014-01-01 - A general model checking framework for various memory consistency models	Relaxed memory consistency models are common and essential when multiple processes share a single global address space, such as when using multicore CPUs, partitioned global address space languages, and so forth. Programming within these models is difficult and error prone, because of non-intuitive behaviors that could not occur in a sequential memory consistency model. In addition, because the memory consistency models vary from language to language, and CPU to CPU, a program which may work correctly on one system may not work on another. To address the problem, this paper describes a model checking framework in which users are able to check their programs under various memory consistency models. More specifically, our framework provides a base model that exhibits very relaxed behaviors, and users are able to define various consistency models by adding constraints to the base model. This paper also describes a prototype implementation of a model checker based on the proposed framework. We have specified the necessary constraints for three practical existing memory consistency models (UPC, Coarray Fortran, and Itanium). Our model checker verified some example programs correctly, and confirmed the expected differences among the three models. © 2014 IEEE. . Keywords: FORTRAN (programming language); Program processors; Co-array Fortran; Consistency model; Global address spaces; Itanium; Memory consistency models; Partitioned Global Address Space; Prototype implementations; Unified parallel C; Model checking
2014-01-01 - Design of high-end server based on tightly-coupled single-hop multi-plane architecture	A new method based on the tightly-coupled single-hop multi-plane (TSMP) architecture is put forward to solve the system expansibility issue in high-end server design. It adopts a two-side single-hop multi-plane topology to support the seamless extending of system from 8 to 32-way. A two-tier directory based cache coherence maintenance method is applied to sustain both high parallel cache coherence requests and high efficiency conflicts handling, which reduces the system transmission delay and disposal delay remarkably. The TSMP architecture has beea used in the design of an Inspur K1 high-end server. Testing results of Stream, SPEC CPU and high pertormance Linpack (HPL) indicate that K1 system performance increases linearly from 1 to 32-way. The K1 high-end server supports the QPI1.0 based Intel Itanium 4-core Tukwila and 8-core Polson CPUs. It is China's first independent commercial high-end server. . Keywords:
2014-01-01 - High-performance mathematical functions for single-core architectures	Nowadays high-performance computing (HPC) architectures are designed to resolve assorted sophisticated scientific as well as engineering problems across an ever intensifying number of HPC and professional workloads. Application and computation of key trigonometric functions sine and cosine are in all spheres of our daily life, yet fairly time consuming task in high-performance numerical simulations. In this paper, we have delivered a detailed deliberation of how the micro-architecture of single-core Itanium® and Alpha 21264/21364 processors as well as the manual optimization techniques improve the computing performance of several mathematical functions. On describing the detailed algorithm and its execution pattern on the processor, we have confirmed that the processor micro-architecture side by side manual optimization techniques ameliorate computing performance significantly as compared to not only the standard math library's built-in functions with compiler optimizing options but also Intel® Itanium® library's highly optimized mathematical functions. © 2014 World Scientific Publishing Company. . Keywords: Computer architecture; Microprocessor chips; Optimization; Pipe linings; Program compilers; Compiler; computing; Computing performance; High-performance computing; Loop unrolling; Mathematical functions; Optimization techniques; Trigonometric functions; Functions
2014-01-01 - The effect of parallelization on a tetrahedral mesh optimization method	A parallel algorithm for simultaneous untangling and smoothing of tetrahedral meshes is proposed in this paper. This algorithm is derived from a sequential mesh optimization method. We provide a detailed analysis of its parallel scalability and efficiency, load balancing, and parallelism bottlenecks using six benchmark meshes. In addition, the influence of three previously-published graph coloring techniques on the performance of our parallel algorithm is evaluated. We demonstrate that the proposed algorithm is highly scalable when run on a shared-memory computer with up to 128 Itanium 2 processors. However, some parallel deterioration is observed. Here, we analyze its main causes using a theoretical performance model and experimental results. © 2014 Springer-Verlag. . Keywords: Parallel algorithms; Itanium 2 processor; Mesh optimization; Parallel performance; Parallel scalability; Parallelizations; Shared-memory computers; Tetrahedral meshes; Theoretical performance; Parallel architectures
2015-01-01 - Easy provisioning manager: Provisioning tool for private cloud on Intel® Itanium® infrastructures	The Private Cloud is a custom-designed cloud computing-like environment within a protected firewall on a closed internal network. In this case, the cloud architecture could be purchased or rented. Boasting tighter security as well as greater reliability, private clouds are quite costly, and thus, resources should be used efficiently. Virtual machines are mostly used to provide a way of sharing resources. This paper deals with the design and deployment of a tool, called Easy Provisioning Manager (EPM). EPM is a platform for a private cloud and Infrastructure as a Service (IaaS) for Itanium® Processor® infrastructures. It enables administrators to provision virtual machines, in minutes, rather than days. EPM offers a self-service infrastructure portal for autoprovisioning/ deprovisioning and built-in lifecycle management to optimize infrastructure, and it also monitors compute resources for cloud IT. Copyright © 2015 by The International Society for Computers and Their Applications (ISCA). . Keywords: Cloud computing; Computer system firewalls; Distributed computer systems; Managers; Cloud architectures; Compute resources; Internal network; Itanium; Life-cycle management; Private clouds; Service infrastructure; Sharing resources; Infrastructure as a service (IaaS)
2015-01-01 - On the error of computing ab+cd using Cornea, Harrison and Tang's method	In their book, Scientific Computing on the Itanium, Cornea et al. [2002] introduce an accurate algorithm for evaluating expressions of the form ab+cd in binary floating-point arithmetic, assuming an FMA instruction is available. They show that if p is the precision of the floating-point format and if u = 2-p, the relative error of the result is of order u. We improve their proof to show that the relative error is bounded by 2u + 7u2 + 6u3. Furthermore, by building an example for which the relative error is asymptotically (as p→∞or, equivalently, as u→ 0) equivalent to 2u, we show that our error bound is asymptotically optimal. © 2015 ACM. . Keywords: Algorithms; Computer software; Software engineering; Asymptotically optimal; Error bound; Floating points; Itanium; Relative errors; Digital arithmetic
2015-01-01 - Using ECC Feedback to Guide Voltage Speculation in Low-Voltage Processors	Low-voltage computing is emerging as a promising energy-efficient solution to power-constrained environments. Unfortunately, low-voltage operation presents significant reliability challenges, including increased sensitivity to static and dynamic variability. To prevent errors, safety guard bands can be added to the supply voltage. While these guard bands are feasible at higher supply voltages, they are prohibitively expensive at low voltages, to the point of negating most of the energy savings. Voltage speculation techniques have been proposed to dynamically reduce voltage margins. Most require additional hardware to be added to the chip to correct or prevent timing errors caused by excessively aggressive speculation. This paper presents a mechanism for safely guiding voltage speculation using direct feedback from ECC-protected cache lines. We conduct extensive testing of an Intel Itanium processor running at low voltages. We find that as voltage margins are reduced, certain ECC-protected cache lines consistently exhibit correctable errors. We propose a hardware mechanism for continuously probing these cache lines to fine tune supply voltage at core granularity within a chip. Moreover, we demonstrate that this mechanism is sufficiently sensitive to detect and adapt to voltage noise caused by fluctuations in chip activity. We evaluate a proof-of-concept implementation of this mechanism in an Itanium-based server. We show that this solution lowers supply voltage by 18% on average, reducing power consumption by an average of 33% while running a mix of benchmark applications. © 2014 IEEE. . Keywords: Benchmarking; Cache memory; Computer architecture; Energy efficiency; Errors; Benchmark applications; Correctable errors; Dynamic variability; Energy efficient; Extensive testing; Hardware mechanism; Itanium processor; Low voltage operation; Surge protection
2016-01-01 - Hash, don't Cache (THE Page Table)	Radix page tables as implemented in the x86-64 architecture incur a penalty of four memory references for address trans- lation upon each TLB miss. These 4 references become 24 in virtualized setups, accounting for 5%{90% of the runtime and thus motivating chip vendors to incorporate page walk caches (PWCs). Counterintuitively, an ISCA 2010 paper found that radix page tables with PWCs are superior to hashed page tables, yielding up to 5x fewer DRAM accesses per page walk. We challenge this finding and show that it is the result of com- paring against a suboptimal hashed implementation|that of the Itanium architecture. We show that, when carefully optimized, hashed page tables in fact outperform existing PWC-aided x86-64 hardware, shortening benchmark run- Times by 1%{27% and 6%{32% in bare-metal and virtualized setups, without resorting to PWCs. We further show that hashed page tables are inherently more scalable than radix designs and are better suited to accommodate the ever in-creasing memory size; their downside is that they make it more challenging to support such features as superpages. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. . Keywords: Dynamic random access storage; Bare metals; Chip vendors; Itanium; Memory references; Memory size; Page table; Runtimes; Memory architecture
2016-01-01 - Hash, Don't Cache (the Page Table)	Radix page tables as implemented in the x86-64 architecture incur a penalty of four memory references for address translation upon each TLB miss. These 4 references become 24 in virtualized setups, accounting for 5% - 90% of the runtime and thus motivating chip vendors to incorporate page walk caches (PWCs). Counterintuitively, an ISCA 2010 paper found that radix page tables with PWCs are superior to hashed page tables, yielding up to 5x fewer DRAM accesses per page walk. We challenge this finding and show that it is the result of comparing against a suboptimal hashed implementation - -that of the Itanium architecture. We show that, when carefully optimized, hashed page tables in fact outperform existing PWC-aided x86-64 hardware, shortening benchmark runtimes by 1% - 27% and 6% - 32% in bare-metal and virtualized setups, without resorting to PWCs. We further show that hashed page tables are inherently more scalable than radix designs and are better suited to accommodate the ever increasing memory size; their downside is that they make it more challenging to support such features as superpages. © 2016 ACM. . Keywords: Buffer storage; Dynamic random access storage; Address translation; Bare metals; Chip vendors; Itanium; Memory references; Memory size; Page table; Runtimes; Memory architecture
2016-01-01 - Poulson: An 8 core 32 nm next generation Intel® Itanium® processor	This article consists of a collection of slides from the author's conference presentation on Intel's Itanium processor family of products. Some of the specific topics discussed include: the special features, system specifications, and system design for these products; system architectures; applications for use; platforms supported; processing capabilities; memory capabilities; and targeted markets. © 2011 IEEE. . Keywords: Specifications; Itanium; Itanium processor family; Memory capabilities; Processing capability; System architectures; System specification; Product design
2016-01-01 - Tukwila - A quad-core Intel® Itanium® processor	This article consists of a collection of slides from the author's conference presentation. Some of the topics discussed include: Tukwila Overview; Exploiting Thread Level Parallelism; Improving Instruction Level Parallelism; Scalability and Headroom; Power and Frequency Management; and Enterprise RAS and Manageability. © 2008 IEEE. . Keywords: Frequency management; Instruction level parallelism; Itanium; Thread level parallelism; Program processors
2017-01-01 - A general model checking framework for various memory consistency models	Relaxed memory consistency models are common and essential when multiple processes share a single global address space, such as when using multicore CPUs, distributed shared-memory programming languages, and so forth. Programming within these models is difficult and error prone, because of non-intuitive behaviors that could not occur in a strict consistency model. In addition, because the memory consistency models vary from language to language, and CPU to CPU, a program that may work correctly on one system may not work on another. To address the problem, this paper describes a model checking framework in which users are able to check their programs under various memory consistency models. More specifically, our framework provides a base model that exhibits very relaxed behaviors, and users are able to define various consistency models by adding constraints to the base model. This paper also describes McSPIN, a prototype implementation of a model checker based on the proposed framework. McSPIN can take a memory consistency model as an input, as well as a program and a property to be checked. We have specified the necessary constraints for three practical existing memory consistency models (Unified Parallel C, Coarray Fortran, and Itanium). McSPIN verified some example programs correctly, and confirmed the expected differences among the three models. © 2016, Springer-Verlag Berlin Heidelberg. . Keywords: Computational linguistics; FORTH (programming language); FORTRAN (programming language); Memory architecture; Multicore programming; Program processors; Co-array Fortran; Consistency model; Distributed shared memory; Global address spaces; Itanium; Memory consistency models; Prototype implementations; Unified parallel C; Model checking
2017-01-01 - VD-IT2, Virtual Disk Cloning on Disk Arrays Using a Type-2 Fuzzy Controller	Disk arrays provide data storage by combining sets of disks into one or more virtual disks (VDs) utilizing specialized control algorithms. A VD is a partition of such combined storage capacity perceived by the user as a physical disk. VD cloning is a data protection technique that copies the data from one VD to another. A typical problem with VD cloning is that it delays user reads and writes, i.e., increases response times. This paper presents VD- Interval Type 2 (IT2), an effective IT2 fuzzy logic control (IT2 FLC) approach to VD cloning that dramatically reduces the response time delay. The VD-IT2 cloning scheme is capable of balancing two adversely affecting processes: VD cloning and user read/write response times. This IT2 FLC-based approach regulates throughput of VD cloning with regards to the user request activity. The VD-IT2 solves the increased delay in user reads and write during cloning by adjusting the time interval between cloning requests. The contribution of this paper is two-fold. First, a novel formula for planning of data backups is presented. The data backup formula predicts the fraction of replicated data blocks in a combined (clone and snap) replication. Second, a novel IT2 FLC scheme reduces response time of user reads/writes in half. The VD-IT2 was tested on an Itanium workstation with 120 disks and proved 50% reduction of user latencies within a short period of time (high response time of 60 ms was reduced to half, within 30 s only), on an average basis. © 1993-2012 IEEE. . Keywords: Controllers; Delay control systems; Digital storage; Fuzzy control; Fuzzy logic; Genetic engineering; Structural analysis; Combining sets; Data protection techniques; Disk array; Fuzzy logic control; Physical disks; Replicated data; Storage capacity; Type-2 fuzzy; Cloning
2018-01-01 - 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2018 - Proceedings	The proceedings contain 276 papers. The topics discussed include: an experimental view on PureB silicon photodiode device physics; characterization of thin boron layers grown on silicon utilizing molecular beam epitaxy for ultra-shallow Pn-junctions; transition metal polysulfides and their potential applications; analysis and modeling of capacitive electrodes for insulin dose detection; simulation study of a deep-trench LDMOS with bilateral super-junction drift regions; large-signal characterization of horizontal current bipolar transistor (HCBT) by load-pull measurements; timing closure of clock enable signals on a 32 nm Intel Itanium processor; modeling of a transmission line pulse measurement setup; a unary coded current steering DAC with sequential stepping of the thermometer coded register in 1 and 2 LSB steps; bulk-driven fully differential difference amplifier for ultra-low voltage applications; minimizing switching time of energy harvesting management circuit; driver for 3D-integrated nonlinear microring resonator-based optical modulators; and driver for 3D-integrated nonlinear microring resonator-based optical modulators. . Keywords:
2018-01-01 - Effects of solution treating on microstructural and mechanical properties of a heavily deformed new biocompatible Ti-Nb-Zr-Fe alloy	The effects of heavy cold plastic deformation by rolling on microstructural features and mechanical properties of Ti-25Nb-6Zr-1.7Fe (wt %) biocompatible alloy (TNZF) were studied. After a preliminary alloy processing, a heavy plastic deformation by cold-rolling (CR) with a total relative degree of plastic deformation of 90% (equivalent plastic strain, ε = 2.42) and a subsequent solution heat treatment (CR+ST) at 900 °C for 0.6 ks/water quenching were applied and then investigated. The CR and CR+ST states have microstructures consisting of mixtures of β''-Ti phases and α''-Ti. The alloy in CR state shows a pronounced deformation texture, with highly deformed and elongated β-Ti grains, exhibiting internal highly fragmented areas, with shear lines at about 45° and a sub-micron/nanocrystalline fine dispersed α''-Ti phase. The alloy in CR+ST state has completely recrystallized equiaxed polyhedral β''-Ti grains, with average grain size close to 52 µm and a sub-micron/nanocrystalline fine dispersed α''-Ti phase. Recorded mechanical properties, for both CR and CR+ST states, show quite similar values for the yield strength (σ0.2), ultimate tensile strength (σUTS) and Vickers microhardness (HV0.1) for CR state (σ0.2 = 603 MPa, σUTS = 1282 MPa and 287 HV0.1) in comparison with CR+ST state (σ0.2 = 598 MPa, σUTS = 1256 MPa and 256 HV0.1). Values for the modulus of elasticity (E) are lower (E = 72 GPa for CR state and E = 61 GPa for CR+ST state) than those for conventional biocompatible alloys (E ~ 110 GPa). © 2018 by the authors. Licensee MDPI, Basel, Switzerland. . Keywords:
2018-01-01 - Timing closure of clock enable signals on a 32 nm Intel Itanium processor	With modern high speed circuit design using state of the art automated place and route (APR) flows, synthesis of clock enable (CE) signals is becoming increasingly difficult in terms of timing closure. The size of APR blocks in digital physical design in microprocessor projects is expanding with every generation of microprocessors as the implementation tools become more capable of handling large designs with high quality results and fast turnaround times. However, larger APR blocks make CE synthesis progressively difficult as timing closure complexity on these signals increases dramatically. The main problem is due to a single CE register driving the signal to a relatively larger area of the design, and to a greater number of clock gating cells. In this paper, we present automated duplication of CE logic in the APR flow to achieve timing closure on a 32 nm Intel Itanium project. We show how timing convergence is achieved without any additional effort from the physical designers, and with no changes required in the RTL. Solutions to the CE problem with smaller degree of automation and more manual effort, which were used on our previous projects, are also discussed and compared, and the reasons they are deemed inadequate are explained. © 2018 Croatian Society MIPRO. . Keywords: Automation; Clocks; Microelectronics; Timing circuits; Turnaround time; Clock tree synthesis; Degree of automation; High speed circuit; Itanium processor; place; route; State of the art; Timing closures; Integrated circuit design
2019-01-01 - VNUMA: A virtual shared-memory multiprocessor	vNUMA, for virtual NUMA, is a virtual machine that presents a cluster as a virtual shared-memory multiprocessor. It is designed to make the computational power of clusters available to legacy applications and operating systems. A characteristic aspect of vNUMA is that it incorporates distributed shared memory (DSM) inside the hypervisor, in contrast to the more traditional approach of providing it in middleware. We present the design of vNUMA, as well as an implementation on Itanium-based workstations. We discuss in detail the enhancements to standard protocols that were required or enabled when implementing DSM inside a hypervisor, and discuss some of the tradeoffs we encountered. We examine the scalability of vNUMA on a small cluster, and analyse some of the design choices. . Keywords: Legacy systems; Middleware; Multiprocessing systems; Computational power; Distributed shared memory; Hypervisor; Legacy applications; Small clusters; Standard protocols; Traditional approaches; Virtual shared memory; Memory architecture
2020-01-01 - Implications of job loading and scheduling structures on machine memory effectiveness	The reliable parameter for the determining the effectiveness of processing elements (such as memory and processors) is the number of cycles per instructions (CPI), execution speedups, and frequency. Job loading and scheduling techniques target instructions processing in a manner to support the underlying hardware requirements. One of the earliest methods of scheduling jobs for machines involves arrangements of instruction sets in serial order known as pipelining. Another technique uses the principle of overlapping for instruction sets in order to allow current processing and executions which is introduced in this paper. Again, there is job scheduling technique that requires enlargement of processing elements known as static approach as in the case of Intel Itanium. But, there is a great concern about the most appropriate means of scheduling and loading jobs entirely composed of dependent and branched instructions. The cooperative processing nature of present-day computation has expanded the need to allow users to be involved in multiple problems solving environments. In addition, the paper investigates the implications of these job loading and scheduling approaches on speedup and performance of memory systems. The paper found that overlapping of instruction sets during execution was most effective technique for speedups and memory elements performance. In future works, there is need to focus on parallelism exploitations among diverse machines cooperating in instruction processing and execution. © Springer Nature Singapore Pte Ltd. 2020. . Keywords: Data storage equipment; Scheduling; Stress analysis; Basic blocks; Instructions; Jobs; Performance; Speedups; Loading
2021-01-01 - Life and Science of Clemens C. J. Roothaan	Clemens Roothaan, a Nazi concentration camp survivor and Professor Emeritus of the University of Chicago, passed away on 17 June 2019, 10 months after celebrating his 100th anniversary. For his doctoral thesis, Roothaan developed the matrix version of the Hartree-Fock equations. These Hartree-Fock-Roothaan equations form the cornerstone of atomic and molecular structure theory. In addition, Roothaan devised computing methods for quantum chemistry, physics, and other scientific fields from the early years. After 1988, he actively helped Hewlett Packard develop the Intel Itanium processor. This article presents the highlights of the life and science of Roothaan. © 2021, Indian Academy of Sciences. . Keywords:
2021-01-01 - Program Obfuscation via ABI Debiasing	The Itanium ABI is the most popular C++ ABI that defines data structures essential to implement underlying object-oriented concepts in C++. Specifically, name mangling rules, object and VTable layouts, alignment, etc. are all mandated by the ABI. Adherence to the ABI comes with undesirable side effects. While it allows interoperability, past research efforts have shown that it provides robust inference points that an attacker can leverage to reveal sensitive design information through binary reverse engineering. In this work, we aim to reduce the ability of an attacker to successfully reverse engineer a binary. We do this via removal of what we call ABI Bias, i.e., the reverse engineering bias that manifests due to a compiler's adherence to the ABI. Specifically, we identify two types of ABI biases that are central to past reverse engineering works on C++ binaries: VTable ordering bias and Function Pointer bias. We present compiler-based techniques that can correctly and efficiently debias a given binary from the aforementioned biases. We evaluate our proof-of-concept implementation on a corpus of real world programs for binary size, correctness and performance. We report an average increase of 1.42% in binary size compared to the baseline, very low performance overhead and lastly, correct execution of evaluation programs in comparison to the baseline. Finally, we demonstrate efficacy of our approach by hindering DeClassifier, a state-of-the-art C++ reverse engineering framework. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. . Keywords: C++ (programming language); Object oriented programming; Program compilers; C++; De-biasing; Itanium; Object-oriented concepts; Performance; Program obfuscation; Research efforts; Robust inference; Security; Side effect; Reverse engineering
