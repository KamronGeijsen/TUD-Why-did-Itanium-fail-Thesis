February 15, 2000 2:29 PM - Intel IDF Report 1 - CPUs.txt	 Just two hours into the first day of the Intel Developer Forum Intel has already demonstrated and announced more than we have seen from them in the past few months, and for the first time in quite a while, theyve showed us more than just a higher clock speed CPU. While there has been very little discussion about chipsets other than throwing out a few names such as Solano, Solano 2, and Camino 2, the main focus of the speeches by Andy Grove and Senior VP Albert Yu has been the upcoming IA-32 architecture found in the Willamette processor and of course Intels flagship server platform, the Itanium. For most AnandTech readers, the Willamette is a bigger concern than the Itanium if for any reason at all, because of cost.
February 17, 2000 12:00 PM - Intel IDF Report 3 - Itanium.txt	 The Spring 2000 Intel Developer Forum is definitely oriented towards a theme of starting a new family of processors, and whats so interesting about it all is that by the end of this year, Intel will have launched brand new members of all of their processor families. While the low end will see nothing more than an updated Celeron platform, the performance PCs, servers, and workstations will be enjoying the benefits of Intels updated IA-32 and brand new IA-64 architecture processors. IA, an Intel coined acronym standing for Intel Architecture, is a term used to define the class of Intel processor that were dealing with. For example, the 32-bit Willamette that we talked about in our first IDF report on Tuesday is a member of the IA-32 family. As quite a few of you already know, Intel began talking about their first IA-64 processor, which was then known as the Merced, quite some time ago. The Merced, as its classification as an IA-64 processor indicates, was to be Intels first 64-bit CPU and has been the talk of the town over here, although it is now known as the Itanium processor. While most AnandTech readers are focusing on what Intel calls, the performance PC segment, and the resulting Athlon vs Pentium III comparison that emerges there, Intel is concerned with much more than competing on a desktop level. Intel is currently positioned as a leader in the CPU industry. Although AMD has recently given them quite a bit of competition with the Athlon, the fact of the matter is that most x86 workstations and servers are still predominantly Intel based. We were talking to an Intel employee about exactly what this barrage of product announcements and demonstrations was really about. And his response was definitely an interesting one, basically he stated that Intel is concerned with making sure that the Internet as a whole is running on the fastest computers not exclusively home users. Intels view at the Spring IDF 2000 is a much more global one than we have seen from them or any of their competitors in the past. Instead of focusing on competing in the desktop market alone, an area we often exclusively look at, Intels plans are to make sure that the entire web community as a whole is moved to this next level of processors. This means that not only the home users are running the next generation of IA-32 processors but the websites that they visit and the databases that they interact with are running the latest IA-32 and potentially IA-64 processors. From the perspective of MCAD, 3D visualization, and other professional users, Intel is attempting to offer them more robust solutions with their new IA-32 and IA-64 processors that would be able to handle the complex designs that these users work with on a daily basis. Intel even told us that one of the customers for their IA-64 processor is Intel themselves. When designing the next generation of CPUs, the computational power required to display and manipulate the processor layout is growing considerably at a rate that will eventually result in the need, according to Intel, for the move to an IA-64 platform.
February 29, 2000 3:15 AM - Intel IDF Report 4 - Wrap Up.txt	 In part 4 of our IDF coverage, we wrap things up and take a look at some of the miscellaneous items that we saw down in Palm Springs. We didn't mention it earlier, but the weather in Palm Springs was absolutely gorgeous. It's a shame that IDF is moving away from Palm Springs, but hopefully the next spot will be just as pleasant. Click to enlarge Memory Roadmap Overview Intel made it clear time and time again that RDRAM is the primary memory technology for the future. In the desktop segment, they expect RDRAM to fill the mainstream desktop market by the end of 2000 and transition to the value segment by early 2001. They believe that since the pin count for RDRAM is reduced compared to SDRAM, they can double the bandwidth easily. Whether this doubling in bandwidth is achieved by doubling the number of RDRAM channel, by increasing the clock, or by using a quad pumped data transfer mechanism was unclear in the presentation, but the first option seems the most likely. As mentioned in our first IDF report, the initial Tahema chipset for the Willamette processor will be RDRAM only, with support for two channels to help satiate that "400 MHz" bus. The workstation space, where memory performance is critical, is also transitioning rapidly to RDRAM under the i840 chipset. The "2+2" configuration, meaning 2 RIMM + 2 DIMM slots, of the i820 chipset is now officially supported by Intel. Look for a number of motherboards to start popping up from virtually every manufacturer with this support. Intel did claim the difficulties they ran into with 3 RIMM's and the i820 is not unsolvable, but since current OEM's are not complaining about the 2 RIMM situation, they see little reason to play around with what works. According to Intel, as the processor performance increases, the benefits experienced from RDRAM will increase as well. The figures they gave us now indicate a 2 - 5% benefit from RDRAM now, and they predict that as the performance of CPUs increase that figure could increase to 30%. They don't see a role for DDR SDRAM in the desktop market, which is very unfortunate, at least for the time being. SDRAM support will continue on in the value desktop and mobile segments as well. The i815 chipset, code named Solano, will finally provide a solution to the desktop market with 133 MHz FSB, PC133 SDRAM, AGP 4x, and UDMA/66 support from an Intel chipset. Up until now, the only option for all these features has been the VIA Apollo Pro 133A. A new ICH should be introduced with the i815 as well that will include USB 2.0 support. This ICH will be used in conjunction with the i820's current MCH to produce the i820E. The i815 will have an integrated graphics core that is virtually identical to that of the i810/i810E, but also add an external AGP 4x port for upgraded graphics. While most AnandTech readers will disable the onboard graphics for any high performance rig, the onboard graphics could still come in handy for use with a second monitor. Look for the i815 in late Q2 2000. Since there is a huge cost premium associated with RDRAM at the moment, Intel is looking to support SDRAM and DDR SDRAM on its server platforms in 2001. The problems they see, however, is that while a single chipset can support both SDRAM and DDR SDRAM, motherboard PCB layout and memory socket requirement are different between the two memory technologies, which makes the migration path difficult. Obviously, AMD and VIA think differently, so we'll see just how they deal with those issues when the release their DDR enabled chipsets later this year. With Intel still firmly pushing RDRAM as the primary memory solution for the future, we weren't the least bit surprised to see RDRAM modules all over the show floor from every memory manufacturer. Here's a few thousand dollars worth of RDRAM at the Samsung booth: Click to enlarge More RDRAM than the rest of the free world We did get a brief chance to speak with the guys at the Micron booth regarding their own DDR SDRAM chipset, known as the Samurai DDR. While they couldn't tell us too much, they are still on track and are very close to release. How close exactly? They wouldn't say, but the future does look bright. An interesting capability of the Samurai DDR is that two north bridges can work together to provide dual PCI buses, additional DIMM sockets, and dual AGP ports. We had previously thought that two north bridges were required for dual processors, but Micron has cleared this up for us and we now know that a dual CPU system can be built with a single north bridge. When we spoke with VIA at Comdex, they were looking to license the Micron chipset for a workstation chipset of their own, but no updates on this little twist from IDF unfortunately. Click to enlarge
March 2, 2000 2:08 AM - IDF CPU Report - Intel  AMD.txt	 The name Intel Developer Forum (IDF) would normally indicate a convention revolving almost exclusively around Intel, but our experience with this years Spring IDF was quite the contrary. While it is true that we got quite an interesting look at what Intel has planned for this year we also got to see a microcosm representing the future of the x86 computing world as a whole, including a few interesting tidbits of information from AMD, the last face youd expect to hear about at an IDF. The Spring 2000 IDF was held in sunny Palm Springs, California, but it will most likely be the last time that the IDF is held in Palm Springs. The fairly small convention was home to over 3000 developers and a handful of the press for its duration, but the forum itself was very organized and it required quite a bit of teamwork to get as much coverage of the happenings as possible. We brought you live coverage of the Forum from the press room in the Wyndham Hotel which is where some of the sessions took place. Our coverage was split into four parts, the first dealing with CPUs, particularly the Willamette and the Timna processors that were demonstrated at the Forum. The second part focused on Serial ATA and USB 2.0, two technologies that were demonstrated (rather poorly demonstrated, but displayed nonetheless), the third part of our coverage focused on Intels workstation/server class Itanium processor and the fourth offered a summary of the show and some pictures of Itanium based servers. Now, two weeks after our initial coverage of the show we are bringing you a complete summary of all that weve seen as well as some unique insight into exactly what Intel and AMD have planned for the future. So without further ado, lets get to Intels first announcement, the Willamette.
August 10, 2000 3:34 AM - AMDs Voyage into the 64-bit Arena x86-64 Revealed.txt	 Ambition can be a double edged sword; on the one hand, it can push you to accomplish that which you'd previously never thought possible, yet on the other, it can set you up for a much larger failure in the long run. In our "little" microprocessor community, we've got two very ambitious manufacturers, the very same names you've been hearing and debating over for the past few years, none other than AMD and Intel. Prior to AMD's release of their Athlon processor, which, for the longest time bore the codename 'K7', placing AMD's name before Intel's in a sentence was pretty much unheard of. The company had been improving their stance in the desktop microprocessor industry, but they were still more than a few steps behind Intel. Now, just under a year after AMD's Athlon release, the company is discussing its plans to compete not only in the desktop, workstation and server markets but also in the extremely high-end enterprise market segment. You can't say that AMD hasn't come a long way from the time when their flagship processor was the K6. Intel has been keeping themselves busy as well; in addition to maintaining their usual product lines covering the mobile, value, performance and server market segments, they are juggling the launch of the new Pentium 4 all while preparing to introduce their first major step away from the IA-32 x86 instruction set architecture (ISA) to their first 64-bit architecture, IA-64. With IA-64, Intel is promising to be free of the shackles that the x86 ISA has placed on their CPUs for over 2 decades, but in doing that, they are also flushing the idea of high performing backwards compatibility with older IA-32 applications. If you look around the microprocessor industry, especially at those companies that already have 64-bit parts available, Intel's strategy for IA-64 isn't all that extreme. Other companies rely on emulation or even separate processors in order to maintain backwards compatibility with 32-bit applications for those customers that are not ready to completely migrate entirely to a 64-bit OS with 64-bit applications. This puts a lot of weight on the consumer (in this case, large businesses, not your usual definition of the word) to decide when moving over to a 64-bit platform would be ideal, since using this approach, you're almost never allowed to have the best of both worlds, a high performing 32-bit solution and 64-bit compatibility. AMD saw a major flaw with this approach and felt that there should be a way for them to become a supplier of a 64-bit processor without making the consumer sacrifice 32-bit performance for that support. We're already familiar with AMD's solution to this problem as they've already announced that they'd be extending the 32-bit x86 ISA to 64-bits with what they call x86-64, but now we're finally beginning to see exactly how x86-64 will work and what it will mean for AMD's future enterprise platform, the K8, also known as SledgeHammer.
August 22, 2000 8:45 PM - IDF Fall 2000 Day 1.txt	 Intel's Developer Forum, known to the rest of the computer loving world as IDF, kicked off today with some interesting news and demonstrations. Let's take a look at what AnandTech saw. The Keynote The event officially started at 8:00am with a keynote address given jointly by Intel CEO Craig Barrett and Albert Yu, senior vice president and general manager of the Intel Architecture Group. Beginning with an introduction by Craig Barrett, the keynote quickly jumped into the importance the processor in internet business and layout, a theme that was duplicated throughout the day. Throwing out catch phrases such as "modular Internet" and "e-Business", Barrett was quick to catch the attention of the OEM and large business companies while almost completely losing the consumer interest. It was not until Albert Yu came on stage around 9:00am PST was the real interesting stuff demonstrated and spoken about. Mr.  Albert Yu Maintaining the flow brought about by Barrett's speech, Yu quickly moved into demonstrating just what Intel has in mind for e-business servers. The demonstration included the running of a live e-commerce application (actually nothing more than an mp3 streaming server) on clustered Itanium servers, with no data interruption when the power to one of the servers was cut on cue. Yu announced that Intel has been shipping 6,000 prototype Itanium systems and has already made 350 common business applications ready to port to the new IA-64 instruction set. If you recall from our x86-64 review last week, implementing 64-bit applications on the now delayed Itanium processor will take more than just a bit of time and is most likely part of the motivation for pushing the Itanium shipping date back from Q4 of 2000 to Q1 of 2001. Soon after the Itanium demonstration, Yu announced that 1GHz Xeon parts running on a 133MHz bus with 256KB of L2 cache were now shipping as of today. Whether these parts will be available or not has yet to be seen. Also, we have yet to see an available server platform to use with these 133MHz FSB Xeons that doesn't use RDRAM. Yu next went to discuss the Pentium III mobile processor, making the transition from large scale servers to small notebooks. While the talk did consist of some discussion about the Pentium III's speed step technology, which allows for different CPU clock speeds and voltages when the laptop is plugged in and when it is on battery, the majority of the talk about the mobile parts seemed to be more of a competitor bash than a product description. The mobile section of Yu's talk was mostly focused on stressing how small the amount of power the CPU consumes is in comparison to the system as a whole. It seemed that Yu was mostly concerned with downplaying the issue of low power consuming laptops by showing that other system components quickly eat up much more power than the CPU itself. This may or may not be true, depending on how Intel got it's CPU power consumption, which they claimed to be as low as .85 watts when running a standard application. We know that the peak power consumption of all mobile Pentium systems is greater than this low number, suggesting that Intel is quoting power usage amounts when the CPU is essentially "asleep", like when typing in an office document. Only the release of the Transmeta processor, as well as other low voltage CPUs will tell if Intel is right about CPU power, preaching that the power of other system components (the graphics processor, for example) must be decreased before any extension of battery life is observed. Finally, Yu went on to talk about the already announced Pentium 4 and its Net Burst technology, as we discussed in our Monday review of Net Burst. Most of what Yu described was already covered in detail in this article, but we did get to see two Pentium 4 systems in action. The first demonstration involved a 1.4GHz Pentium 4 system with 400MHz (PC800) RDRAM and involved running a "photo-realistic 3D application." Although this sounds impressive, all it proved to be was an application that looked nearly identical to the pond demo that shipped with the original GeForce256, even having the same NVIDIA logo on the bottom. Yu and the Intel employee demonstrating the product proclaimed that only the power the the Pentium 4 could produce such photo-realisitc effects. We beg to differ, as even our old Pentium 550E had no trouble running the original pond demo that shipped with the GeForce256. Along with the "power" of the Pentium 4 that was displayed with the "photo-realistic 3D application," Yu demonstrated on the fly DV to MPEG2 video decoding. This, unlike the pond demo, would prove to be rather stressful for a processor at lower speeds and without SSE2, as the demonstration proved. Running the encoding and decoding process on a Pentium III 800 proved to be a challenging task, dropping 408 frames out of the 938 sent to it. The Pentium 4 system, on the other hand, had no problem performing these highly complex calculations, capturing 938 out of 938 frames. The final part of Yu's demonstration included showing an air-cooled Pentium 4 system running at 2.0GHz. Impressive in itself, as it proved that Moore's law still applied (it was exactly 18 months ago that Intel demoed a 1GHz system), the system was only running CPUID and nothing else. Much like the 1.5GHz Pentium 4 system demoed at last spring's IDF, it is most likely that only CPUID was run due to the fact that stressing the system with other applications would result in system instability.
August 23, 2000 6:42 PM - IDF Fall 2000 Day 2.txt	 The Intel Developer Forum entered day 2 today, bringing with it a bit more info regarding Intel's vision of the future. The day began with the usual keynote, where Ron Smith, vice president and general manager of wireless communications and computing group at Intel was able to demonstrate Intel's new XScale microarchitecture Targeted at hand held devices and smart appliance, the XScale microarchitecture is based on Intel's current StrongARM technology that is widely incorporated in products today, such as the empeg as well as the Compaq iPaq. XScale microarchitecture promises to bring a whole new world to these devices, as the beta silicon demonstrated was able to show not only extreme speed but also very low power consumption. Perhaps the most interesting part of the keynote was when Smith was actually able to show of the new product. Running Drystone 2.1 benchmark, the small development platform featuring the beta XScale core technology was first displayed running at 600 MHz and consuming .5 watts of power. Next Smith showed the dynamic frequency and voltage capabilities of the XScale processor core, dynamically pushing the chip up to 800 MHz with less than 1 watt of power. The chip was now running at over 1000 mips (million instructions per second), surpassing the watt per mips ratio seen an any prior product. Next was to push the XScale core up even more, reaching 1 GHz with 1.75 watts of power consumption and 1270 mips, a figure that seemed quite impressive to us, considering that the chip is targeted at low power, high performance hand held systems. The possibilities that come with having such a high performance, low cost, low power chip seem endless, with the prospect of running MPEG2 quality video on a hand held not too far away. Intel stated that the XScale core used "super pipeline technology," that allowed for the reaching of high clock speeds. This could be in reference to increasing the length of the pipeline and lowering the effective number of instructions per clock in order to reach a higher operating frequency. This would be much like what Intel is calling their "Hyper Pipelined Technology" which is used to help increase the operating frequency of the upcoming Pentium 4 processor. Next was a demonstration of not how fast the XScale can go, but rather how slow. By taking into account the fact that power is equal to capacitance times frequency times the square of the voltage, XScale's dynamic voltage and frequency adjustments have the possibility to produce a very low power consuming chip. The beta XScale core demonstrated, based on a .18 micron architecture, was able to run at a mere .055 watts at a speed of 50 MHz. Still able to process 250 mips, the XScale chip at this speed is able to be run off a single AA battery. Although the 250 mips number coming from the 50 MHz XScale core may not seem very fast in comparison to the 1270 mips of the 1 GHz XScale chip, when it is compared it to the .25 mips of the IBM PC/XT or the 100 mips speed of the early Pentium chips, the 250 mips is quite impressive, especially when one considers the low power consumption of the chip at this speed. It was a bit odd to contrast the presentation of Intel's mobile section from Albert Yu yesterday with the XScale technology demonstration of today. If you recall from our IDF Fall 2000 Day 1 report, yesterday Yu stressed that the power consumption of the CPU in notebooks is not of concern, as the CPU only takes up a small amount of power when compared to other parts of the system (more on this later). Today the scene was a bit different, with Smith stressing the very low power consumption of the XScale microarchitecture as a result of its dynamic voltage management. Sure, the XScale technology is NOT target for notebook solutions by any means, but with the line between notebooks and hand helds becoming less and less defined it only seems logical that the issues with the two systems must be related.
August 30, 2000 2:13 AM - IDF Fall 2000 Summary.txt	 This year's fall IDF conference put on by Intel turned out to be quite an interesting show. As usual, the meat of the show consisted of "tracks," grouped seminars regarding a specific technology. Aimed to inform OEM companies, developers, and the press of what is going on around Intel and what future technology we can look forward to, these tracks proved to be very informative as well as interesting. Since many of you could not be there, we covered exactly what went on at the tracks we attended at the first two days of the show. You can find our day 1 report here and our day 2 report here. In addition, we recently wrote an interesting article about two wireless technology tracks we attended at the conference which can be found here. Besides all the key note speeches and the track sessions, IDF is just like any other computer show where companies come to show of their own upcoming and present technologies. As with many shows, IDF has a showroom floor that serves as a place to demonstrate this technology. In the case of IDF, the show floor was open towards the end of the day, when tracks were dying down. It was here that we got to see what interesting stuff companies other than Intel had to show. Before we take a look at what we saw on the show floor, let's take a look at beautiful San Jose, where IDF was held this fall. As you can see, this view from the adjacent Hyatt hotel shows that the San Jose convention center is quite large, especially considering the fact that this view only shows about half of the center. Naturally the weather was beautiful, as it tends to be in California. Intel, who had to organize lunch for the 5,000 conference attendees, had quite a task on their hands. The picture above shows the massive dining hall that provided weary conference goers both food and drink.
November 21, 2000 6:13 PM - AMD CPU  Chipset Roadmap - November 2000.txt	 AMD has truly made 2000 into a banner year for them. Amidst skepticism as to whether or not they would be able to overshadow their numerous shortcomings and actually make the Athlon a successful alternative to Intels flagship processors, AMD has gone above and beyond the expectations of virtually everyone in this industry. If you had talked about AMD as being a performance leader back at the release of their K6-III processor last year you wouldve been ridiculed. However, with the most recent statistics showing that AMD has almost 40% of the performance desktop market, 24% of the market belonging to Athlon CPUs alone, it is obvious that things have turned around for AMD. However, when looking at the performance advantage the Athlon holds over the Pentium III in most applications and games, you quickly realize that AMD hasnt earned its piece of the pie based on sheer performance. In many cases, the performance difference between an Athlon and a Pentium III came down to a few percent, and in some cases the Pentium III even came out on top. So why has AMD had such a wonderful year while we started out our article on Intels future by saying the exact opposite? It truly comes down to the issue of execution and their roadmap. For the first time in quite a while AMD had executed with near perfect precision. They were the first to the 0.18-micron process and the first to release a number of clock frequencies including the landmark 1GHz. Just less than a month ago AMD launched their 760 chipset which brought Double Data Rate (DDR) SDRAM down to the performance PC level, and it wont take long, as youll soon see, to bring that down to even lower cost price points. Three days ago we brought you a look at Intels plans for the remainder of this year and their roadmap for 2001. And just yesterday we gave the keystone to their roadmap, the Pentium 4 processor, a very thorough look. With almost all of Intels cards on the table, its time to take a look at AMDs hand and see what theyre holding. Before we dive into AMDs roadmap lets point out a few very important topics to keep in mind regarding the future of both Intel and AMD: Intels roadmap is heavily dependent on two things: the execution of the Pentium 4 and the ramping of its clock speed. If either one of these things fails to come through, then Intel will be in a very unfortunate situation. As we discovered in our Pentium 4 Review, the processor itself requires quite a bit of attention for its true potential to be seen. SSE2 optimizations are almost a requirement to see the Pentium 4 excel in quite a few applications. On the AMD side of things, the Athlon is running wonderfully, and when combined with a DDR platform its performance at 1.2GHz puts the Pentium 4 to shame. Unfortunately the Athlon processors of today are simply getting too hot, and at 1.2GHz the Athlon already puts out more heat than the Pentium 4 at 1.5GHz. The Duron is still not selling as well as it should be in North America. Apparently the sales in Europe are incredible, however the Duron is still in need of a truly low cost platform to run on. With that in mind, lets take a look at how AMDs future stacks up.
March 5, 2001 12:00 PM - Intel Developer Forum Conference - Spring 2001 Part 1.txt	 Currently, Intel is far from the most popular CPU manufacturer among the enthusiast community. During the days of the Pentium II and Celeron, when AMD's most fierce competition was the AMD K6-X line of processors, the community was definitely much more appreciative of Intel processors. In fact, looking at the stats from our System Rigs Database for AnandTech Forums users we see that there are still 13% more users running Intel CPUs than there are with AMD CPUs. This isn't to discredit what AMD has been able to do with the Athlon. Had the Athlon not been the success it is today, that 13% gap would have widened considerably. Combined with the weakened Intel product line towards the end of 1999 and the lackluster performance of the Pentium 4 in today's applications and benchmarks, it isn't surprising that AMD has been the choice of many lately. At this year's Spring Intel Developer Forum Conference the tone was of a completely different nature than what we have seen at IDF for the past few sessions. Within the course of this article you will not only learn about some very interesting technology from Intel for use in the server market, but you will also see their plans for DDR SDRAM and RDRAM, a chipset with 6.4GB/s of memory bandwidth, the return of the Memory Translator Hub and much more. A different type of IDF If you remember this time one year ago, there was a lot of activity at IDF as the Pentium 4, then going under the codename Willamette, was first revealed to the public. We got the chance to see the first glimpse of what a 1.5GHz processor looked like and we were shocked by the fact that the Willamette's Integer units would operate at twice that frequency: 3GHz. Later that year, the Pentium 4 was released and although it offered clearly advanced technology on paper it would not perform well in any of today's applications and games. A lower clocked Pentium III or Athlon would easily trump the Pentium 4 even while the latter was running at a 25% higher clock speed. However passing judgment on the Pentium 4 wasn't as simple as we had originally thought. There isn't a doubt about it that the performance of Rambus DRAM (RDRAM) on the Pentium III platform was poor at best. Realizing that the Pentium 4 actually benefited from RDRAM helped us understand exactly why Intel pushed for RDRAM so early, even though the Pentium III couldn't offer the performance improvement both Intel and Rambus were promising. The Pentium 4 is a completely different beast, its 100MHz Quad-pumped FSB keeps the processor fed with data and coupling it with an equally bandwidth powerful memory bus is necessary to keep its performance levels high enough. We have already illustrated this not too long ago in our review of the VIA KT133A chipset in explaining why the performance improvement gained from DDR SDRAM on the AMD 760 isn't as great as we'd like to think. At this IDF, the tone was definitely more sedated. There was much less marketing hype surrounding the show and more of an idea of looking towards the future. The Pentium 4 has been released, and Intel will be ramping up production of it faster than they ever have in the past. And for the first time since AnandTech has been visiting IDF we really got a chance to see Intel focus on the future rather than trying to explain why current performance is the way it is. There's less hype for the press to write about, but there's actually worlds more information and technology to digest. Let's get started.
May 14, 2001 12:00 PM - AMD Athlon 4 - The Palomino is Here.txt	 Although they may not be the largest x86 processor manufacturer on the block, they are definitely the favorite of quite a few in the enthusiast community these days. Their recent burst in popularity is not without merit. The launch of the Athlon microprocessor close to two years ago has truly brought desktop performance to new levels. It is clear that the Pentium III architecture is an aging one, so the Athlon's original competitor is now on its way out of the picture. What AMD does have to worry about is the Athlon's latest bully, the Pentium 4. From a purely architectural standpoint, the Pentium 4 is quite impressive. Its performance in today's applications is disappointing but judging by its performance in upcoming applications and games, the Pentium 4 does have a much brighter future than was originally thought. On a performance level, the Athlon can continue to compete with the Pentium 4 but the match up won't be anything like it has been for AMD in the past where the Pentium III was its only competitor. The Pentium 4 platform offers more memory bandwidth and the promise of higher clock speeds than the Athlon's Thunderbird core can deliver. AMD isn't one to rest on their laurels however and with that in mind they have already scheduled three replacement cores for the Thunderbird. The core being announced today is one of the most highly talked about improvements to the Athlon core ever since the Thunderbird was released in June of last year. This of course is the Palomino core which is found in the latest release of the Athlon processor that AMD is calling the Athlon 4. Why Athlon 4? Technically speaking, the Palomino core does mark the fourth AMD Athlon core since the release of the original K7 core in 1999. If we begin counting at the K7 core there was the 0.18-micron Athlon which was based on the K75 core, then the 0.18-micron Thunderbird with on-die L2 cache and the fourth Athlon core would be the 0.18-micron Palomino core. However our most astute readers will know very well why AMD chose to call the Palomino the Athlon 4. Intel has spent millions of dollars on their Pentium 4 marketing campaign, marking this as their largest processor ramp in history. By the end of 2001 Intel expects the Pentium 4 to completely push out the Pentium III in the markets and in order to do so they must create quite a bit of brand recognition. AMD is obviously using the name Athlon 4 to build off of Intel's aggressive marketing. It is much like what they did with the K6-III processor. Originally the K6-X series of processors used regular numbers after the name to denote the processor series, for example the original K6 was just called the K6 and the second generation part was called the K6-2. Before the third generation K6 launched Intel called their latest processor at the time the Pentium III, so AMD went ahead and called their third generation K6 processor the K6-III. The reasoning behind calling the Palomino the Athlon 4 is completely marketing. We can go into the reasons why they should've or why they shouldn't have done so but honestly it doesn't matter when it comes down to performance. You all are smart enough not to fall for marketing gimmicks so there's no point in drawing this discussion out any further. AMD decided to call the Palomino the Athlon 4, case closed.
June 5, 2001 12:00 PM - AMD 760MP  Athlon MP - Dual Processor Heaven.txt	 Support for multiprocessor (MP) configurations has almost always been present in AMD CPUs. In fact, the AMD K6 supported MP operation; however, it lacked the chipset support to bring it to the MP market. The CPU wouldnt have done too well in that market in any case, but the technology to take it there was present. The original Athlon released in 1999 was perfect for MP systems as well, especially since when it first came out it was offering performance greater than that of Dual Pentium III systems while only being used in a single processor configuration. Unfortunately, AMD had enough problems getting the Athlon accepted as a single processor desktop solution, much less an MP workstation/server platform. Since then, the Athlon has enjoyed tremendous success in the performance desktop market; it was only a matter of time before it was finally paired up with a truly high-end platform to try to attain the same type of success in the server and workstation markets. However, in order to succeed in these two markets AMD cannot be dependent on companies like ALi and VIA to provide chipsets for their processors as neither of the aforementioned companies even intends to branch out into the truly high-end workstation and server markets anytime soon. Instead, AMD took it upon themselves to design the chipset that would drive their Athlon processor into the workstation/serer markets: the 760MP chipset. Unlike the desktop 760 chipset, AMD does intend to manufacture the 760MP chipset for as long as there is demand. Their roadmap doesn't have the 760MP being replaced by a third party solution anytime soon, mainly because of reliability issues. The 760MP has been going through revisions for two years now, with AMD insistent on making its launch as picture-perfect as possible. While AMD has gained clout in the enthusiast market because they are much more reliable than they once were, the same reputation doesn't follow in the workstation and server markets. The immense amount of testing that the 760MP has been through virtually guarantees it to be the most reliable Socket-A chipset ever to be made available. On the CPU front, the 760MP's release is accompanied by the first server version of the Athlon core. Just three weeks ago AMD announced their mobile Athlon 4 processor, which is based on the Palomino core. The server version of the Athlon is also based on the same Palomino core but carries a different name, much like how Intels Xeon uses the Willamette core but carries a different name from the desktop Pentium 4. The name of the server Athlon is the Athlon MP, the MP obviously coming from the fact that the CPU is validated by AMD for operation in multiprocessor mode. From an architectural standpoint, the Athlon MP is no different from the mobile Athlon 4. Although AMD introduced new naming for some of the features of the Athlon MP such as Smart MP Technology, the same features are present in the mobile Athlon 4. Although Athlon MP is the only CPU that is validated by AMD for operation in dual processor (DP) mode, it isnt the only processor that works in DP configurations. In fact, the first and only 760MP motherboard being released today was tested and debugged using regular Athlons with Thunderbird cores. Even Durons will work in DP mode without any problems, but AMD is only officially supporting configurations with Dual Athlon MPs. This is somewhat like Intels insistence that the Celeron would not work in MP mode, though ABIT obviously proved them wrong with their BP6, which was designed with DP Celerons in mind. We will get into the technology behind the Athlon MP and the 760MP chipset later in this article, but first its important to establish the rules of the game when it comes to the high-end workstation and server markets that the Athlon MP and 760MP target.
October 8, 2001 8:30 PM - The Future of CPU Packaging Intel's BBUL.txt	 It's the people behind the scenes that rarely get the credit they deserve. The people that help put together news broadcasts, movies, and even microprocessors. By far the focus of most discussion about the CPUs we use in our systems is about their silicon; the architecture of the processor, how much cache it has, the bus interface, the clock speed, etc The only credit we give to the other factors that influence CPU performance and reliability is in our discussions of manufacturing processes. Even then we only talk about how large the die is, how high (or low) yields are expected to be, and what the size of the circuits (manufacturing process) being used. In all of this we actually overlook one of the most important parts of the CPU that is key to stability, performance and the ramping up of clock speeds. There are two major parts to every CPU, the silicon that actually contains the "processor" and the packaging that connects the core to the rest of the outside world. The packaging of a processor can control how high of a clock speed the CPU will reach, and it can also control factors such as FSB frequencies. How does something as simple as the packaging of a CPU control things like clock speeds? To answer that question we'll have to dive into the world of packaging for a moment.
October 23, 2001 2:57 AM - AMD's Hammer Architecture - Making Sense of it All .txt	 When AMD first revealed the K7 architecture at the Microprocessor Forum in 1998 who would have thought that the Athlon would turn out to be this successful? This two-year old micro architecture from AMD has been able to dethrone Intel's Pentium III and make many users think twice before adopting the Pentium 4 platform. Today the Athlon XP boasts the highest desktop performance of any x86 processors and it does so based on the K7 architecture that was originally introduced at MPF. Fast forwarding to the present day; AMD has gained a significant amount of market share and at this year's MPF the underdog in green introduced the details of their next-generation microprocessor architecture under the "Hammer" codename. Knowing what you do about the stellar performance of the Athlon and its humble beginnings at MPF '98, what can AMD possibly do to surpass their present achievements?
February 11, 2002 3:58 AM - Inside Intel From Silicon to the World.txt	 Almost as absurd as the idea of Intel backing out of their IA-64 development in favor of x86-64 is the unfortunate perception that the worlds largest desktop microprocessor manufacturer is not driven by engineering but rather by marketing. If microprocessor design were easy enough for a team of PR agents to tackle then many of us would have to change majors or head back to school very soon. The fact of the matter is that Intel is as driven by innovation as they are by engineering and in order to get an idea of what exactly goes on inside Intel, we made a trip up to the Intel Labs in Hillsboro, Oregon. This article wont cover any NDA information, it wont reveal secrets about Intels roadmaps for 2002, rather it will serve as a bit of insight into how Intel operates at the engineering level. The Intel you normally hear about is the Intel thats pulling balloons down at Computex or the Intel thats suing VIA for something or other, today well be showing you the Intel that doesnt care about anything outside of making fast, reliable and powerful circuits.
February 25, 2002 5:53 PM - Intel Developer Forum Spring 2002 - Day 1.txt	 For the past few Intel Developer Forum conferences the first day has always kicked off with some very impressive demonstrations. In years past we've seen the first air-cooled multi-GHz CPUs, we've had technologies like Hyper Threading introduced and we've had revelations about future CPUs like Banias. The first IDF of 2002 however started on a much different but well appreciated note. Intel's CEO Craig Barrett began the first day of IDF with his thoughts on the economy and honestly I can say that his attitude and opinions regarding the future of the technology market are right on the ball. It's clear that Barrett has the right mindset to lead Intel out of the tough times that all technology companies are facing - we need to "innovate our way out of this recession." This year's Spring IDF is by far the largest I've ever attended, with over 4000 attendees and over 50 countries represented. This caused the venue to be moved down to the San Francisco Moscone Center instead of its usual habitat in San Jose. But of course you all came to read about the interesting demonstrations and not about the economy, San Francisco or the beauty of wireless internet access hotspots so let's get to it.
February 27, 2002 12:46 AM - AMD's Hammer in Action The most impressive demo of IDF.txt	 Intel has had a great IDF thus far; the attendance is great, the products are innovative and honestly it's an exciting time to be involved with the tech industry. The first developer forum of 2002 is testament to the fact that even when companies are hurting financially and the sun doesn't seem to shine just as bright as it used to, engineering and innovation will prevail. After all, it's a love for this sort of technology that attracted most of the brightest minds in this industry to the sector in the first place. In spite of all of that and in spite of the excellent keynote speech by Craig Barrett, the one thing that has been lacking from this year's IDF are the hard hitting product demonstrations that we're used to. Where's the air-cooled 4GHz Pentium 4? Where's the talk of the Pentium 4's successor? While IDF week is only halfway over, there has been a lack of some of the cool demos that we're used to seeing at these things - until we visited AMD. As you've undoubtedly heard, AMD has announced that they have a working version of their upcoming Claw Hammer processor. Not only did we get a chance to see it running two different operating systems, but we actually got to hold the CPU (how geeky can we get?) and take a look at AMD's first reference board based on the AMD-8000 chipset.
August 13, 2002 4:02 AM - Intel's 009-micron Process - More Details Emerge.txt	 There has been a lot of talk about AMD's forthcoming Hammer processor but very little time is devoted to its imminent competitor - Prescott. Intel's Prescott core will be a relatively major upgrade to the Pentium 4 platform; the chip will bring higher clock speeds, a 1MB L2 cache, Hyper Threading and new instructions to the Pentium 4. Many of these enhancements will be made possible through the use of a new manufacturing process for the transistors that make up the Prescott core. As mentioned in Intel's last quarterly report, over 50% of Intel's processor shipments are of 0.13-micron (130nm) CPUs such as the newest Pentium 4s. Intel's Prescott core, due out in the second half of next year, will be made on Intel's 0.09-micron (90nm) process. This new process is much more than a "simple die shrink" and actually contains a number of improvements, some of which we never expected Intel to be introducing this early. Today Intel is sharing some more information about their 90nm process, expanding on what they revealed in March. Manufacturing based on the 90nm process has already commenced; if you'll remember back to our story from March, Intel has produced functional SRAM silicon based on their 90nm process. Obviously producing working SRAM (cache) silicon is not nearly as complicated as producing other parts of a CPU, but the bottom line is that the process is working as expected and they're meeting internal goals for it. This is a welcome change from some of the dismal reports we've seen elsewhere about transitioning to smaller manufacturing processes. AMD's move to 0.13-micron was delayed and as of last quarter TSMC is only shipping around 1% of their parts on a 0.13-micron process. Intel has been able to extend a performance lead over the competition in recent times and part of the reason for this has been their lead in manufacturing capabilities. By the end of this year Intel will have Prescott samples produced on the 90nm process sampling to partners, and by the time the second half of 2003 comes around Intel will be shipping 90nm parts in volume. Although the first 90nm chips are far away from being in even our hands, we're able to give you a glimpse of what to expect down the road based on Intel's announcements today.
September 9, 2002 6:13 PM - Intel Developer Forum Fall 2002 - Preview.txt	 This article first appeared on the AnandTech Newsletter. Click Here to subscribe. The Fall 2002 Intel Developer Forum does not officially start for another couple of hours but we're already ready to bring you some information directly from the forum in San Jose, California. Our first meeting of the day was with members of Intel's Microprocessor Research Lab and as you can expect, the topic of discussion was quite interesting. We'll get to some of the things disclosed in a bit, but first we'll talk a little about the show itself. For the first time in quite a while the Intel Developer Forum has not been crashed by AMD; if you'll remember just 6 months ago AMD stole the show with the world's first public demonstration of Hammer. According to AMD they won't be at IDF: "The real reason is that this is the week of September 11th and we didn't think it would be appropriate to set up shop at IDF to bang a drum or show off the latest hardware/software/other bits when most folks have their minds on friends and family during this somber time."So unfortunately (and understandably) you won't be seeing any more Hammer photos or information during our week at IDF. With that said, there's a number of things that you will be seeing this week. There's going to be a lot of talk about the first Hyper-Threading enabled Pentium 4 CPUs, we're going to learn a little more about the mobile-wonder known as Banias, and although we won't see a demonstration of it at the show, some Prescott information should be flowing this week as well. As usual, to stay on top of our IDF coverage be sure to subscribe to the AnandTech Newsletter as we'll be sending out most of our coverage over the Newsletter first.
September 9, 2002 8:52 PM - Intel Developer Forum Fall 2002 - Day 1 May the show begin.txt	 Today marks the opening of the 12th Intel Developer Forum; for six years Intel has been putting on these developer forums but they have attracted much more than a simple software crowd. IDF has become a place for the AnandTech Community to get a look into Intels world and it has historically been a place for many interesting announcements. At past IDFs we have seen technologies such as NetBurst and Hyper-Threading unveiled not to mention the clock speed barriers have been consistently broken at IDF; just six months ago we were shown a 4GHz Pentium 4 processor and today is looking to surpass that demonstration as well. Todays keynote started at 1PM PST, which is quite common for the first day of IDF. The day usually starts later than normal, either that or is moved to the Tuesday of IDF week. The keynote was given by Paul Otellini, President and COO of Intel; unfortunately the keynote started off on a relatively slow note with most of the material being a regurgitation of past IDFs. Although the latter part of the presentation held some interesting demos from Intel, the majority of the presentation was stolen by Microsoft with some interesting demos of their Windows XP Tablet PC Edition. Much of Intels presentations were ridden with too much satirical propaganda and not enough meat unfortunately; even a normally energetic Pat Gelsinger seemed to be unenthused during his introduction, maybe it was just because he wasnt talking about his usually very interesting engineering topics. Hopefully things will get better as the week goes on. Orin Smith, CEO of Starbucks stopped by to serve coffee and talk about 802.11 in Starbucks coffee houses. Intels major announcements included the opening of a software college to train developers in optimizing for the next-generation of multithreaded/multicore CPUs as well as a ~500M transistor count for the Madison Itanium 2 core.
September 10, 2002 12:00 PM - Intel Developer Forum Fall 2002 - Day 1 An end to a start.txt	 To close Day 1 coverage of the fall Intel Developer Forum, weve got lots of exciting hardware to show to you that most consumers wont see for months. This new and exciting hardware is found at IDFs Technology Showcase, where all types of companies come together to show off their latest products or the latest products from allied companies. We were quite pleased to see future technologies like DDR-II, Serial ATA, and PCI Express when walking around the show floor. Granite Bay on Display By far the most interesting Pentium 4-related technology on the show floor was Intels upcoming Granite Bay chipset. Based on dual DDR266 technology, Granite Bay offers a massive 4.2GB/s of bandwidth for the Pentium 4s consumption. Intel can pretty much launch Granite Bay whenever they want at this point, but it seems as if theyre still waiting to see how AMD plays its cards with Hammer. Despite the uncertainty surrounding Granite Bays launch, motherboard makers Tyan and MSI are saying Q4 as the launch date for their Granite Bay boards. MSI's Granite Bay Motherboard Click to Enlarge Tyan's Granite Bay Motherboard Click to Enlarge
September 16, 2002 6:05 PM - Intel Developer Forum Fall 2002 - Hyper-Threading  Memory Roadmap.txt	 This year's fall Intel Developer Forum came to a close last week and we're here today to tie up some loose ends and conclude our coverage of the conference. There are many ways to measure the success of the show; attendance was relatively unchanged from the Spring show which is a good thing considering the current state of the industry. In terms of how exciting the show was, compared to last Spring there were definitely fewer announcements of interest; for example, here's some of what we saw just 6 months ago at IDF: - Details of the Itanium roadmap through 2004 - Early introduction of the Prescott core - Introduction of two new PC form factors (Tidewater and Bigwater) - The world's first public Banias demo on the Odem chipset This fall however, we were only given a little bit to digest: - More Banias information - Pentium 4 with Hyper-Threading 3.06GHz demonstration As usual, the Technology Showcase held some surprises but we definitely had hoped for more information on forthcoming products from Intel (a few Prescott tidbits would've helped seeing as how the chip will be sampling by the end of this year but it may be silence that's Intel's weapon against Hammer). Considering that the two major products talked about at this IDF were Banias and the HT-enabled Pentium 4 you'd expect us to devote the most of our time to covering those two. We've already talked Banias to death in our earlier coverage, so today we'll talk a lot about Hyper-Threading and what it will mean for your desktop experience.
September 23, 2002 2:14 AM - Inside ATI  NVIDIA How they make frames fly.txt	 Ever since we started our Inside series the two companies we've received the most requests for have been ATI and NVIDIA. After touring Intel's research labs in Oregon and visiting VIA in Taiwan, we felt it was time to visit the two kings of graphics - ATI and NVIDIA. We combined these two companies into one article for one reason in particular - our tours were complementary. ATI imposed very strict restrictions on photographs during our visit to their offices in Thornhill, Ontario; we saw a lot of interesting things at ATI's offices (including the foundation for their fountain of fire in the lobby of their main building) but we weren't able to take pictures of most of them. On the other hand, ATI sat us down with one of their chip architects and we were able to get a wealth of information about how their GPUs were made. NVIDIA wasn't able to set us up with any engineers for an extended period of time (although lunch with Chief Scientist, David Kirk is always informative) but they were much more lax on the picture front so we were able to bring you more of the behind the scenes from NVIDIA. The combination of the two visits provided us with enough material to put together this piece, so without further ado let's take you inside ATI and NVIDIA.
February 19, 2003 12:51 PM - Intel Developer Forum Spring 2003 - Technology Showcase.txt	 This article first appeared on the AnandTech Newsletter. Click Here to Sign Up. The Technology Showcase at the Intel Developer Forum is always a very interesting place to learn about next-generation technologies, especially since Intel has loosened their grip on what exhibitors can and can't show on the floor. While it used to be the case that you wouldn't see any unreleased Intel products on the exhibition floor, today we were able to see everything from Centrino notebooks to 800MHz FSB Pentium 4 processors running on 865 and 875 platforms. But in the usual IDF fashion, we'll provide coverage from a company that you almost always see around this time of the year - AMD.
February 20, 2003 12:52 PM - Intel Developer Forum Day 3 - More from the Tech Showcase.txt	 We just finished sitting through the third day of keynotes at IDF, and although there's not much new to report from Mike Fister's keynote on Enterprise computing we still have quite a bit to talk about from the Technology Showcase.
March 23, 2003 3:43 PM - Weekly CPU  Memory Price Guide March 2003 1st Edition.txt	 Introduction For those of you not familiar with the guides here is a brief overview on how we construct them: We select vendors to appear in our Price Guides based on two requirements: solid consumer feedback and having the lowest possible pricing. You'll notice that these vendors aren't paying us to be listed here; we do not accept requests to be listed here. We have tried to eliminate vendors with low feedback rating, but we do encourage you to do your own research before purchasing from any of these vendors. With the new and improved Price Guides we have not only increased our standards when evaluating online vendors, but we have also partnered up with our friends at CNet to offer yet another service to you all. Using CNet's Shopper.com search engine we can provide you with an additional pool of vendors to search from that are checked up on by CNet. You can wade through those results by clicking on the CNet Pricing link in the second to last column on the right. If there are any problems with the guide or a vendor's pricing changes dramatically then be sure to email the author listed at the top of the page and we'll take appropriate action. Remember that although some vendors may be cheaper, we only list those with generally positive feedback from a decent number of consumers. As usual, if you have any questions, comments or suggestions feel free to email us or post in the AnandTech Forums. Also be sure to check out AnandTech's Hot Deals Forum for even more great CPUs, Video Cards and other technology buys. Disclaimer AnandTech does not endorse any vendor listed in the following price guide. AnandTech does not sell positions on the Price Guide. AnandTech nor any of the vendors listed in this guide guarantee the prices presented in the following pages. This Week As usual, if you have any reports of incorrect prices please feel free to email us with the name of the vendor and the product/price that appears incorrectly on their website. This will help us produce a more useful guide in the future. Another good week for DDR shoppers. We are anxiously anticipating both 800MHz FSB Intel CPU's and 400MHz FSB AMD CPU's.
April 23, 2003 3:03 AM - AMD Opteron Coverage - Part 1 Intro to OpteronK8 Architecture.txt	 In AMD's entire history as a mainstream microprocessor manufacturer, they have never seen as poor execution as they did with the K5. After several months of delays, the chip finally hit the streets only to be a CPU that ran too hot and couldn't offer very competitive performance. On the flip side, AMD had never seen their engineers on top of their game like they did during the days of the K7. What eventually became the Athlon microprocessor was truly a feather in AMD's cap; what ended up holding back the success of the K7 core was sheer marketing and politics - two things that even the best design teams out of AMD could not dream to control. Even with the disadvantages AMD was faced with, the K7 still went on to become the underdog's best effort to date - and thus when they announced its successor, the K8, at the Microprocessor Forum in 2001, expectations were very high. We concluded our first article on AMD's K8 (aka Hammer) architecture in October of 2001 with the following: AMD is clearly not the company it was a few years ago. They are constantly making steps towards becoming more of an industry leader as opposed to the follower they have been criticized of being for so long; the Hammer architecture is the most vivid depiction of what sort of an industry leader AMD is capable of being. At the same time we shouldn't discount Intel as they still hold the majority of the market and they do have the potential to take their technology very far. What AMD's recent gains do prove however is that there won't be a return to domination for Intel anytime soon; this two man race will be continuing for some time to come. Both AMD and Intel have had their slipups; while Intel's have been more recently, AMD is far from immune to them. The technology behind Hammer is there, as is the potential for it to succeed. But AMD has a lot of work to do between now and its release in the next 12 months. Many forget that until the Athlon, AMD didn't have the best execution track record. It's a long road ahead for the Hammer design team, good luck guys. Fast forwarding to today, it's clear that AMD not only didn't hit their target release with Hammer, but the process was far from another K7 launch. We asked AMD's CTO, Fred Weber, to characterize the execution of the K8 launch and he fairly summarized it as not as good as the K7 but better than the K5/K6. It has been a very long road for AMD; dealing with a new microprocessor architecture, never-before-used manufacturing process (Silicon on Insulator) and all while facing very tough competition from Intel and a weakening world economy. The light at the end of the tunnel is finally able to be seen, and although the K8 isn't going to be available on desktops for another several months, its workstation/server release is here and it's called Opteron. This article will focus exclusively on the architecture of the K8 core and how it relates to Opteron, we have three other pieces published in tandem with this one that cover everything from enterprise performance to desktop performance of Opteron. Here are direct links to the other articles: AMD's Opteron Coverage - Part 2: Enterprise Performance AMD's Opteron Coverage - Part 3: 1U Server Roundup AMD's Opteron Coverage - Part 4: Desktop Performance We strongly suggest reading this article in its entirety before proceeding onto the rest, unless you're already intimately familiar with the K8 microarchitecture. With that said, and without further ado, let's get to it.
May 26, 2003 1:46 AM - Weekly Hardware Price Guides May 2003 Edition.txt	 It's been a while since we had a full blown guide installment, so this week is jam packed with a little of everything from the CPU, motherboard, memory and video card categories. If you haven't got a chance to take a look at our RealTime Price Engine, feel free to check it out. Last week we wrote a small introduction since we refer to the engine constantly in our guides.
June 2, 2003 2:23 AM - Computex Half-a-World Away - Part 1 Intel Roadmaps Revealed.txt	 This article first appeared on the AnandTech Newsletter. Click Here to Sign Up. Normally, every June the AnandTech Staff gets geared up for the 14 - 18 hours of plane travel to take us half way around the world to Taipei, Taiwan for the annual Computex tradeshow. Because of the threat of SARS, the manufacturers and event partners have delayed this year's show until the week of September 22nd instead of its usual period the first week of June. The hope is that SARS will be more under control by this Fall, but even despite this some manufacturers are already stating that they won't be attending the show even in the Fall. With Computex being my personal favorite show out of all of those we attend, and with the high quality of information usually given out at the show, it would definitely be a shame to potentially miss out on that this year - thus the idea of doing a remote Computex was born. The idea is simple; instead of flying to Taiwan to see all of the products and meet with the manufacturers, we met via email and got the products sent to us via carrier in order to bring you some of the sort of coverage you could expect from the show. We've split the coverage into three parts, the first focusing on Intel's roadmaps (as usual, the motherboard guys are great for giving us information on Intel's plans), the second focusing on Small Form Factor solutions and finishing up with coverage on what's new from the motherboard guys. There's no way to duplicate the greatness of the Computex show without actually having one, but we'll try our best to give you all the information you're looking for sans show.
September 16, 2003 4:27 PM - Intel Developer Forum Fall 2003  Day 1 Otellini's Keynote.txt	 It's another Fall and we're met with yet another Intel Developer Forum. Although a bit shortened this year, we're still expecting the usual amount of information to flow our way. Things started off on a bit quicker of a pace with the opening keynote by Paul Otellini, which you will hear about in the coming pages. We heard quite a bit about Intel's plans for threading with things like Hyper Threading, dual and multi-core CPUs in the desktop, workstation and server markets. You'll also learn about LaGrande, Intel's secure computing technology as well as Vanderpool technology, which we found quite interesting. Expect to hear lots more about Prescott , Dothan and maybe even Grantsdale as we make our way through the days of IDF.
September 16, 2003 11:36 PM - PM Forum - Q32003 Part 1.txt	 After the conception of our CEO Forum way back at the discussion table, we werent finished with what we had envisioned. Our first CEO Forum gave users and institutions that were geographically away from the Taiwan manufacturing hub the ability to understand where the market is residing and is going to reside in the near future. Granted, some of what the CEOs stated back then still has yet to be played out, but it still lent some interesting thoughts that many people did not expect. Our original plans were to take the approach of the CEO Forum from a strategic business standpoint, so as to leave room for another project down the line. After all, CEOs are best suited to answer these types of [business] questions. The standpoint we left room was the current and future technical details relating to a company's products. However, many times CEOs are not the best people to contact for this type of stuff. (Though, they are often tech-capable.) Instead, many of the day to day duties, which involve decisions such as which product line to pursue, are the responsibilities for the VP of Product Marketing or someone in a similar position. However from company to company, official companies titles and responsibilities vary and get blurred. This is more a function from the way a company is structured, if anything else. And in a very simple explanation, Product Marketing is a mixture of engineering, marketing, and public relations. Product decisions, in the broadest sense, can come down to who can provide the best bid price, a product that meets specifications, and technical troubleshooting (something companies even need in the development and life of the product). Because of this, we decided to go with Product Managers as those to include in our next forum, which is to take things from a product/technical standpoint. Product Managers are engaged with consumers and are well equated with the technical side, because they deal with engineers (they often are engineers) and are involved with the product decision making process. For the purposes and reasons we cited in our first CEO Forum, we are turning to Product Managers in the HQ of each motherboard manufacturer to get a global look of all the industry trends. The format is still the same as the CEO Forum, but we are just involving PMs (product managers) for a more technical look into industry trends. We should note that much of this took place in a months timeframe, about from the beginning of August. Some products were announced in this timeframe, and so some of the responses we are publishing appropriately reflect this. You will notice that Darryl Chan is "Marketing Devision Director" of Albatron, and the only one to be dubbed with "marketing" in this line up. We should make clear that he is also the head of all product managers at Albatron, but there is no official title for this position. Therefore, his official title has been listed for our purposes. Participant - Official Title, Company. Hunter Lee  Product Manager, ABIT Computer Darryl Chan  Marketing Division Director, Albatron Technology Co., LTD. Gerald Wang  Sr. Director, AOpen Inc. Richard Liu  Product Manager, Asustek Computer Inc. Steven Kuo  Product Manager, Chaintech Computer Fanny Chen  Product Manager, DFI Eric Kuo  Deputy Manager, Elitegroup Computer Systems Calvin Yen  Product Manager, Epox Computer Co., LTD. Chris Wang  Director of Product Marketing Division, Giga-Byte Technology Scott Yang  Senior Product Manager, Micro-Star International Co., LTD. Jonathan Yi  V.P. of Product Marketing, Shuttle Inc. Tom Yang  Section Manager of Product Marketing Division, SOYO Computer Inc. John Nguyen  Product Manager, Tyan Computer Note: From this first run of the mill, so to speak, we have been able to test the water and have gotten some good feedback. For one, the length of the article has been consistently raised as an issue, because executives, investors, marketing personnel have found the many pages plus the amount of quotes to cumbersome and often times redundant. It is understandable that time is money for them, as they are indeed busy people. Though, for us to leave out quotes that may be repetitive in tone/thought would go against our original intent, because the range and number of quotes help emphasis the majority or the minoritys thought on the subject. So if possible, we will try and split our forum articles into two sections. This way we maintain the style and berth of input that was originally intended and keep the length of the read much lower and easier than what we first introduced. The second issue has been the topic of drawing conclusions. We have tended not to voice where the market should turn, but instead provide as much information as possible and let readers decide for themselves. Basically, letting the cards fall where they may. This is something that we will continue, all the while juggling the need for us to lend our own thoughts and insights. It is a fine line that we will continue to keep watch for, and decide on for a case by case basis. With all that taken care of, on to AnandTechs first ever PM Forum...
September 26, 2003 7:54 PM - Computex 2003 - Day 4 XGI Motherboards and cheap Itaniums.txt	 More than anything else, Computex 2003 has been about fierce competition. The main competitive focuses at this years Computex were the launches of high-end desktop processors from AMD and Intel; AMDs Athlon 64 FX and Intels Pentium 4 Extreme Edition. Youve no doubt seen dozens of CPU reviews across the Internet comparing and contrasting these two high-end microprocessors in an attempt to help readers make informed decisions for their computing needs. Hardware review web sites eat up anything Intel and AMD nowadays, and so this type of wide coverage is not at all uncommon. A little background on the current CPU wars takes us back to 1999, when AMD launched their K7 (Athlon) architecture that sparked this arms race of sorts. Since then, both AMD and Intel have had more or less equal shares of the performance lead, with Intel taking the lead at the high-end the last 10 months or so, and AMD securing the low and middle-end channels during that time. A couple days ago, we saw AMD take back that performance lead from Intel. Now, the only question that remains is whether AMD can supply enough CPUs to channels and OEMs to meet demand. Given the large die size and relatively low yields of current Athlon 64 iterations over at Fab 30 in Dresden, Germany, AMD will no doubt have their hands full. Various manufacturers tell us that AMD is confident that they can produce 400,000 Athlon 64 processors by the end of this year. While certainly a step in the right direction, this quantity is barely enough to feed high-end customers and enthusiasts, and no where near enough to supply the mainstream markets. The competition in the desktop video card world is equally fierce, as ATI and NVIDIA have been biting and gnawing at each other for years now. This graphics competition really started to brew when ATI introduced the Radeon 8500 in the fall of 2001. After the GeForce4 came along just a few months later, however, NVIDIA pretty much held a dominant lead in every segment of the add-in video card desktop market up until September 2002, when ATIs Radeon 9700 Pro become readily available. ATI was easily the high-end desktop GPU leader after that, and right up until GeForce FX 5900 Ultras became available in late June of this year. It was a dead heat between NVIDIA and ATI after the GeForce FX 5900 Ultra introduction, but that ended a few weeks ago when the world learned of NVIDIAs serious DX9 shortcomings in titles such as Half Life 2. This seemed to signal the beginning of the end of NVIDIAs steady market share lead over the last year. However, just a few days ago it was announced by Valve that Half Life 2 was going to miss the September 30th target. In a way, ATI faces the exact opposite scenario that AMD is in; ATI can supply enough DX9 (9600 and 9800) video cards to market, but may not see an upsurge in those sales (mostly in the retail channel) in the near future because of Half Life 2s delayed introduction. Of course, there are other legitimate DX9 titles coming soon that will spurn sales of ATIs DX9 cards, and there even may be a great deal of users that figure they can simply buy ATI cards now and wait for Half Life 2 (which we suggest you do if youre buying now). Either way, competition is a great boon for any industry, and we certainly hope it never stops. Anyway, read on as we discuss the latest happenings in our Day 4 coverage of Computex.
October 23, 2003 9:30 AM - The Last Bout of 03  NVIDIAs GeForce FX 5700 Ultra.txt	 Our introduction to NV36 in the form of the GeForce FX 5700 Ultra has really been a different experience than we expected. We thought we would see similar gains on the 5600 that we saw the 5950 make over the 5900. We also didn't expect NVIDIA to drop the veil they've had on the technical aspects of their products. From the first benchmark we ran, we knew this would turn out to be a very interesting turn of events. In going down to San Francisco for NVIDIA's Editor's Day event, we had planned on inquiring about just how they were able to extract the performance gains we will reveal in our benchmarks. We got more than we had bargained for when we arrived. For the past few years, graphics companies haven't been very open about how they build their chips. The fast paced six month product cycle and highly competitive atmosphere (while good for consumers) hasn't been very conducive to in depth discussions of highly protected trade secrets. That's why we were very pleasantly surprised when we learned that NVIDIA would be dropping their guard and letting us in on the way NV35 (including NV36 and NV38) actually works. This also gives us insight into the entire NV3x line of GPUs, and, hopefully, gives us a glimpse into the near future of NVIDIA hardware as well. Aside from divulging a good amount of technical information, NVIDIA had plenty of developers present (a response to ATIs Shader Day, no doubt). For the purposes of this article, I would like to stick to the architectural aspects of the day rather than analyzing NVIDIA developer relations. It isn't a secret that NVIDIA spends a great deal of time, energy, and money on assisting game developers in achieving their graphical goals. But we believe that "the proof is in the pudding" so to speak. The important thing to us (and we hope to the general public) isn't which developers like and dislike working with an IHV, but the quality of the end product both parties produce. Truth be told, it is the developer's job to create software that works well on all popular platforms, and its the IHV's job to make sure there is sufficient technical support available for developers to get their job done. We should note that NVIDIA is launching both the NV36 (GeForce FX 5700 Ultra) and the NV38 (GeForce FX 5950 Ultra) today, but since we have already covered the 5950 in our previous roundups we will focus on the 5700 Ultra exclusively today. First let us look at the card itself.
December 17, 2003 9:15 AM - AMD Opteron 248 vs Intel Xeon 28 2-way Web Servers go Head to Head.txt	 The launch of the Athlon MP was a bittersweet victory for AMD just under two years ago. AMD was able to deliver performance that was significantly faster than Intels brand new Xeon, but despite performance leadership, the CPU never really took off. AMD had limited success in the server market with the Athlon MP, with most of their sales going to HPC customers for large clusters, but very few sales in the web and database server arenas. With no Tier 1 OEMs supporting the platform, most of the larger IT firms wouldnt touch the Athlon MP with a 10-foot pole, so Intel enjoyed uninterrupted dominance in the web/database server markets. By the end of the Athlon MPs life, Intels performance had improved significantly to the point where AMD no longer held a performance advantage (although their usual low cost was a factor), further reducing any reason to pursue Athlon MP based servers. The launch of the Opteron processor gave AMD a much needed breath of new life and energy, especially with the announcement that IBM would be producing servers based on the new Opteron platform. Unfortunately, IBMs designs are, once again, targeted at the HPC market and left the web and database servers for Intel and IBM processors to handle. More recently, Sun announced support for the Opteron in their 2004 product line, but again, it is on the shoulders of the 2nd and 3rd tier manufacturers to provide Opteron solutions for web and database serving applications. But before there can be a demand, there must be some information on the performance of the Opteron in these sorts of applications. Weve already seen how the Opteron can perform in most computation-intensive applications as well as workstation applications, but what about as a web server? Or a database server? In our original coverage of AMDs Opteron, we offered some performance analysis of both web and database server applications with the Opteron, but AMD has made a couple of steps recently to warrant a second look at the performance picture. First and foremost, the launch of 4-way Opteron platforms has made many of our IT readers (and us included) wonder how a 4-way Opteron would stack up against a 4-way Xeon MP box. With AMDs more scalable Opteron architecture, any performance advantages a 2-way Opteron had over a 4-way Xeon should, in theory, be greater. AMD has also recently launched higher clock speed versions of the Opteron at 2.2GHz, equal in speed to the fastest Athlon 64 FX currently available. But quite possibly one of the biggest reasons for this comparison is that weve been looking internally to upgrade our server platforms from the aging Athlon MPs and needed to evaluate the Opteron as a potential upgrade path. Since we last wrote about our server upgrades at AnandTech, we added a 2-way Xeon DP 2.8GHz server with Hyper-Threading and were pleasantly surprised with the performance offered by the platform. We have also spent a great deal of time looking at 4-way solutions for a potential upgrade to our database servers, also requiring a more in-depth look at the latest in Opteron offerings. We have more than just this one article to bring to you the full spectrum of Opteron performance; but to kick it all off, were going to look at web serving performance in a head-to-head match between the Opteron and Xeon. Were not going to rehash any of the Opterons architecture in this article, so make sure that youve read our Intro to Opteron/K8 Architecture before proceeding.
February 18, 2004 10:41 AM - Intel Developer Forum Spring 2004 - Day 1 Wider and Faster.txt	 Introduction IDF this year is shaping up to be a very informative week at the beginning of many changes sweeping through the PC world. We are about to see new chipsets, busses, processors, architectures, sockets, and memories, and some of those announcements are starting to be made here. The highlights of day one consist of PCI Express graphics card announcements, the state of DDR2 (and a little about RAMBUS as well), and, of course, the fact that Intel has officially announced its adoption of x86-64 for its Nocona Xeon core.Without further ado, let's check out what's new in computing.
February 19, 2004 12:47 PM - Intel Developer Forum Spring 2004 - Day 2 Roadmap Updates.txt	 The day two keynote wasn't quite as eventful as the keynote of Tuesday. Basically, Mike Fister, Bill Siu, and Anand Chandrasekher brought us up to speed on all the developing technologies and initiatives emphasized last Fall. The phrases we heard most often in the keynote were very similar to "as we indicated last IDF." We heard about Vanderpoolagain, this time with the corporate environment being emphasized rather than home use. La Grande was mentioned again as an important factor in future secure computing, but nothing really new was introduced. The possible downsides of the technology still haven't been addressed, but all this stuff is still very much in its infancy and is likely to evolve a great deal before anything solid comes about. Of course, roadmaps were updated. We have been filled in on the holes that were left by the introduction of 64bit extensions, and Mike Fister gave us the code names of the dual processing and low voltage IA-64 processors in development, as well as the Bayshore (PCIe and DDR2) platform for Itanium processors. There was also mention of visualization technology Silvervale, and reliability hardware Pellston of we know exactly as much as this sentence conveys. Fully buffered DDR2 DIMMs are also being placed on the map for enterprise platforms as well. Dothan is apparently ready for a "quick ramp" according to Anand, while we can expect the Sonoma mobile platform to be available in the second half of the year bringing all its enhancements to Centrino. We got another look at a rundown of the 64bit extensions Intel has implemented in the Prescott and Nocona cores which looks very much like it could have been a slide from AMD a few years back.
February 23, 2004 8:44 PM - Intel Developer Forum Spring 2004 - Wrapup.txt	 The final day's keynote is always a thought provoking experience. This is the time during the forum where Intel looks deep into its R&D labs and gives us a little glimpse of what the future holds. We heard from Sean Maloney, VP and GM of Intel's Communications Group, and Pat Gelsinger, the CTO of Intel, on all the latest and greatest ideas Intel is focusing on. In addition to the final day's keynote, this wrap up will take a look at the floor of the Technology Showcase. We will also be looking a little more in depth at what exactly is going on with PCI Express, ATI, and NVIDIA. We are still reading through documents and doing research on Intel's x86-64 extensions, though there isn't any more news we can bring you at this moment. As with other processors technologies, when x86-64 is finally enabled (when Nocona launches), we will have an in depth analysis of the architecture enhancements. Broadband Wireless Technology Back in the days of the original Quake, average users first realized that their computer just wasn't fast enough. In response, processors, graphics cards and systems were pushed to run games very well. Even still, games are the applications that tend to push users sytems to their limits. Sean Maloney pinpoints broadband as the next area that will push computers to their limit. As broadband wireless becomes a reality, portable wide pipes will push PDAs and other devices to actually use the data to which they have access. In looking at future technology to push portable devices, Intel is targeting key areas that are current bottlenecks with portable systems. Their first announcement of the keynote was of a 90nm NOR Flash Memory device intended to help speed up the normally slow memory used. Sean then ran a demo of a portable visualization technology (codenamed Carbonado) that can play full motion video and push quite a few polygons/second to run 3D games at smooth frame rates. At this rate, we may have to expand our graphics coverage to include cell phone GPUs. Unfortunately, Sean didn't want to talk much about their radio enhancements (indicating that the next IDF might lend a little more information in this area). He did indicate that Intel is exploring MEMS systems for use in radios. The success or failure of products using these technologies depends heavily on the availability of wireless broadband and pervasive networking. Intel isn't going to leave those technologies alone either. We saw a demo of Xilinx's implementation of the recently finalized AS interconnect standard. In addition, Intel is working on 10Gbps and 1Gpbs network switch silicon (90nm of course), 4Gbps optical transceivers (due out 2h '04), and even a 10 gigabit PCI-X ethernet card. Sean was also very happy with the current push toward 802.16 and WiMAX. One of the most interesting numbers Intel threw out is that they expect 802.16e (portable WiMAX) to pop up in 2006.
March 13, 2004 9:51 PM - AT News Update Centrino and Xeon 2004 Roadmaps.txt	 As Q1 begins to draw to a close, we have one more Intel roadmap to add to the mix. You may have already read some of our other Intel and AMD roadmaps from earlier this year. Intel also recently got some attention in our server shootout between Opteron and Gallatin/Prestonia cored Xeons. We have hinted at Nocona, Potomac, and Jayhawk earlier this year and in other roadmaps. Just as a small refresher, Potomac is the Xeon MP replacement scheduled for Q1 '05. Nocona (for 1P and 2P servers) will show up next quarter, while Nocona's successor (Jayhawk) will show up in 2005. The Pentium M derivatives are based off Intel's Dothan core. 2004 Xeon Roadmap CPU Manufacturing  Process Bus  Speed L2  Cache Size L3 Cache Size Release  Date Xeon 3.8GHz 90nm 800MHz 1MB 0MB Q3 '04 Xeon 3.6GHz 90nm 800MHz 1MB 0MB Q2 '04 Xeon 3.4GHz 90nm 800MHz 1MB 0MB Q2 '04 Xeon 3.2GHz 130nm 533MHz 512KB 2MB Already Available Xeon MP 3.0GHz 130nm 400MHz 512KB 4MB Already Available Xeon 3.06GHz 130nm 533MHz 512KB 1MB Already Available Xeon 3.06GHz 130nm 533MHz 512KB 0MB Already Available Xeon 3.0GHz 90nm 800MHz 1MB 0MB Q2 '04 Xeon 2.8GHz 130nm 533MHz 512KB 1MB Already Available Xeon 2.8GHz 130nm 533MHz 512KB 0MB Already Available Xeon 2.8GHz 90nm 800MHz 1MB 0MB Q2 '04 Above you can see we included the $3700 3.0GHz Xeon MP disclosed last week as a placeholder. The biggest news here is Intel's Nocona core, which is slated to start showing up in Q2. Nocona looks to become AMD's largest contender to Opteron, as it incorporates 64 bit registers. This implentation of x86-64, dubbed IA-32e, was originally announced at February's IDF. Akin to the Pentium 4 lineup, 3.2GHz is essentially the end of the line for 130nm as all new processors will be made using the 90nm process. Our roadmaps also hint at a Nocona release past 3.8GHz before the end of the year. Fortunately, Nocona will retain the same mPGA4 (Socket 604) design, even as it migrates from E7501 to the upcoming Lindenhurst (E7710) and Tumwater (E7515) chipsets in Q2. Intel's recently disclosed plans to dedicate a separate memory bridge will not show up in either of these chipsets. Dothan's fate seems much more complex than Xeon's. Centrino brought much excitement, as well as absolute dominance to the mobile market last year; and Intel shows no signs of letting up. However, we were perhaps most intrigued to see Dothan variants continue to show up in the blade roadmap as cheap 1P alternatives to Low Voltage Xeons. Even today, those 1.4GHz Banias (Centrino) Pentium Ms with 1MB L2 cache dance circles around just about any equivalently clocked chip. Dothans with 2MB of L2 cache even excite us. 2004 Pentium M Roadmap CPU Manufacturing  Process Bus  Speed L2  Cache Size Release  Date Pentium M 2.13GHz 90nm 533MHz 2MB Q4'04 Pentium M 2.0GHz 90nm 533MHz 2MB Q4'04 Pentium M 2.0GHz 90nm 400MHz 2MB Q3'04 Pentium M 1.86GHz 90nm 533MHz 2MB Q4'04 Pentium M 1.8GHz 90nm 400MHz 2MB Q2'04 Pentium M 1.73GHz 90nm 533MHz 2MB Q4'04 Pentium M 1.7GHz 90nm 400MHz 2MB Q2'04 Pentium M 1.7GHz 130nm 400MHz 1MB Already Available Pentium M 1.6GHz 90nm 533MHz 2MB Q4'04 Pentium M 1.6GHz 90nm 400MHz 2MB Q3'04 Pentium M 1.6GHz 130nm 400MHz 1MB Already Available Intel must realize Dothan is a surefire winner. As we can see, 2004 will be the year of Dothan. During the progression of the year, Intel will migrate Centrino's platform of choice from i855 over to i915 with 533FSB support. All existing 400FSB Dothans and Pentium M's will phase out before 2005. News of Dothan's release schedule falls hand in hand with Intel's recent news that it will move from a MHz based product naming system to a new "product named" strategy. Dothan's lower clock makes it a potential victim for Intel's MHz-is-better campaign. 2004 Itanium Roadmap CPU Manufacturing Process L3  Cache Size Release  Date Itanium 2 1.5GHz  130nm 6MB Already Available Itanium 2 1.4GHz  130nm 4MB Already Available Itanium 2 1.3GHz  130nm 3MB Already Available Itanium 2 DP 1.6GHz  130nm 3MB Q2 '04 Itanium 2 DP 1.4GHz  130nm 3MB Q2 '04 Itanium 2 DP 1.4GHz  130nm 1.5MB Already Available Itanium 2 Low Voltage 1.2GHz  130nm 3MB Q3 '04 Itanium 2 Low Voltage 1.0GHz  130nm 1.5MB Already Available For those who find Itanium 2 important, the roadmap is actually slightly more interesting than it looks. For starters, there is an impending revision the 1.5+GHz Itanium 2 which plans to boast an extremely impressive 9MB of L3 Cache. This new processor is based on a variant of the Madison core, dubbed Madison 9M. Unfortunately, there will not be an increase in the 128-bit front side bus; Madison 9M shall remain at 400MHz FSB. The Itanium roadmap gets particularly juicy when we see migration from Deerfield Itanium 2's over to Fanwood 400 and 533. Even though Fanwood will only replace the smaller dual processors, the move to a 533MHz FSB will continue to keep Itanium 2 competitive with big iron.
July 12, 2004 2:16 PM - Know your Processor Cores Codename Cheatsheet.txt	 In our world we deal with code names constantly. Nocona this, Alderwood that; RS480, NV40, Toledo, Tejas, etc... Since Anand and I find ourselves constantly flipping around the internet searching for the correct code names of things, we thought we ought to post a quarterly update on product code names, and basic information. As always, you can search our site and find more in depth coverage about all of the technologies covered. Below are the more common Intel processor names. Code Name Socket Process L2 L3 Max Freq FSB Pentium Northwood mPGA478 130nm 512KB 0MB 3.4GHz 800MHz Prescott LGA775 90nm 1MB 0MB >4.0GHz 800MHz Prescott 2M LGA775 90nm 2MB 0MB >3.73GHz 1066MHz Dothan 90nm 2MB 0MB >2.0GHz 533MHz Xeon Gallatin mPGA603 130nm 512KB 4MB 3.2GHz 400MHz Cranford mPGA604 90nm 1MB 0MB 3.66GHz 667MHz Potomac mPGA604 90nm 1MB 8MB >3.5GHz 667MHz Prestonia mPGA604 130nm 512KB 2MB 3.2GHz 533MHz Nocona mPGA604 90nm 1MB 0MB 4.0GHz 800MHz Irindale mPGA604 90nm 2MB 0MB >3.8GHz 800MHz Itanium Madison 9M mPGA700 130nm 1MB 9MB 1.7GHz 667MHz Fanwood mPGA700 130nm 1MB 4MB 1.7GHz 533MHz LV Fanwood mPGA700 130nm 1MB 3MB 1.3GHz 400MHz Montecito mPGA700 90nm 1MB 24MB ??? 400MHz Milington mPGA700 90nm ??? ??? ??? 533MHz LV Milington mPGA700 90nm ??? ??? ??? 400MHz Absent are Tejas and other experimental processor cores (from which we hear are not dead, just renamed). Of course we will add moreas we learn them! Below are AMD's code names. Code Name Socket Process L2 Max Freq FSB Sempron Thoroughbred mPGA462 130nm 256KB >1.8GHz 333MHz Paris mPGA754 130nm SOI 256KB ??? ??? Palermo mPGA754 90nm SOI 256KB ??? ??? Athlon 64 Clawhammer mPGA754 130nm SOI 1MB >2.6GHz 1600HT Newcastle mPGA939 130nm SOI 512KB >2.6GHz 2000HT Winchester mPGA939 90nm SOI 512KB ??? ??? Athlon FX San Diego mPGA939 90nm SOI 1MB ??? ??? Toledo mPGA939 Dual Core 90nm SOI ??? ??? ??? Stay tuned next week when we follow up this cheat sheet with a chipset one.
August 28, 2004 9:00 AM - bUpdatedb CPU Cheatsheet - Seven Years of Covert CPU Operations.txt	 Introduction Update: 8/27/04 - The charts have all been revised. Thanks go out to all the people that posted corrections in the comments section as well as sending them via email. In addition to the corrections, some further information and commentary has been added to the pages. For anyone that actually comes back to this article for reference information, enjoy the changes! Foreword by Kristopher Kubicki: From time to time we stumble upon some truly gifted and patient people here at AnandTech. Some weeks ago I wrote a CPU codename cheatsheet as just something to do in an airport terminal to kill time. Very soon after, an extremely diligent Jarred Walton showed me his rendition of the CPU family tree that he was keeping just for fun!? Knowing I was bested, I offered Jarred a chance at writing a pilot for AT, and here it is! Please enjoy the second, extremely thorough CPU Cheatsheet 2.0. But loud! what lurks in yonder chassis, hot? A CPU, my programs it will run! O Pentium, Pentium! wherefore art thou Pentium? Obscure thy benchmarks and refuse thy name. What's in a name? that which we call a chip By any other name would run as fast. My sincere apologies to Shakespeare, but that mangled version of Romeo and Juliet is an apt description of the world of computer processors. Once upon a time, we dealt with part numbers and megahertz. Larger numbers meant you had a faster computer. 80286 was faster than 8088 and 8086, and the 80386 was faster still, with the 80486 being the king of performance. Life was simple, and life was good. But that is the distant past; welcome to the present. Where once we had a relatively small number of processor parts to choose from, we are now inundated with product names, model numbers, code names, and features. Keeping track of what each one means is becoming a rather daunting task. Sure, you can always try Googling the information, but sometimes you'll get conflicting information, or unrelated web sites, or only small tidbits of what you're trying to find out. So, why not put together a clear, concise document that contains all of the relevant information? Easier said than done; however, that is exactly what is attempted in this article. In order to keep things even remotely concise, the cutoff line has been arbitrarily set to the Pentium II and later Intel processors, and the Athlon and later AMD processors. Anything before that might be interesting for those looking at the history of processors, but for all practical purposes, CPUs that old are no longer worth using. Also absent will be figures for power draw and heat dissipation, mainly because I'm not overly concerned with those values, not to mention that AMD and Intel have very different ways of reporting this information. Besides, Intel and AMD design and test their CPUs with a variety of heatsinks, motherboards, and other components to ensure that everything runs properly, so if you use the proper components, you should be fine. So what will be included? For this first installment, details on clock frequencies, bus speeds, cache sizes, transistor counts, code names, and a few other items has been compiled. The use of model numbers with processors is also something people will likely have trouble keeping straight, so the details of processors for all Athlon XP and later AMD chips and Pentium 4 and later Intel chips will follow. The code names and features will be presented first, with individual processor specifics listed on the later pages. As a whole, it should be a useful quick reference - or cheat sheet, if you prefer - for anyone trying to find details on a modern x86 processor. With that said, on to the AMD processors. Why AMD first? Because someone has to be first, and AMD comes before Intel in the alphabet.
September 2, 2004 12:00 AM - Intel Roadmap Update Mobile and Server Edition.txt	 Going along with the recently announced addition of the 6xx series of Pentium processors in Intel's latest roadmap update, we have the latest information on their Mobile and Server chips as well. Things are a little less dramatic in these sectors, with no major name changes or additions being announced. You can compare this with our last update on the mobile sector from July 2004. Starting in the mobile sector, we have a couple updated launch dates as well as some new additions to the Celeron M and Mobile P4 sectors. You might recall, Intel currently has four different mobile lines. The high-end parts are the Pentium M, with a value counterpart called the Celeron M, and then there are the mobile variants of the Pentium 4 platform, dubbed the Mobile Pentium 4 and the Mobile Intel Celeron Processor. The Celeron M 380 has a targeted launch date of Q2'05, and Intel has also added a new Mobile Pentium 4 Processor, the 548. The remainder of the lineup for the mobile sector remains virtually unchanged. With the last update, Intel also added support for the execute disable (XD) bit across their entire product line. This corresponds to the NX bit from AMD. Previously announced parts without the XD support will continue to be sold for some time, and the XD versions will have a "J" suffix. Future processors will only come in XD flavors and may lack the "J" suffix. Similar to the Celeron "A" chips, the "J" is only used when there is overlap with an existing processor. We will include the complete range of upcoming processors in the charts below, while omitting many of the processors that have been shipping for a while. Please refer to our CPU Cheatsheet if you're looking for details on older processors. Intel Pentium M Lineup Processor Core Speed Cache FSB Launch Date Pentium M 770JDothan2.13 GHz2 MB533 MHzQ1'05 Pentium M 765Dothan2.10 GHz2 MB400 MHzQ4'04 Pentium M 760Dothan2.00 GHz2 MB533 MHzQ1'05 Pentium M 758J LVDothan1.50 GHz2 MB400 MHzQ1'05 Pentium M 755Dothan2.00 GHz2 MB400 MHzAlready available Pentium M 753J ULVDothan1.20 GHz2 MB400 MHzQ1'05 Pentium M 750JDothan1.86 GHz2 MB533 MHzQ1'05 Pentium M 745Dothan1.80 GHz2 MB400 MHzAlready available Pentium M 740JDothan1.73 GHz2 MB533 MHzQ1'05 Pentium M 738 LVDothan1.40 GHz2 MB400 MHzQ3'04 Pentium M 735Dothan1.70 GHz2 MB400 MHzAlready available Pentium M 733/J ULVDothan1.10 GHz2 MB400 MHzQ3'04 Pentium M 730JDothan1.60 GHz2 MB533 MHzQ1'05 Pentium M 725Dothan1.60 GHz2 MB400 MHzQ3'04 Pentium M 723 ULVDothan1.00 GHz2 MB400 MHzQ3'04 Pentium M 715Dothan1.50 GHz2 MB400 MHzQ3'04 Pentium M 713 ULVBanias1.10 GHz1 MB400 MHzQ3'04 There's not a whole lot new to say about the Pentium M roadmap. A few processors have been pushed back to Q1'05, and we continue to see several future processors that will not include the XD support. The only explanation would seem to be that Intel has already started production of these parts even though they are not yet available. Notice that all the 533 FSB Dothan parts include the "J" suffix while the 400 FSB parts do not. Intel Celeron M Lineup Processor Core Speed Cache FSB Launch Date Celeron M 380Dothan1.60 GHz1 MB400 MHzQ2'05 Celeron M 373 ULVDothan1.00 GHz512 KB400 MHzQ1'05 Celeron M 370Dothan1.50 GHz1 MB400 MHzQ1'05 Celeron M 360/JDothan1.40 GHz1 MB400 MHzQ4'04 Celeron M 353 ULVDothan900 MHz512 KB400 MHzQ3'04 Celeron M 350/JDothan1.30 GHz1 MB400 MHzQ4'04 The latest Celeron M chips are based off of the Dothan core, only with 1 MB of L2 cache instead of the full 2 MB. While we don't have any specific performance numbers yet, larger caches, especially in the mobile sector, do not necessarily improve performance a lot. This means that the "value" Celeron M mobile chips should be pretty similar in performance to the Banias platform, provided all other components are the same. This is good news for those interested in less expensive laptops. Somewhat less exciting is the release schedule for these chips. You can see the only addition to the Celeron M platform is the 380 part, which will include the XD bit but lacks the "J" suffix (similar to the 370 part). While performance should be close to that of the Banias 1.6 GHz part, it will not hit the market until the middle of next year. Right now, there is little competition for the Celeron M and Pentium M chips, and so Intel is in no rush to push faster chips into the market. Love it or loath it, it's simply good business sense. (Yes, the Athlon 64 is available in lower power DTR versions, but right now Intel owns the lion's share of the performance mobile market.) Mobile Intel Pentium 4 and Celeron D Lineup Processor Speed Cache FSB Launch Date Mobile P4 5583.60 GHz1 MB533 MHzQ1'05 Mobile P4 5523.46 GHz1 MB533 MHzQ1'05 Mobile P4 5483.33 GHz1 MB533 MHzQ3'04 Mobile Celeron D 3503.20 GHz256 KB533 MHzQ1'05 Mobile Celeron D 345/J3.06 GHz256 KB533 MHzQ4'04 Mobile Celeron D 340/J2.93 GHz256 KB533 MHzQ3'04 Intel seems to be phasing out the mobile P4 and Celeron chips, as there are fewer planned launches in this segment. No parts are listed for Q2'05 and beyond, so that may be the end of the line. We also see the new addition of the P4 548 part, with availability planned for the end of this month. This part was most likely added due to OEM demand. This will be a 3.33 GHz Prescott chip, taking up position between the already released 538 and the upcoming 552 parts. Since the 552 part isn't scheduled for release until Q1'05, OEMs probably felt that five or so months with no new Mobile P4s was too long. One last thing worth mentioning on the Mobile front is the upcoming chipsets Intel has planned. In Q3 or Q4 of 04, Intel will launch a tri-band chipset, something many never expected from Intel. 802.11a uses the less common 5.2 GHz radio frequency, which gave it superior transfer rates and less congestion but at the same time limited range. Intel has apparently seen enough demand for 802.11a support that they are now including it in their Intel Pro/Wireless 2915ABG chipset. Then, in Q1'05, Intel has the 815GM, 915PM, 915GMS, and 910GML "Express Chipsets" scheduled for launch. The 910GML is slated for the value Celeron M platform, but all chipsets will include support for PCI Express, obviously in a different package than the desktop PCIe slots. The 915 chipsets will also bring 533 FSB support to the Pentium M platform, which is arguably the more useful technology.
September 7, 2004 5:11 PM - Intel Developer Forum Fall 2004 Day 1 Keynote.txt	 Introduction We have been patiently waiting this Fall's Intel Developer Forum for quite some time. Rumors that Intel would host a live dual core demonstration have kept our interest piqued. It's tough to deal with that guilty kind of excitement, the kind where you hope for something you couldn't expect and the very hope itself seems to crush the chances of its fulfillment. After all the waiting, we can say that the rumors were true: Intel just finished demonstrating a running dual core Montecito Itanium processor. This is a good thing and a bad thing, but we'll talk more about that in a minute. Amidst a few new things, we did hear plenty of the same old thing from Intel about technologies that are either here now, or years off into the future (with no new insight). For instance, much of the keynote covered Hyperthreading, EM64T, and Wifi, or focused on previously demonstrated technologies like Vanderpool. While all of these things are fun and interesting, we've already heard about them time and time again. Granted, the Vanderpool demo was from a business perspective rather than a home user perspective. It's cool to see 4 different hardware virtualized systems running on one computer, but the concept's potential and its uses have been explored previously via software such as VMWare. But buried in the presentation were a few tidbits we did find useful, and that's what where here to bring you today. The following pages will cover the the new things mentioned at the opening keynote, as well as the growing importance Intel places on parallelism.
September 10, 2004 7:22 PM - IDF Day 3 AMD and Intel's Dual Core Demonstrations The Race to Dual Core.txt	 AMD and Intel's Dual Core Demonstrations; The Race to Dual Core The first time I, personally, had ever heard anything about "dual core" processors was several years ago at a regional conference when none other than Anand Shimpi gave a keynote about the advantages of parallelism, particularly with multiple core CPU applications. Needless to say Intel and AMD must have come to the same conclusions earlier this decade since today we have seen server demonstrations with 90nm server chips from both companies. As the timeless adage states, "Many hands make light work." We first got a hint that things would go toward dual core when we heard of a technology from Intel called "Hyperthreading." This precursor to truly multi-core processors emulated two processors on the same CPU. Many industry insiders almost took this as a warning for programmers to start optimizing and designing code for multithread operations. As quoted by Paul Otellini during Keynote Q&A of the recent Fall 2004 IDF, "over 90% of HT capable Xeons run with HyperThreading enabled." With the dual core announcement, forum members (even in the AMD suite) were constantly correcting each other as to new nomenclature; no longer do we use "2-way, DP or 2-processor" to describe a server. Instead the correct terminology becomes a "2-Socket configuration." A dual socket, dual core Intel configuration actually refers to eight logical processors! Four physical processors exist on two sockets/dies with HyperThreading enabled on all four. Extremely threaded applications - such as web and database servers - are specifically tailored to take advantage of the additional threads, and things only get better from there. During the Day 1 and Day 2 Keynote speeches, a technology that grabbed our attention was something called Vanderpool Technology or VT. VT is an upcoming feature on several processor families (including the dual core families) that allow processor emulation and/or virtualization. As explained by Intel, with Vanderpool Technology, we can actually run two operating systems simultaneously on the same machine, and even dedicate one processor to a given operating system, and another processor to a different operating system. Of course, no Intel Developer Forum is ever complete without a stop by the AMD hospitality suite usually within a block or two of the convention center. In fact, AMD has an extremely similar technology to Vanderpool dubbed "Pacifica." According to our AMD correspondents, Pacifica technology will allow us to run multiple operating systems and dedicate specific CPUs to those operating systems. AMD also unveiled to us a technology called "Presido", which looks identical to Intel's Lagrange Technology (LT).
September 13, 2004 12:05 AM - Intel Xeon 36 (Nocona) vs AMD Opteron 250 - Database Test.txt	 Introduction Over a year and half has passed since AMD announced their K8 architecture to the world, and what has changed? Well, "a heck of a lot" is the answer. The Opteron has proven itself as a worthy competitor to the infamous Intel Xeon line-up of processors, and Intel has been following AMD for a change, something no one could have predicted a few years ago. Dual-core processors are on the horizon; AMD demonstrated theirs with Hewlett Packard just a few weeks ago, and Intel demonstrated theirs at the Intel Developers Conference in September, 2004. So, we've seen AMD compete on both the desktop and server market, but does this transgress into a victory in corporate America? Well, it has certainly piqued their interest enough for Intel to comment about it in a recent news.com article. Itanium hasn't been quite the success that Intel was hoping for, but that doesn't mean that AMD has the server market by the reigns quite yet, not even close. AMD still has an uphill battle to fight, with Intel owning over 80% of the PC processor market, and AMD owning about 15% as of August 2004. To AMD's credit, they have signed a few of the first-tier customers like HP, IBM and Sun, and last November, AMD has announced their new manufacturing plant in Dresden to keep up with demand. One thing for certain is that neither of the processor giants are sitting around taking their success lightly. Well aware that AMD is knocking on the door, Intel has finally released the new Nocona line of Xeons, which follow in the 64bit footsteps of AMD with EM64T. AMD has released their latest Opteron clock increase with the new 250 line of processors, which is a 2.4 GHz Opteron for those who prefer the clock speed version.
October 22, 2004 3:09 PM - Intel's Dual Core Strategy Investigated.txt	 Been hearing conflicting dual core information lately? Here's a compilation of everything we have and know about Intel's dual core plans for the next two years. Dual Core for Desktops in 2005 Intel has yet to determine what brand they will market their first desktop chips under, although we'd expect them to continue to use the Pentium 4 brand but with some sort of appendage like Extreme Edition or Lots of Cores Version. Intel has, however, already determined what the specifications and the model numbers of their dual core chips will be. Currently set for release in Q3 2005, Intel has three dual core chips on their desktop roadmap: the x20, x30 and x40. The only difference between these three chips is clock speed, with the x20 running at 2.8GHz, the x30 running at 3GHz and the x40 running at 3.2GHz. All of the chips are LGA-775 compatible and run off of an 800MHz FSB. Hyper-Threading is not enabled with Intel's dual core chips. As far as architecture goes, the x-series of dual core CPUs from Intel are built on the little talked-about Smithfield core. While many have speculated that Smithfield may be Banias or Dothan based, it's now clear that Smithfield is little more than two 90nm Prescott cores built on the same die. There is a requirement for a very small amount of arbitration logic that will balance bus transactions between the two CPUs, but for the most part Smithfield is basically two Prescotts. But doesn't Prescott run too hot already? How could Intel possibly build their first dual core chip out of the 90nm beast that is Prescott? The issue with Prescott hitting higher clock speeds ends up being thermal density - too many transistors, generating too much heat, in too small of a space. Intel's automated layout tools do help reduce this burden a bit, but what's important is that the thermal density of Smithfield is no worse than Prescott. If you take two Prescotts and place them side by side, the areas of the die with the greatest thermal density will still be the same, there will simply be twice as many of them. So overall power consumption will obviously be increased by a factor of two and there will be much more heat dissipated, but the thermal density of Smithfield will remain the same as Prescott. In order to deal with the fact that Smithfield needs to be able to run with conventional cooling, Intel dropped the clock speed of Smithfield down to the 2.8 - 3.2GHz range, from the fastest 3.8GHz Prescott that will be out at the time. The reducing in clock speed will make sure that temperatures and power consumption is more reasonable for Smithfield. Smithfield will also feature EM64T (Intel's version of AMD's x86-64 extensions), EIST (Enhanced Intel SpeedStep Technology) and Intel's XD bit support. Chipset support for Smithfield will come from Glenwood and Lakeport, both of which support the 1066MHz FSB (as well as 800) and Dual Channel DDR-2 667 and 533. Glenwood (the successor to 925X) will support up to 8GB of memory, making it the perfect candidate for EM64T enabled processors that want to break the 4GB barrier.
October 27, 2004 4:01 AM - Intel CPU Roadmap Less is More.txt	 Intel CPU Roadmap Update We have already covered some of the major announcements from Intel in our look at Intel's Dual-Core Strategy, but plenty of processors will still be sold between now and when the first dual-core chips ship. We are going to focus primarily on the changes in this roadmap update, but we will list basic information on most currently shipping Intel processors for comparison. There are not that many updates from month to month in Intel's roadmaps, so to help you find the changes from our last Intel Desktop Roadmap we have highlighted them with red text. We will cover all three sectors - desktop, mobile, and server/enterprise - starting with what remains the largest in terms of volume, the desktop sector. Intel Desktop Lineup LGA775 Performance Processors Processor Speed Cache FSB Launch Date Other x40 Dual Core 3.20 GHz 2x1MB 800MHz Q3'05 EM64T, EIST, EDB x30 Dual Core 3.00 GHz 2x1MB 800MHz Q3'05 EM64T, EIST, EDB x20 Dual Core 2.80 GHz 2x1MB 800MHz Q3'05 EM64T, EIST, EDB Pentium 4 XE 3.73GHz 3.73GHz 2MB 1066MHz Q1'05 EM64T, EIST, EDB Pentium 4 XE 3.46GHz 3.46GHz 512K+2MB L3 1066MHz Nov'04  Pentium 4 670 3.80GHz 2MB 800MHz Q2'05 EM64T, EIST, EDB Pentium 4 660 3.60GHz 2MB 800MHz Q1'05 EM64T, EIST, EDB Pentium 4 650 3.40GHz 2MB 800MHz Q1'05 EM64T, EIST, EDB Pentium 4 640 3.20GHz 2MB 800MHz Q1'05 EM64T, EIST, EDB Pentium 4 630 3.00 GHz 2MB 800MHz Q1'05 EM64T, EIST, EDB Pentium 4 580J 4.00GHz 1MB 800MHz Cancelled  Pentium 4 570J 3.80GHz 1MB 800MHz Q4'04 EDB Pentium 4 560J 3.60GHz 1MB 800MHz Already available EDB Pentium 4 550J 3.40GHz 1MB 800MHz Already available EDB Pentium 4 540J 3.20GHz 1MB 800MHz Already available EDB Pentium 4 530J 3.00GHz 1MB 800MHz Already available EDB Pentium 4 520J 2.80GHz 1MB 800MHz Already available EDB Budget Processors Processor Speed Cache FSB Launch Date Other Celeron D 350 3.20GHz 256KB 533MHz Q2'05 EDB Celeron D 345 3.06GHz 256KB 533MHz Nov'04 EDB Celeron D 340 2.93GHz 256KB 533MHz Already Available EDB Celeron D 335 2.80GHz 256KB 533MHz Already Available EDB Celeron D 330 2.66GHz 256KB 533MHz Already Available EDB Celeron D 325 2.53GHz 256KB 533MHz Already Available EDB The biggest news is of course the announcement of the x20, x30 and x40 dual-core processors. At present, it is not clear if these are the final names for these parts or simply placeholders. The "x" may be replaced by whatever number eventually gets assigned to the dual-core parts. That's one interpretation of it, anyway, as Intel's roadmap refers to "6xx series" and "xxx" series parts - note the use of the lowercase "x" here. With the addition of the 600 series in the last roadmap, it stands to reason that the dual-core NetBurst chips will eventually be given a numerical designation. "X" is already one of the most overused letters in the alphabet as far as marketing goes, so we can hope that Intel will refrain from adding another component to the already crowded "Generation X" hardware genre. The Pentium 4 eXtreme Edition (TM) has already paved the way, however, so don't get your hopes up. There is no HyperThreading support scheduled for the dual core Processors. Besides the introduction of the dual core parts, most of the roadmap remains unchanged. A few parts have been pushed back one quarter on their launch date, and in the case of the 4.0 GHz 580J, it has been cancelled. Some will point to the product cancellations - there are a couple more elsewhere in this roadmap - as a sign of Intel's pending doom. We prefer a slightly less alarmist view and believe that Intel is simply being cautious. Overclocking headroom on Intel parts has always been pretty good relative to their competitors, and rather than pushing parts through validation at close to their maximum speed, it is probable that Intel is making sure the parts run reliably. Other than the ill-fated Pentium III 1.13 GHz Coppermine, Intel CPUs have usually been binned at least two levels below their maximum clock rate. Many enthusiasts are already overclocking Prescott cores to 4.0 GHz using only air-cooled HSFs, so such clock speeds are certainly attainable. With more recent steppings of the Prescott core (akin to minor software revisions), we would expect slightly more headroom that 4.0 GHz. Time will tell us whether Intel is really having problems or is simply being conservative - they could have always gone with a water cooled CPU setup like the Apple G5 systems if they felt the increased speed was truly worthwhile. If you look at the 600 series of processors, Intel has now announced plans to include a 3.0 GHz part, the 630. As these processors will all include 2 MB of L2 cache compared to the 512K L2/2M L3 cache of the earlier Gallatin-based P4EE (or the 1M L2/2M L3 of the socket 775 P4XE), performance should at worst match the older 3.2EE and 3.4EE parts on a clock-for-clock basis. Performance of the Irwindale chips (also referred to as Prescott 2M and Nocona 2M) should also at least match the current Prescott chips, and in most instances it will surpass them. NetBurst remains an extremely bandwidth hungry design, and you can almost see the gears turning in the brains at Intel: The P4EE was able to keep Intel competitive with AMD in the race for the performance crown, and if they combine the 2M cache of the P4EE with the higher clockspeeds and other architectural tweaks of the Prescott design, they ought to get the best of both worlds! With the 630 and 640 targeting the mainstream market, prices should be under $300 when the parts ship in early 2005. The P4XE will continue to be an ultra-expensive part, and to help it maintain a lead relative to the 600 series parts, it will be getting a 1066 MHz FSB on the future versions. Say what you want about the elegance of the design, but the fact is that a 2 MB L2 cache NetBurst chip will remain competitive with most of AMD's Athlon 64 parts. AMD will still hold the lead in certain applications - i.e. gaming - but Intel is not out of the picture yet. The only remaining change on the desktop platform is that the 600 series of processors as well as the P4XE 3.73 will now have their EM64T support activated. (The P4XE 3.43 uses the older Gallatin design, which is a Northwood core with L3 cache, so there is no 64-bit support to activate.) As Intel's position has always been that they will "have 64-bit enabled processors available when software support is ready," this makes sense. Windows XP-64 should finally ship within the next couple quarters, and 64-bit versions of Linux have been available for a while now. We have yet to see any significant benefit for 64-bit applications, but that should come as 32-bit applications are recompiled and tuned for the additional registers. One topic that comes up repeatedly in the latest Intel roadmap is Intel's "leadership role" in getting 64-bit computing to the desktop. Revisionist history at it's finest! The important thing is that, regardless of who did the work, both x86-64 solutions will be compatible. The same is true of Intel's "Execute Disable Bit" (EDB); in practice, it functions the same as AMD's XD and they are compatible.
November 4, 2004 4:13 AM - Multiple Front Side Buses Coming to an Intel Server Near You.txt	 Dissecting code names seems like a full time job for some of us lately. Fortunately, a full time job pays for my schooling so here are the breakdown of Intel's newest code names, and what they mean to us. Hopefully you have kept up with the last few articles from Anand and Jarred concerning dual core product SKUs and general Intel roadmaps. Of course, processors are only half of Intel's business - core logic composes the less glamorous side of Intel. Although we have been talking about Glenwood and Lakeport for several months now, today we are going to look at Intel's new server chipsets, Blackford and Greencreek. Greencreek (also known as Greencreek 2S - 2Socket) is slated as the Tumwater (E7525) replacement, while Blackford is the Lindenhurst (E7520) replacement. In short, the two chipsets are nearly identical with Blackford serving as the high end version of the platform. The large news here is Intel's push for dual front side buses. The latest Intel roadmaps thoroughly describe dual independent buses as:  "New 2S platform architecture for improved performance" 2S, of course, denotes 2 Sockets. With advances towards dual core processing, the idea of independent buses per socket probably indicates the best approach possible for Intel to de-saturate the front side bus. We already know that Twin Castle will utilize independent memory controllers, and with Greencreek/Blackford showing up soon after with dedicated paths to their memory controllers, we could see large boosts in memory IO. The roadmap indicates Twin Castle will utilize multiple buses in later revisions as well. Aside from dual buses, the new processors incorporate some other features as well, the largest being fully buffered DIMMs (FBD or FB-DIMM). The Memory Forum actually has a very concise IDF presentation with the memory architecture explained here. The apparent problem with DDR2 DIMMs (particularly when there are 8 or more) is that the signal gets very dirty. FB-DIMM attempts to solve this by placing a memory buffer directly on the DIMM itself. FB-DIMM aims to replace registered DIMMs, although the significant complexity of FBD will surely make it extremely expensive. Blackford/Greencreek also boast "enhanced storage controllers" (most likely SATA-II) and EM64T compatibility as well. Below we have a small cross section on various chipset names and the details we know about them. Blackford: Lindenhurst successor for the Dempsey dual core processor. EM64T. Dual Independent FSB. FB-DIMM Support. Scheduled for H1'06. Greencreek: Tumwater successor for the Dempsey dual core processor. Will utilize two sockets. EM64T. Dual Independent FSB. FB-DIMM Support. Scheduled for H1'06. Twin Castle: 4 Socket platform for Xeon MP. Utilizes a dedicated memory controller. Will probably support dual core processors with a future revision that supports multiple (or at least dual FSB). Scheduled for Q1'06. Alviso: Intel's 915P chipset for the next generation Centrino platform. Scheduled for Q1'05. Sonoma: Next generation Centrino platform including Alviso. Scheduled for Q1'05. Napa: Successor for Sonoma designed for dual core mobility processors. Adds DDR2-667. Scheduled for Q1'06. Lakeport: 915P successor, supports 1066FSB, more PCIe lanes and 667MHz DDR-2, AMT and more EIST revisions. Scheduled for Q2'05. Glenwood: 925XE successor. Supports everything Lakeport does but should also include ECC capability. Scheduled for launch with Lakeport. Tekoa: Intel's next generation gigabit Ethernet chipset for deployment with Intel core logic. Mukilteo: Uniprocessor workstation release for the Glenwood/Lakeport generation. Supports EM64T. ICH7: Next generation south bridge. Comes in several different versions including a "Raid" version, DH (Ditigal Home), DO (Digital Office), DE (Digital Enterprise) versions as well. Recently we have seen motherboard manufacturers disclose Pentium M platforms for the desktop. As more and more people start adopting higher clocked Pentium Ms over Celerons and Pentium 4s, don't be surprised to hear a lot more people giving attention to all things Centrino.
January 25, 2005 7:44 PM - The Consequence of Waking Up a Sleeping Giant Intel Roadmaps Inside.txt	 Anand and I got particularly antsy this month to get an Intel roadmap up - this month's changes look nothing short of spectacular. When we look at some of our other Intel roadmaps in retrospect, there are very few new developments outside of the Smithfield and Yonah announcements. This month looks completely different however; new processor announcements and details in Q1'06, chipset information and - for the first time in a very long time - most of Intel's processor roadmap has moved up, ahead of schedule. It takes time to realign a huge corporation such as Intel, and we can guess that the recent roadmaps have been the proverbial "calm before the storm". Intel doesn't normally make a lot of noise about major changes in the public as that can lead to reduced sales of current products. However, with AMD making some inroads against Intel and the lackluster retail reception of current 915/925 chipsets, that may not be as much of concern right now. Another possibility is that Intel was working feverishly on some new products and they are now confident enough of their release dates to add them to their roadmaps. The recent corporate shuffles in Intel must have made the company more aware of their consumer position or more lean to deal with it. Either way Intel is still the 800 pound gorilla; we don't need to look much past their last quarters earnings in relation to AMD's to verify that. If you thought Intel was aggressive before their regrouping last year this year ought to be impressive - to say the least. Chipsets First let's take a look at the chipset side of things. It was no surprise that the first generation Socket 775, DDR2, PCIe chipsets Alderwood and Grantsdale faced delays, production problems and poor saturation. Unfortunately such is the life of a first generation chipset. The second generation usually does better, and it looks like Lakeport and Glenwood should be no exception. Actually we no longer need to refer to the next generation DDR2 chipsets by their code names as Intel has cheerfully dubbed the two core logics as 945P and 955X respectively. Even though the launch is yet another month away i945 and i955 news will flood headlines in the upcoming weeks without question. Before we go under NDA for the launch, here are a few tidbits about 945 that we already know: First platforms for dual core support (915, 925 won't support dual core) Both platforms support 1066MHz FSB 945G will have Intel GMA 950 graphics Both platforms support 667MHz DDR2 955X will support 8GB of ECC DDR2 For much further details you will probably have to wait for the launch next month. We also have the upcoming launch of the 915PL and 915GL chipsets, but there's nothing exciting there. 915PL is the new budget 915P, and it drops HD Audio and DDR2 support, as well as limiting the chipset to 1 DIMM per channel with a maximum of 2GB of RAM. The 915GL is similar and falls roughly between the 915GV and 910GL in terms of features. DDR2 support is dropped, but both 533 and 800 FSB support remains. Performance enthusiasts will want to stay away from any of the GL/GV platforms, as usual. The latest iteration of the roadmap also paid a peculiar amount of attention on Vanderpool Technology or VT. Intel simply refers to this first generation of VT as "the first step in Intel's long term Virtualization roadmap." VT is supposed to take virtual machine applications and allow them to run simultaneously on the same hardware with the same processor - if we are to believe Intel's IDF keynote. Rather than setup two different machines for Linux and Windows, VT aims to unify them both in the same computer. However, the catch seems to be that the processor, chipset, BIOS and software all have to be aware of this process and it isn't a transparent, free upgrade. Vanderpool won't show up right away however. Intel claims the technology will start showing up in Itanium configurations by the second half of this year, with the mass production server launch date by Q1'06. This almost implies that we will not see any steps forward with this technology until the next processor launch for Xeon, but that's another story in itself. Desktop processors, starting with the Prescott 2M, will get the feature sometime in 2H'05.
February 7, 2005 3:09 PM - Rambus in Cell Processors and Intel's Dual Core Announcements.txt	 Rambus in Cell Rambus just proudly announced that their XDR memory interface would be used in the elusive Cell processor, being announced today at the International Solid State Circuits Society conference (ISSCC) in San Francisco. There's not much surprise that Rambus was selected to be involved with the Cell project, given their previous history with Sony and the Playstation 2, as well as their ability to deliver extremely high bandwidth memory devices on very low pincounts. Sony and Toshiba also signed a licensing agreement back at the start of 2003 to work on the Cell project. For years Rambus has been telling us that they've been working with GPU manufacturers on getting their high-bandwidth designs into future GPU architectures, and their design win with Sony may just be the key to getting XDR on PC graphics cards as well - especially since NVIDIA handled GPU design for the Playstation 3. The other interesting part of Rambus' announcement is that they are also responsible for the Cell processor interfaces - it's connection to the outside world (or to other Cell processors). Rambus has had a serial processor bus interface in their IP repertoire for quite some time now, called FlexIO. FlexIO is being used as the processor interface standard for Cell. FlexIO implements two very important features - what Rambus is calling FlexPhase, and DRSL (Differential Rambus Signaling Level). Normally when traces (wires on a PCB) are laid out, they have to be arranged in such a way that all of the traces going to the same chip have equivalent lengths. As buses get wider and board designs become more complex, trace routing becomes a very serious engineering problem. Because of the need to match trace lengths, you'll often see traces wrapped around themselves or laid out in artificially long paths to make sure that the signals they carry don't arrive sooner than they should. FlexPhase is a technology that allows for on-chip data and clock alignment for signals that don't all arrive at the same time, allowing for traces that aren't matched in length on the PCB. There is an added element of latency introduced by FlexPhase as the chip must handle all clock and data adjustments that are out of phase, but the idea is that what you lose in latency do to FlexPhase, you make up for it in design simplicity, potentially allowing for higher data rates. The next technology that FlexIO enables is DRSL with LVDS (Low Voltage Differential Signaling), which is a technology similar to what Intel uses in the Pentium 4 to reduce power consumption of their high-speed ALUs. We will actually explain the technology in greater detail later on this week in unrelated coverage, but the basic idea is as follows: normally the lower the voltage you run your interfaces at, the more difficult it becomes to detect an electrical "high" from an electrical "low." The reason being that it is quite easy to tell a 5V signal from a 0V signal, but telling a 0.9V signal from a 0V signal becomes much more difficult. DRSL instead takes the difference between two voltage lines with a very low voltage difference and uses that difference for signaling. By using low signal voltages, you can ensure that even though you may have a high speed bus, power consumption is kept to a minimum. The technology isn't quite sophisticated enough to make the transition to the mobile world, but with some additional circuitry to dynamically enable/disable interface pins it would be quite easy to apply FlexIO to mobile applications of the Cell architecture. The culmination of these two features is that FlexIO offers up to 8.0GHz data rates based off of a 400 - 800MHz interface clock. It is worth noting that such a high input clock frequency would inherently require some pretty sophisticated technologies to implement. Because Rambus is providing both the memory and processor I/O interfaces for Cell, it's not too surprising that 90% of the Cell's signaling pins are using Rambus interfaces. Looking at any modern day microprocessor, the biggest use of signaling pins goes to things like enabling multiprocessor support, a chipset interface and a memory interface (obviously varying based on the type of processor we're talking about) - so Rambus' statistics aren't too surprising. There are still some unanswered questions - mainly whether or not FlexIO will be used to interface with NVIDIA's graphics core (which we're guessing it will) and whether or not XDR will be used for the GPU's local memory (which we're also guessing it will). Given the negative impression of Rambus amongst PC enthusiasts, a successful implementation in PS3 and with NVIDIA's GPU could mean a virtual second chance for Rambus in the PC market.
February 8, 2005 4:00 PM - The Quest for More Processing Power Part One "Is the single core CPU doomed".txt	 Introduction "What you have seen is a public demonstration of 4 GHz silicon straight off our manufacturing line. We have positive indications to be able to take Netburst to the 10 GHz space." "While architectural enhancements are important, Intel intends to continue its lead in raw speed. Otellini demonstrated a new high-frequency mark for processors, running a Pentium 4 processor at 4.7 GHz." The first assertion was made at IDF Spring 2002, and the second press release was broadcasted after Fall IDF 2002. Fast forward to the beginning of 2004, and we read in the Prescott presentation: "2005-2010: the era of thread level parallelism and multi-core CPU technology. " What happened to "the 10 GHz space"? Fig 1. "2005-2010: the era of thread level parallelism and multi-core CPU technology ". The presentation of the new 6xx Prescott even states that Intel is now committed to " Adding value beyond GHz". This sounds like Intel is not interested in clock speeds anymore, let alone 10 GHz CPUs. Already, the hype is spreading: Dual core CPUs offer a much smoother computing experience; processing power will increase quickly from about 5 Gigaflops to 50 gigaflops and so on. It is almost like higher clock speeds and extracting more ILP (Instruction Level parallelism), which has been researched for decades now, are not important anymore. At the same time, we are hearing that "Netburst is dead, Tejas is cancelled and AMD's next-generation K9 project is pushed back." Designs built for high clock speeds and IPC (Instructions per Clock) are no longer highly regarded as heroes, but black sheep. They are held responsible for all the sins of the CPU world: exploding power dissipation, diminishing performance increases and exorbitant investments in state of the art fabs to produce these high clock speed chips. A Prescott or Athlon 64 CPU in your system is out of fashion. If you want to be trendy, get a quad core P-m, also known as Whitefield [2], made in India. To the point I am exaggerating, of course. A good friend of mine, Chris Rijk, said: "PR departments having no 'middle gears': they either hype something to great lengths, or not at all." Trying to understand what is really going on is the purpose of this article. We are going to take a critical look at what the future CPU architectures have to offer. Is the traditional approach of increasing IPC and clock speed to get better performance doomed? Does multi-core technology overcome the hurdles that were too high for the single-core CPUs? Are multi-core CPUs the best solution for all markets? Will multi-core CPUs make a difference in the desktop and workstation market? In this first instalment, we explore the problems that the current CPU architectures face. The intention is to evaluate whether the solution proposed by Intel and other manufactures is a long-term solution, one that really solves those problems. We will also investigate one CPU in particular, the Intel Prescott. So, basically there are 4 chapters in this article that will discuss: The problems that CPU architects face today: Wire Delay, Power and the Memory wall. Chapter 1 - The brakes on CPU power The reason why Intel and others propose dual core as a solution to these problems. Chapter 2 - Why single core CPUs are no longer "cool" Whether or not these problems can be solved without dual core. Chapter 3 - Containing the epidemic problems A case study of the Intel Prescott. Chapter 4 - The Pentium 4 crash landing Although Intel is undeniably the industry leader in the CPU market, this doesn't always mean that the solutions proposed are the right ones. For example, remember MMX, which was a technology that should have turned the (x86-based) PC into a multimedia monster. In hindsight, the critics were right. MMX was little more than a marketing stunt to make people upgrade. The first implementation of hyperthreading on Intel's Foster Xeon (Willamette Xeon) was turned off by default by all OEMs. And hyperpipelined CPUs with 30+ stages turned out to be an impressive, but pretty bad idea. In other words, not all hypes have turned out to be beneficial for the customer. Millions of customers are still waiting for the rich content on the Internet that is enabled by and runs so much faster on the Netburst architecture...
March 2, 2005 3:00 AM - IDF Spring 2005 Day 1 - Gelsinger Speaks nForce4 Intel and more.txt	 Our first day of IDF coverage has come to a close, and it has been quite eventful. From the barrage of multicore information, to the 64bit evangelism, down to the tech showcase, we've seen quite a bit from Intel. It is usually a pleasure to listen to Pat Gelsinger deliver a keynote, and today was no different. The Intel VP took us through new technologies from Intel that will be introduced over the next year or two. Included in the long list of topics covered are network acceleration, advanced management features, virtualization, and, of course, multicore. We got to see a 32-way Itanium machine recognize faces in a split second, as well as another demo of Intel's virtualization technology -- this time focused on security. On the floor of the tech showcase, we saw quite a few interesting booths. There is everything from fuel cell and battery research, to new a new server oriented chip from ATI (which we will talk more about later in the week). Also appearing is the NVIDIA nForce 4 SLI Intel Edition, brining into light the fruits of the NVIDIA/Intel cross-licensing agreement announced in November. We'll be talking more about these and other exciting exhibits over the course of IDF. We hope you enjoy the conclusion to our coverage of IDF Spring 2005 Day 1. It's time for us to collapse into dreamland and prepare for another big day tomorrow. Intel I/O Acceleration Technology: Improving Network Performance When looking at network servers, a huge percentage of the overall work being done has to do with protocol overhead processing. In order to get more of the work the users is interested in done, the industry has pushed forth network hardware that handles some of the overhead of TCP/IP processing. Network cards that include a TCP Offload Engine (TOE) are able to help reduce overhead processing done on the server's CPUs. Rather than simply rely on a NIC with a TOE, Intel has found that improving the rest of the platform is actually a better way of handling network overhead (especially when looking at many small packets). We didn't get an in depth technical look at IOAT at Pat's keynote today, but we do have a little general information. The slide we are looking at lists "CPU Improvements" and "OS Support/IA Tuned Software" as architectural enhancements behind IOAT. Making faster and more parallel processors will definitely help without needing any IOAT specific optimizations, and OS support of hardware features is usually a prerequisite for their use. Of course there could be more too these aspects of the technology, but we'll have to wait an see. The real meat likely comes in the chipset data accelerations, Data Movement Engines, and Edge Device Accelerations. We could envision enhancements to the NIC and chipset that allow for further removed overhead processing as well as optimized or prioritized movement of data through they system. We'll bring out more details as we get them.
March 17, 2005 12:05 AM - Understanding the Cell Microprocessor.txt	 Three very interesting things happened over the past couple of weeks here at AnandTech: Intels Spring IDF 2005 turned out to be a multi-core CPU festival, with Intel being even more open than ever before about future plans for their multi-core microprocessor architectures.  Intel has over 10 multi-core CPU designs in the works, and they made that very clear at IDF. At GDC 2005, AGEIA announced that they had developed a Physics Processing Unit (PPU) that could be used to enable extremely realistic physics and artificial intelligence models. Johan De Gelas went one step further in his quest for more processing power earlier this week to find that theres quite a lot of potential for multi-core CPUs in the gaming market, at the expense of increasing development times. So, what do these three things have in common?  The aggregate of the three basically summarize what weve come to know as the Cell microprocessor - a multi-core CPU, part of which is designed for parallel physics/AI processing for which it will be quite difficult to program. Cell, at a high level, isnt too difficult to understand; its how the designers got there that is most intriguing.  Its the design decisions and building blocks of Cell that well focus on here in this article, with an end goal of understanding why Cell was designed the way it was. A joint venture between IBM, Sony and Toshiba, the Cell microprocessor is the heart and soul of Sonys upcoming Playstation 3.  However, this time around, Sony and Toshiba are planning to use Cell (or parts of it) in everything from consumer electronics to servers and workstations.  If you dont already have the impression, publicly, Cell has been given some very high aspirations as a microprocessor, especially a non-x86 microprocessor.
April 26, 2005 4:00 AM - WinHEC 2005 - Keynote and Day 1.txt	 Introduction Landmarks events don't occur very often in life, and as advancements in technology come and go, the landmark evens come less frequently. Consider the area of transportation. For centuries, we relied on the brute force approach, while in the last 125 years we have gone from horse-drawn carriages to steam powered and gas powered engines, on to flight and eventually to space travel. The last major landmark is now more than 30 years in the past, and it's difficult to say what will happen next. We can see similar progress in the realm of computing, and narrowing things down a bit in the history of Microsoft. There have been a few major transitions over the past 25 or so years. 8-bit to 16-bit computing was a massive step forward, with address spaces increasing from 20-bit to 24-bit and a nearly complete rewrite of the entire Operating System. The transition from 16-bit to 32-bit came quite a bit later, and it happened in stages. First we got the hardware with the 386, and over the next five years we began to see software that utilized the added instructions and power that 32-bit computing provided. The benefits were all there, but it required quite a bit of effort to realize them, and it wasn't until the introduction of Windows NT and/or Windows 95 that we really saw the shift away from the old 16-bit model and into the new 32-bit world. You can see the historical perspective in the following slide. Keep in mind that while there are many more releases in the modern era (post 1993), many of the releases are incremental upgrades. 95 to 98 to Me were not major events, and many people skipped the last version. Similarly, NT 4.0 to 2000 to XP while all useful upgrades didn't exactly shake the foundations of computing. The next transition is now upon us. Of course we're talking about the official launch of Windows XP 64. It has taken a long time for us to really begin to reach the limits of 32-bit computing (at least on the desktop), and while the transition may not be as bumpy as the change from 16 to 32-bits, there will still be some differences and the accompanying transitional period. This year's WinHEC (Windows Hardware Engineering Conference) focused on the advancements that the new 64-bit OS will bring, as well as taking a look at other technologies that are due in the next couple of years. Gates characterized the early 16-bit days of Windows as an exploration into what was useful. For many people, the initial question was often "why bother with a GUI?" After the first decade of Windows, the question was no longer present, as most people had come to accept the GUI as a common sense solution. The second decade of Windows was characterized by increases in productivity and the change to 32-bit platforms. Gates suggested that the third decade of Windows and the shift to 64-bits will bring the greatest increases in productivity as well as the most competition that we have yet seen.
April 29, 2005 2:00 PM - WinHEC 2005 Coverage Wrap.txt	 Introduction And yet again we've saved the best for last. This article takes everything we've missed over the past week and rolls it together in to one giant Microsoft fest. We've got the expo floor, more Longhorn goodness, and some interesting tidbits about the next version of DirectX and why current hardware may not be able to provide "Gold" level Logo compatibility. Here's a look at a pretty cool Avalon demo we caught at the show: Click to enlarge For the quick summary, Avalon will be the new programming interface for desktop graphics and will allow easy hooks into 3D hardware. This will likely be similar to the way OS X uses OpenGL to accelerate their desktop. We'll get into a bit more detail in a moment. Beyond Avalon and graphics, there is plenty of information left to cover. We hope that you enjoy our final report on WinHEC 2005.
June 3, 2005 7:48 AM - No more mysteries Apple's G5 versus x86 Mac OS X versus Linux.txt	 Introduction It is a professional 64 bit Dream machine with supersonic speed! It is beautiful. It is about the ultimate user friendliness. It is about a lifestyle. It is a class apart. You guessed it - I am parroting Apples marketing. For some reason, the performance of Apples gorgeous machines has been wrapped in a shroud of mystery. Yes, you could find a benchmark here and there, with one benchmark showing that the PowerMac is just a mediocre PC while another shows it off as a supercomputer, the unchallenged king of the personal computer world. This article is written solely from the frustration that I could not get a clear picture on what the G5 and Mac OS X are capable of. So, be warned; this is not an all-round review. It is definitely the worst buyers guide that you can imagine. This article cares about speed, performance, and nothing else! No comments on how well designed the internals are, no elaborate discussions about user friendliness, out-of-the-box experience and other subjective subjects. But we think that you should have a decent insight to where the G5/Mac OS X combination positions itself when compared to the Intel & AMD world at the end of this article. If you like a less performance-obsessed article about Apple, OS X and the G5, you should definitely give Anands articles in the Mac section on AnandTech a read... In this article, you will find a pedal to the metal comparison of the latest Xeon DP 3.6 GHz (Irwindale), Opteron 250, Dual G5 2.5 GHz and Dual G5 2.7 GHz. Scope and focus Apples PowerMac is an alternative to the x86 PC, but we didnt bother testing it as a gaming machine. Firstly, you have to pay a big premium to get a fast video card  as a standard, you get the ATI Radeon 9650 - even on the high-end PowerMacs. Secondly, there are fewer games available on this platform than on the x86 PC. Thirdly, hardcore gamers are not the ones buying Apples, but rather, creative professionals. So, we focus on workstation and server applications, especially the open source ones ( MySQL, Apache) as Apple is touting heavily on how important their move to an open source foundation is. The 64 bit Apple Machines were running OS X Server 10.3 (Panther) and OS X Server 10.4.1 (Tiger), while our x86 machines were also running a 64 bit server version of a popular Open Source Operating Unix system: SUSE Linux SLES 9 (kernel 2.6.5). We also included an older Xeon 3.06 GHz ( Galatin, 1 MB L3) running SUSE SLES 8 (kernel 2.4.19) just for reference purposes. Some of the workstation tests were done on Windows XP SP2.
June 7, 2005 4:55 AM - Apple's Move to x86 More Questions Answered.txt	 Yesterday's WWDC keynote started like many other of Steve Jobs' keynotes, with an update on the iPod, Apple stores and Mac sales. But then came the turning point. After Steve was done talking about iPod, iTunes, OS X Tiger and the rest of Apple's product line, he said the magic words: "now, let's talk about transitions." And so it began, the minute that the word "transitions" was put up on the projection screen, you could hear the silence in the packed keynote hall. No one could believe it, despite how strong the rumors seemed to be, despite the slide that was staring everyone in the face, it hadn't even begun to sink in. Then came the "it's true" slide, and all possibilities of doubt, theories of Intel making PowerPC chips, Apple using Itanium, all of that went out the window. The two words said more than any two words ever have in the entire PC industry. From that one slide, we all knew that Apple would be switching to Intel processors, and it would be none other than Intel's x86 line of CPUs. Some of you have asked if the crowd boo'd - they did not. You could hear gasps and even feel the looks of disbelief on many faces, but there wasn't a single boo in the audience. What's interesting about the Apple crowd is that they really trust this guy, they truly believe in Steve Jobs and in Apple. We've never been to a keynote by any major industry leader and seen the same sort of support; many will call it a reality distortion field, but regardless of what causes it, it is still a lot of support. It is the type of support that a company needs to be able to complete an entire architectural change in less than two years; it is the type of support that only Apple seems to have. Whether that support will always be there, should Apple grow in size, remains to be seen, but it's there now and Apple needs it. Many are worried about the negative impact that yesterday's announcements will have on Apple's present-day sales. Apple will begin shipping Intel based Macs starting around the middle of 2006, so why would you ever want to be stuck with a PowerPC based Mac that you just bought less than a year prior? Apple does seem committed to offering PowerPC support for as long as it takes, meaning that OS X 10.5 (Leopard) will most likely be offered for both PPC and Intel based Macs, not to mention all of the applications that will definitely transition to a universal binary system. We honestly don't expect sales to suffer that much. Those who can wait will obviously do so until next year; those who cannot will still enjoy the same compatibility (most likely better at first) later on when the Intel Macs begin shipping. Apple is doing their best, however, to control excitement about the switch to Intel. Unlike previous major announcements, this one isn't plastered all over the front page of Apple.com. The G5 product pages still showcase how a 2.0GHz G5 is still significantly "faster" than a 3.6GHz Pentium 4; interestingly enough, the very CPU that Apple appears to be supplying in their development kits. Not putting much marketing muscle behind the switch makes sense at this point - the real work that's needed is on the developer side. That being the case, Apple also released their Universal Binary Programming Guidelines yesterday to aid developers in making sure that their applications work on both PPC and Intel based Macs. Based on Apple's guidelines, we can also conclude a few things about Apple's x86 implementation. The default compiler for Apple's x86 line will continue to be GCC. Another very blunt statement from the documentation is that "Macintosh computers using Intel microprocessors do not use Open Firmware." Rosetta, Apple's PPC to x86 binary translation software, also has some limitations: "Rosetta is designed to translate currently shipping applications that run on a PowerPC with a G3 processor and that are built for Mac OS X. Rosetta does not run the following: Applications built for Mac OS 8 or 9 Code written specifically for AltiVec Code that inserts preferences in the System Preferences pane Applications that require a G4 or G5 processor Applications that depend on one or more kernel extensions Kernel extensions Bundled Java applications or Java applications with JNI libraries that can't be translated." Apple has confirmed that their Intel based Macs should be able to run Windows, but you will not be able to run the x86 version of OS X on any hardware platform that you choose. Obviously with the switch to Intel's architecture, it is going to be much more difficult for Apple to prevent users from circumventing any protection that they may have implemented to run the x86 OS on their own hardware. Even if Apple's protection is cracked, you can expect driver support to be extremely limited for configurations outside of what Apple will be shipping.
June 14, 2005 4:33 PM - Intel On the Offensive Roadmap Details and Analysis.txt	  Anand and I had a discussion about a year ago that went something along the lines of "2005 is going to be sooooo boring for Intel and AMD". But then a week later, AMD and Intel both started announcing dual core stuff internally and 2005 has been one of the better processor slugfests that we have seen yet. After passing up the last few roadmaps due to various tradeshows and NDAs, we decided to plunge into the most recent Intel roadmaps and really take an in-depth look at what is coming up for the next year or so. We have talked extensively about Yonah - the dual core successor to Dothan - without much attention to its clock speed and other features. The latest roadmap has provided quite a bit of new information about several of the cores. We'll take a more in-depth look at Yonah, Presler and Cedar Mill in this update. Desktop Over the last month we were bombarded by dual core announcements and releases from AMD and Intel. Not only because dual core is an interesting and new concept to desktop computing, but also because after all of the hype, AMD and Intel were both able to deliver fairly good products (both AMD [RTPE: AMD Opteron Italy] and Intel [RTPE: "Pentium D"] are shipping already). Inevitably, there have been some pretty major changes to Intel's product naming, for starters: Pentium 4 Extreme Edition processors are now simply named Pentium Extreme Edition. Although these processors are nothing more than best of breed server and desktop cores, they will now have their own product category at least. (This started with the most recent dual core Pentium XE.) Pentium 4 now only refers to the single core Prescott and upcoming 65nm Cedar Mill cores. Pentium D will refer to Smithfield and 65nm Presler. All 5x1 processors are now 64-bit enabled. All 6x2 processors are now VT enabled. All 6x3 processors are 65nm Cedar Mill (which has VT enabled). Let's take a look at the upcoming dual core roadmap. Intel Dual Core Performance Desktop Lineup LGA775 Processor Speed L2 Cache FSB Launch Pentium D 950 3.40GHz 2x2MB 800MHz Q1'06 Pentium D 940 3.20GHz 2x2MB 800MHz Q1'06 Pentium D 930 3.00GHz 2x2MB 800MHz Q1'06 Pentium D 920 2.80GHz 2x2MB 800MHz Q1'06 Pentium D 840 3.20GHz 2x1MB 800MHz Now Pentium D 830 3.00GHz 2x1MB 800MHz Now Pentium D 820 2.80GHz 2x1MB 800MHz Now Aside from the additional cache, also note that the new 9xx series processors are the same 65nm Presler cores mentioned earlier. Presler will in fact be nothing but two Cedar Mill cores sharing the same package, just as Smithfield is only two Prescotts sharing the same core. Shrinking to 65nm frees up a considerable amount of space on the die, so moving to 2MB L2 cache seems logical. Also, all 9xx processors will have EIST enabled, whereas only the 840 and 830 Pentium D processors have EIST. That makes sense for 8xx, as EIST currently just drops the CPU speed to 2.8 GHz; we would hope that the 9xx models will improve the functionality of EIST. EM64T and XD are supported on all upcoming processors. The last final addition to the 9xx line is VT, or Vanderpool Technology. We have discussed Vanderpool in the past (first in 2003), but it seems like there is still a large amount of confusion regarding the technology. Vanderpool - and AMD's competing technology, "Pacifica" - enable a CPU to run multiple operating systems on a single CPU at the same time. The particular demos that we have seen show four operating systems running on a single machine independent of each other. Unfortunately, this requires BIOS support, Chipset support and Processor support, not to mention OS support. Intel claims that VT will roll out on the 6xx processor line before the end of the year, so we expect 945 and 955 to fully support VT as of now, since the next chipset revision from Intel won't come until Q2'06 (Broadwater). Intel Single Core Performance Desktop Lineup LGA775 Processor Speed L2 Cache FSB Launch Pentium 4 672 3.80GHz 2MB 800MHz Q4'05 Pentium 4 663 3.60GHz 2MB 800MHz Q1'06 Pentium 4 662 3.60GHz 2MB 800MHz Q4'05 Pentium 4 653 3.40GHz 2MB 800MHz Q1'06 Pentium 4 643 3.20GHz 2MB 800MHz Q1'06 Pentium 4 633 3.00GHz 2MB 800MHz Q1'06 Pentium 4 631 3.00GHz 2MB 800MHz Q1'06 Pentium 4 571 3.80GHz 1MB 800MHz Soon Pentium 4 561 3.60GHz 1MB 800MHz Soon Pentium 4 551 3.40GHz 1MB 800MHz Soon Pentium 4 541 3.20GHz 1MB 800MHz Soon Pentium 4 531 3.00GHz 1MB 800MHz Soon Pentium 4 521 2.80GHz 1MB 800MHz Soon With 13 new performance SKUs in the next year, Intel certainly has its work cut out. The new 5x1 processors are set to launch before the end of this month, although the only real advantage that they have over the existing 5x0 and 5x0J chips is EM64T support. The interesting processors (highlighted in bold) are the upcoming 65nm Cedar Mill cores. Roadmaps hint that Cedar Mill will top out at 95W TDP per core and 3.80GHz is the highest clock on the roadmap. The Pentium 4 551 has a TDP of 84W. The roadmap also reveals that the prices of the 6xx line will cut dramatically mid-August, and VT enabled Prescott 2M (Pentium 4 672 and Pentium 4 662) will retain a premium over their non-VT counterparts. As we mentioned earlier, two Cedar Mills running at lower clock speeds will compose a Presler dual core processor (Smithfield's replacement). HyperThreading, EIST, XD and EM64T are enabled on all of these new processors. Furthermore, all Cedar Mill chips will also receive VT technology, with the exception of the Pentium 4 631. The Pentium 4 631 and 633 are identical except the absence of VT in the 631. We're not entirely sure why the 631 is even being produced, other than to perhaps fill a niche market. All of the Cedar Mill cores clearly have VT support (as do the Prescott 2M cores), so why Intel would want to deactivate it in one model is anyone's guess. With both 65nm Presler and 65nm Cedar Mill, the interesting thing to note is how low the FSB clock speeds remain. We had almost expected next generation Pentium processors to ramp up to 1066FSB after the most recent Pentium 4 EE utilized 1066FSB. In fact, Intel relaunched its 925X motherboard lineup (with 925XE) to support the faster bus, and all current generation motherboards make a big deal of supporting 1066FSB. As our tests revealed, 1066FSB did virtually nothing to improve performance on the 3.46EE over 800FSB. The conservative FSB speeds on Presler and Cedar Mill indicate to us that Intel probably did nothing to shorten Prescott's pipeline, although there may be other reasons why they have decided to keep the bus speed low as well.
August 8, 2005 3:13 AM - Intel Q3'05 Roadmap Conroe Appears Speculation Ensues.txt	 Intel CPU Roadmap Update We have a small update to the Intel desktop roadmap, and not much has really changed. Everything from our last update remains the same, and it's basically business as usual. So what's new? We'll start off with the most interesting area in our view, the dual core units. As usual, we'll highlight the updates and additions. Intel Desktop Performance Roadmap Processor Core Name Clock Speed Socket Launch Date ??? Conroe ??? ??? 2H'06 Pentium D >= 950 Presler ??? LGA 775 Q2'06 Pentium D 950 Presler 3.4 2x2MB LGA 775 Q1'06 Pentium D 940 Presler 3.2 2x2MB LGA 775 Q1'06 Pentium D 930 Presler 3.0 2x2MB LGA 775 Q1'06 Pentium D 920 Presler 2.8 2x2MB LGA 775 Q1'06 We already covered the arrival of the Presler Pentium D cores last month (and Smithfield has been available for a few months). The chips will be dual core 65nm parts with EM64T, VT, EIST, and XD. If you're not familiar with those acronyms, here's the recap: EM64T adds 64-bit support and is the Intel equivalent of AMD64. XD provides some protection against buffer overflow attacks, again matching up to AMD's NX (No-eXecute) technology. VT stands for Virtualization Technology and provides hardware level support for running multiple OSes concurrently on a single computer. As we mentioned in our recent AMD roadmap update, it was only possible to run multiple OSes concurrenty in the past through such third party tools as VMware, and the hardware support should increase the performance quite a bit. As with the other technologies mentioned, VT has an AMD counterpart, dubbed Pacifica. The remaining technology warrants further explanation. EIST stands for Enhanced Intel Speedstep Technology, which allows the processors to throttle down to lower clock speeds and voltages when idle and thus conserve power. The version of EIST in the Presler core should be superior to that of the Smithfield core as it will also be available on the 2.8 GHz model. Current EIST on Pentium and Pentium D chips reduces the clock speed to 2.8 GHz, making it a useless feature for a chip that runs at 2.8 GHz by default. We don't have any specific details on the new EIST, but we hope that it will offer more benefits than a static clock speed and voltage reduction. Ideally, we'd like to see something like AMD's Cool and Quiet where all lower CPU multipliers are unlocked - that's what Intel has in their Pentium M chips as well. Overclockers in particular like to have such control; however, Intel may or may not offer that degree of tuning. We have one new entry for a potentially faster Presler model: 960 running at 3.6 GHz is the most probable candidate, although whether or not Intel decides to release such a chip will depend on a variety of factors. The more interesting addition is Conroe, which will use Intel's next generation architecture. Details on what Conroe will bring to the table are scarce, but we would imagine that all the previously mentioned technologies will be present. The major change is that Conroe will not use the NetBurst architecture that has been used in the Pentium 4 (and derivatives) line. For those that don't follow processors closely, here's a brief explanation on why this decision was made. The long pipeline of NetBurst has become a liability with clock speeds beyond 4 GHz producing a lot of heat. Increasing clock speeds have always created more heat, but now we're hitting the point where they begin to scale out of control. Rather than trying to find ways of dealing with 150W power levels (or perhaps even higher), Intel has designed a new architecture "from the ground up." Of course, they're not really starting over, as they'll be using elements of all of their previous designs, but Conroe will be enough of a change that it will have a new name.
August 23, 2005 2:37 PM - Fall IDF 2005 - Day 1 Pictures of Intel's Next Generation CPUs.txt	 We've spent the majority of this morning talking about the architecture and the direction behind Intel's new microprocessor architecture, but now we've got some pictures of the chip. Above you can see the 65nm Conroe chip, and below you can see it compared to the 65nm Pentium D:
August 24, 2005 1:39 PM - Fall IDF 2005 - Day 2 Pat Gelsinger  Reveals Intel's Server Plans.txt	 Pat Gelsinger, in his new role at the helm of Intel's Digital Enterprise group, kicked off today's keynote with a recap on Intel's Active Management Technology (AMT) and its uses. The most interesting AMT demonstration occurred earlier in the week and used AMT in combination with Intel's Virtualization Technology (VT) to provide a more secure Ethernet connection. The idea is to virtualize the Ethernet connection, providing your primary OS with a virtual NIC driver that all Ethernet traffic goes over. The actual NIC resides within its own virtual OS that can be secured more effectively, thus making the overall platform more secure. AMT wasn't the only thing recapped, we also saw a few Virtualization Technology demonstrations. Pat made the world's first demo of Microsoft's Virtual Server 2005 Enterprise Edition with VT technology. The demo was running on a Xeon system with 3 virtual machines: a NT4 legacy OS, a SQL server partition, and a Web Server partition. The next part of the demo was to migrate one of these virtual machines, the NT4 partition, to a separate physical system on the fly, without rebooting. Literally the process was no different than making a few clicks, and in a matter of moments the entire virtual OS partition was sent to another Xeon server on the network. The implications of this sort of ease of movement on the enterprise environment is huge; now if you have one overloaded server with a number of OS partitions and you migrate to a two server network, instead of having to completely reinstall all software when the new server is brought into the network, simply power up the new server and move the virtual OS that will reside on it. Pat also made the first demonstration of VMware ESX Server 2 with VT support. This demo was to showcase isolation between the virtual OS partitions, where Pat made one OS crash and noted that crash's independence from the other two running OSes. A blue screen But the windows Server 2003 OS remained unaffected As did the Linux partition VT will be in Pentium 4 66x and 67x platforms by the end of this year, and early next year in the Pentium D. Intel also mentioned that they will be migrating VT over time to including support for things like graphics.
September 12, 2005 12:00 AM - Intel Enterprise and Corporate Roadmap - Q3'05 Update.txt	 Intel Roadmap Introduction We've skipped a lot of the corporate and enterprise discussion for most of our recent Intel roadmaps. There's a good reason for the omission: very little had changed from the previous roadmaps. Every time we see a new Intel roadmap, the amount of information compressed into the 60 to 80 slides is simply staggering. Desktop, Mobile, Server, Enterprise, and Internet Appliance plans are included, and that's just the broad categories. Within each of those you find information on the chipsets, motherboards, and sometimes cases and other details. With few exceptions, there is almost always enough content for a couple articles, though you often need to dig a little deeper to find the interesting bits. We've got several pieces coming out of our latest Intel roadmap, with this article focusing primarily on the corporate sector. Besides the information density of their roadmaps, devoting large portions to the needs of businesses and corporations is one of the things that really sets Intel apart from AMD right now. Sure, AMD has the faster desktop parts, and there are many AMD adherents that feel that should be enough for anyone looking to purchase a new computer. That is, simply put, a distorted view of the world. For home users and enthusiasts, that attitude makes a lot of sense. As much as I like my AMD systems, though, if I were to start a business that had 25 or more computers, there's a good chance I'd be running Intel systems, and there's a good chance they would come from Dell (or some other large OEM). How can that be!? Am I not an enthusiast? That's near-blasphemy! Before any of you begin leveling charges of Intel favoritism, let me explain. If I were running a medium-sized or larger business, as much as I would like to have faster systems, taking the time to hand-build that many PCs is simply not making good use of time. Businesses normally want identical PCs (in order to simplify support), they want better warranties, they want one point of contact, and they want all of the systems assembled and delivered in a relatively short time frame. Higher performance that might enable employees to play games would actually be a bad thing, so sticking with an integrated graphics solution unless something faster is required would be a good idea. Finally, businesses don't want some fly-by-night shop to disappear after building the systems, leaving them to deal with problems on their own. Until AMD can get partners that focus on bringing out Corporate/Enterprise desktop systems - not just "Small or Medium Business" systems - with AMD processors, most companies won't consider switching. (Incidentally, we're actually testing some AMD SMB systems right now, and they've left a good impression. It's unfortunate that they aren't billed as Large Business systems, though.) Before we continue with the roadmap, we found some information at the end of the roadmap that can serve as a helpful glossary and/or technology primer. Intel throws around code names, acronyms, and technical jargon with wild abandon in their roadmaps, and we tend to follow suit. (We would guess that there are at least 50 code names listed in any given roadmap!) We'll use quite a few of these terms throughout many of our roadmap articles, so it's only fair to give you a quick cheatsheet. Intel Technology Glossary Feature Description Hyper-Threading Technology (HTT) Improves CPU utilization by processing two software threads on one core. 64-bit computing / Intel EM64T 64-bit computing and related instructions. Demand Based Switching (DBS) with EIST Enables server/workstation platform to go into reduced power state during periods of low use. PCI Express Next generation serial I/O technology offering scalable bandwidth up to 8 Gigabits/Second. DDR2 Memory Enables faster memory and increased memory bandwidth at lower power compared to DDR. Dual Core Improves processor throughput by increasing CPU resources. Intel I/O Acceleration Technology (I/OAT) Platform level I/O acceleration based on improvements in the Processor; MCH and LAN (ESB2 or NIC). FBD (Fully Buffered DIMM) Memory Next generation memory technology that uses DDR2 DRAMS in a serial point-to-point interconnect. Intel Active Management Technology (IAMT) System state-independent access to management functions and asset data. Intel Virtualization Technology (VT) Hardware enhancements to the processor enabling Improved virtualization solutions. Pellston Certain cache errors can be handled without restarting the system. Foxton Enables CPU to operate at increased frequency when CPU power is below specified max levels. You know things are complex when the simplified definitions of terms include cross references and even self-references. Virtualization Technology enables improved virtualization solutions? Who would have guessed? If you'd like additional explanations of what some of the terms mean, feel free to ask and we'll do our best to answer. Several of the above features that are summarized with a single sentence could easily be the topic of a lengthy article.
November 9, 2005 12:05 AM - Itanium - is there light at the end of the tunnel.txt	 Introduction On HP's website, these prophetic words are hidden, but can still be found: "EPIC is the old term for what is now known as the ItaniumTM processor family architecture, co-developed by HP and Intel. This design philosophy will one day replace RISC and CISC. It is a gateway into the 64-bit future but it still remains completely 32-bit compatible." These sentences showed how bullish HP and Intel were a few years ago about their new creation. But in 2005, the reality is somewhat different: "Dell will phase out its remaining computer based on Intel's Itanium microprocessor, in another sign of the waning interest in a chip that cost an estimated several billion dollars to develop." The Wall Street Journal, September 15th 2005. While it is hardly news that Dell, who doesn't believe in "big iron" anyway, is dropping Itanium, the rest of the sentences that the WSJ journalist wrote down seem to spell doom. As the Itanium market is still limited to HPC and (ultra) high-end servers, Microsoft is losing interest in the Itanium. IA 64 versions of Longhorn are low priority and only the future of the High Performance Computing version for Itanium seems certain. Visual Studio 2005 does not even support the Itanium platform. Dell and IBM are no longer interested. It is not going too well for Itanium. A few years ago, analysts predicted doom for Sun; not completely without reason, as the Intel Itanium 2 and IBM Power 5 clearly wipe the floor performance-wise with the UltraSparc CPUs. However, Sun's revenge is very sweet. Sun's newest Galaxy servers with up to 16 Opteron cores are a very competitive platform for the expensive Itanium servers. The Galaxy servers are well suited for clustering, so even in the market niche that requires more than 16 CPUs, are the Itanium based machines threatened by a cheaper alternative? Although the AMD Opteron targets a different market than the Intel Itanium, the Opteron market is expanding towards the high end, thanks to Sun, which in turn forces Intel to expand the feature set of the Xeon. Back in 2004 when EM64T was introduced, Intel pointed out that EM64T was only introduced on the Xeon DP. Intel probably expected the Opteron to be limited to workstations and entry level servers. However, the Opteron was very successful in the quad CPU market, and then it entered the 8-way and 16-way CPU market too. Intel had no choice then to counter attack and equip the Xeon MP with EMT64 and much higher clockspeeds than before the Opteron era, better RAS features and massive (for x86) L3 caches, up to 8MB big. Is Itanium nothing more than over an ambitious project that resulted in a CPU of titanic proportions? In this article, we try to answer the question of whether or not the EPIC CPU has a bright future ahead. To answer that question, we'll focus on the technical advantages and disadvantages of the chip, and look ahead to see if the architecture can still grow enough to outpace the competition. The End of a Generation Indeed, you might ask yourself, why do we even bother writing articles about Itanium? It is, after all, a massive CPU that ends up in very expensive machines, mostly huge database servers and HPC machines for scientific purposes; machines that most of us will never consider buying, not even for business purposes. Sturdy heatsinks for the Itanium And Itanium is in a lot of trouble. The newest generation, Montecito, was projected to arrive in 2004 when Intel first mentioned it. Then the PowerPoint slides mentioned 2005, and it became clear now that the newest Itanium wouldn't make its appearance before mid-2006. Many people feel that this is one of the many signs that the "Itanic" is sinking slowly, but steadily. Still, despite its rather dull reputation of a big iron CPU, and the flood of negative predictions, the EPIC has something fascinating. From a purely technical and academic point of view - completely ignoring the economical and business logic - there are some strong indications that time may well be on the side of the EPIC CPU despite all doom scenarios. That might sound insane right now, but allow me to explain this statement. As we stated in the "The Quest for More Processing Power, Part One", the CPU performance increase that we enjoyed during the golden era of the PC from 1981 to 2002 has hit the brakes, and is decreasing quickly. Back in the nineties, Intel and others introduced techniques like superscalar wide issue, out of order execution with big reorder buffers, speculative execution, integrated L2-caches, register renaming and dynamic branch prediction, which all increased the number of instructions that could be processed per cycle (IPC) on average. The AMD Athlon, which was introduced in 1999, and the Thunderbird incarnation in 2000 could be considered as the last representatives of this superscalar generation. Macro ops fusion, introduced in the Athlon, where two operations are travelling down the pipeline together until they get separated to get executed, was one of the last major tricks of this generation. Since then, only one improvement has really pushed performance per cycle forward: the on die memory controller (ODMC). Sure, there have been other "little tricks" that have steadily improved performance, but nothing spectacular. The CPU engineers still have a few tricks upon their sleeves that can improve IPC somewhat, but are limited to those that do not increase leakage and dynamic power loss. The focus is no longer on IPC or Instruction Level Parallelism (ILP). It is on Thread Level Parallelism (TLP). A good example of how the engineering focus has shifted is branch prediction. Quite a bit of resources have been spent on the Pentium 4's branch predictor, involving a whole team of Intel engineers. The result was that, on average, the Pentium 4 branch predictor is accurate 95-97% of the time, while the P6 BPU was accurate only 90% of the time. At Spring IDF 2005, when Anand, Derek and I asked Justin Rattner what Intel is doing in the field of even more advanced Branch prediction, he smiled. He told us that the current team who works on branch prediction is very small...around one person. There is no doubt that the whole industry has shifted their focus away from ramping clock speed and improving ILP to increasing performance by exploiting TLP. So, how does this affect Itanium and its EPIC foundation? Before we answer that, let us quickly review the basics behind the Itanium/EPIC philosophy.
December 29, 2005 10:03 AM - SUNrsquos UltraSparc T1 - the Next Generation Server CPUs.txt	 Introduction SUN's Ultrasparc T1, formerly known as Niagara, is much more than just a new UltraSparc. It is the harbinger of a new generation of CPUs, which focus almost solely on Thread Level Parallelism. No less than 32 independent parts of a different program (threads) can be "in flight" on the chip. It is SUN's first implementation of their Throughput computing philosophy, and compared to what we are used to in the AMD/Intel world, it is a pretty extreme architecture that focuses on network and server performance. Fig 1. The 2U SUN T2000 SUN's Ultrasparc T1 is little less than a revolution in the server world. How else would you describe a 72 W, 1.2 GHz chip that is almost 3 times (in SpecWeb2005) as fast as four Xeon cores at 2.8 GHz, which consume up to 300 W? Of course, there are a few snakes in the grass too, as T1 does not like every kind of server workload. In this article, we explore the architecture and the principles behind it, and how it performs. Stubborn Server applications The basic idea behind the UltraSparc T1 is that most modern superscalar Out-Of-Order CPUs may be excellent for games, digital content creation and scientific calculations, but they are not a good match for commercial server loads. These complex CPUs can decode up to 3 (Opteron) to 8 (Power 5) instructions in parallel, put in a buffer and try to issue them across 9 or more units. In theory, these CPUs can decode, issue, execute and retire up to 3 (Opteron) to 5 (IBM Power) instructions per clock cycle. They have huge buffers (up to 200 instructions) to keep many instructions in flight. Server workloads, however, cannot make good use of all this parallelism for several reasons. The main reason is that commercial server loads move a lot of data around and perform relatively little calculation on that data. Moving a lot of data around means that you may need a lot of accesses to the memory, which results in many cycles wasted while the CPU has to wait for the data to arrive. As many different users query different parts of the database, caching cannot be as efficient (low locality of reference). In the past years, memory latency has become worse as memory speed increased a lot slower than the speed of the CPU. Memory latency is even worse on MP (Multi-Processor) systems, and has risen from a few tens of CPU cycles to 200-400 clock cycles. The second reason is that many of the calculations performed on that data involve data dependent (read: hard to predict) branches, which makes it even harder to do a lot in parallel. You might counter these two problems by eliminating the branches through predication and incorporate very large caches. That is what the Itanium family does, but even the mighty Itanium is not capable of running those server loads at high speeds despite predication and gigantic caches. Below, you can see Intel's own numbers for CPU utilization on the 3 different workloads. Fig 2: Intel reporting the percentage of stalled cycles of different applications on the Itanium 2 family. Source:Intel. So, while the Floating point intensive applications such as scientific simulations and 3D rendering achieve relatively good parallelism on the superscalar CPUs, even the chip with the highest IPC stalled 85% of the time in Enterprise (i.e. server loads). The applications that can be found inside Spec Integer benchmark are still rather compute-intensive compared to server applications. Compression, FPGA Circuit Placement and Routing, Compiling and interpreting, and computer visualization are representatives of very CPU intensive integer loads. On average, the best desktop CPUs such as the Athlon 64 or Intel Dothan are capable of sustaining 0.8 to 1 instructions per clock cycle in this benchmark, while the Pentium 4 is around 0.5-0.7 IPC. Itanium is capable of a 1.3-1.5 IPC. That may sound like very low numbers, but let us compare SpecInt with typical server loads. In the table below, you find how the 4-way superscalar USIIIi does on the various benchmarks. Benchmark IPC SPECint 0.9 SPECjbb 0.5 SPECweb 0.3 TPC-C 0.2 Rather than focus on the absolute numbers, it is more important to note that web applications have 3 times less IPC than CPU intensive integer apps. OLTP databases (TPC-C) do even worse: the CPU sustains on average 0.2 instructions per clock pulse, or 4.5 less than SpecInt. These numbers are no different for the Opteron or Xeon. So despite Out of Order execution, nifty branch prediction schemes and big caches, commercial server loads utilize a very meagre 10 to 15% of the potential of modern CPUs. One possible solution is to focus on clock speed instead of trying to process as many instructions in parallel (ILP, instruction level parallelism). The long pipelines of such CPUs make the branch prediction problem worse, and the power consumption goes up exponentially as we discussed in a previous article about dynamic power and power leakage.
March 13, 2006 12:05 AM - Cebit 2006 High end Motherboards and Server News.txt	 Introduction While Anand and Derek were covering IDF and Intel Conroe Core benchmarking, and Wesley already previewing hardware in Taiwan, the European branch of AnandTech (that is, me) went to Cebit 2006 in Hannover. The main focus of this article is server and related IT subjects, but there were a few cool things that I couldnt resist, so they ended up in the same report. Cebit may not be the big launch event trade show anymore, but it is still by far the biggest ICT trade show in the world. Die Hannover Messe AG claims that almost 6,300 exhibitors from 70 countries were present at CeBIT 2006. A total of 300,000 square meters is filled with IT related stands, which welcome more than 500,000 visitors. Intel Intel had a very big stand on Cebit, and the showpiece of the booth was one of the Formula one cars of the Intel sponsored BMW Sauber F1 Team. Unless you were the Bundeskanzlerin of Germany, you couldnt enter the supercar. Most of the Intel news was already reported in our IDF report. Woodcrest, the server version of Conroe, will have a TDP of 80 Watt. No hyperthreading is available, which is a bit weird considering that the Core architecture is an architecture that depends on extracting high levels of Instruction Level parallelism. Database applications have a much lower IPC than SPEC CPU Integer 2000 and games. Hyperthreading could probably have done more for Woodcrest than it ever could for the Nocona and Irwindale Xeons. Woodcrest (Core architecture) and Dempsey (Netburst) are pin compatible and it should be possible to replace a Dempsey CPU with a Woodcrest CPU. However, most server manufactures to whom we spoke, are not so sure. Some said that a new PCB of their motherboard will be necessary, while others said it will be possible only on motherboards that have been validated for Woodcrest (not the current Bensley platforms). But who is going to buy Dempsey when Woodcrest is out? Virtualisation Many IT people are looking into virtualisation as a way to make better use of the available server power. However, our own tests in the lab show that software virtualisation is not really easy; it is our experience that guest OS crashes more often on virtualised servers than on real hardware servers. We definitely would advise against running anything mission critical on the current batch of software virtualisation solutions. And Microsofts virtual server is not even a real virtual layer - it runs on top of a Windows 2003 server OS. In a nutshell, we are rather sceptical about software virtualisation, and rather have set our hope on hardware virtualisation. Christian Anderka of Intel confirmed that the current Xeon Paxville and all Xeons (Dempsey, Woodcrest) on the Bensley platform feature hardware virtualisation of the CPU. The CPU gives the hypervisor or virtual layer a separate and higher privileged mode than the kernel mode in which the current OS run. That should increase the stability of virtualised servers greatly. Intel announced the next generation of Intel Virtualization Technology (Intel VT) for enterprise servers: Intel Virtualization for Directed I/O (Intel VT-d). VT-d adds support for hardware supported virtualisation of disks and others, but it is very unclear when this technology will be really ready. VT-d includes technology such as hardware DMA remapping and works also on the interrupts level. We will discuss virtualisation later in more detail, but it is clear that VT-D will also need to be supported in PCIe, the chipset components and peripherals. The quad core Clovertown, which is little more than two (dual core) Woodcrests in the same CPU package, will include this VT-d technology somewhere in 2007. Clovertown is socket-compatible with the Bensley (Dempsey, Woodcrest) platform and is slated to ship in early 2007. It is, however, also clear that the new instructions that are available in the new highly privileged mode for virtual layers will not be used to their full potential at first. We expect that it will take many releases of newer and improved virtual layers (hypervisors) before the new virtual layer instructions are used to their full potential.
April 24, 2006 2:00 AM - Eight Core Servers Opteron 880 (Egypt) vs Xeon MP 30 GHz (Paxville).txt	 AMD Dual Core Opteron vs. Intel Dual Core Xeon It's been a rough road for the Intel Xeon since AMD introduced the Opteron. AMD's market share is climbing, and in a complete role reversal of years gone by, AMD is reported to have 81.5% of the US retail PC market with Intel sitting at 18.5%. In the server space (lower volume, higher profit) AMD is now sitting at over 14%, up 5% from 2004. There is no question that the fight is fierce between the two processor giants: recently AMD hired one of Intel's high ranking Itanium designers. While the past few years haven't been Intel's best, it certainly doesn't mean that the future will carry on that trend. Intel's next generation products are in development, and on paper look quite promising. Performance per Watt is a key focus for Intel and is something the industry is in dire need of. As we alluded to in a previous article, power consumption is on everyone's mind. Opteron is not only the most scalable enterprise processor today, but it's also the most efficient. With a dual socket Opteron system using up to 40% less power than Intel's current Bensley systems, there is a lot of room for improvement. While it seems like AMD can sit back, relax and enjoy the growth curve, that wouldn't be a wise move. Intel is not going to take this for much longer - they can't. It's clear Intel is banking heavily on the last two quarters of the year, and the next generation products have to perform and win back mind and market share. Woodcrest and Kentsfield are no secret, and if they live up to expectations, the tables could turn. Intel is already competitive with Opteron performance in dual processor configurations with the soon to be released Bensley platform. With a reduction in power consumption Q4 of 2006 will be interesting to say the least.
June 7, 2006 12:00 PM - Intel Woodcrest AMD's Opteron and Sun's UltraSparc T1 Server CPU Shoot-out.txt	 Introduction In Q1 of 2006, AMD-based systems accounted for over $1 billion, or one sixth of the x86 server space. The Opteron grew from a 6% market share to 15% market share in the astonishingly short period of only one year. In four socket servers, the Opteron grabbed 48% of the US market, up from 23% last year. What's more, this is not a "US only" phenomenon: the Opteron has a firm grip on 36% of the worldwide four socket market. Bear in mind that less than 4 years ago, AMD was nothing more than a blip on the server CPU radar. Sun, which was getting strangled by the high volume Intel Xeon and the mighty Itanium, has also made a big comeback. An attractive UltraSparc IV+ with a fast, integrated L2 cache and massive L3 cache keeps the traditional Sparc buyers loyal, while the well-designed Galaxy Opteron based servers are pretty popular and the UltraSparc T1 "throughput CPU" attacks the midrange x86 market. It's high time for Intel to find a proper response, as the competition is taking the wind out of Intel's server CPU sails. What's the answer? A Xeon based on the Core architecture: Woodcrest. We compared the Core and K8 architectures just a month ago. Memory disambiguation, large OOO buffers and a large but low latency shared L2 cache should make the Core architecture more efficient in server related tasks than any other x86 CPU. This article compares a Woodcrest based Intel server with its closest rivals: AMD Opteron based servers such as the HP DL385 and MSI K2-102A2M and the UltraSparc T1 based Sun T2000 server.
November 8, 2006 6:01 PM - NVIDIA's GeForce 8800 (G80) GPUs Re-architected for DirectX 10.txt	 We always get very excited when we see a new GPU architecture come down the pipe from ATI or NVIDIA. For the past few years, we've really just been seeing reworked versions of old parts. NV40 evolved from NV30, G70 was just a step up from NV40, and the same is true with ATI as well. Fundamentally, not much has changed since the introduction of DX9 class hardware. But today, G80 ushers in a new class of GPU architecture that truly surpasses everything currently on the market. Changes like this only come along once every few years, so we will be sure to savor the joy that discovering a new architecture brings, and this one is big. These massive architecture updates generally coincide with the release of a new DirectX, and guess what we've got? Thus we begin today's review not with discussions of pixel shaders and transistors, but about DirectX and what it will mean for the next-generation of graphics hardware, including G80. DirectX 10 There has been quite a lot of talk about what DirectX 10 will bring to the table, and what we can expect from DX10 class hardware. Well, the hardware is finally here, but much like the situation we saw with the launch of ATI's Radeon 9700 Pro, the hardware precedes the new API. In the mean time, we can only look at our shiny new hardware as it performs under DX9. Of course, we will see full DX9 support, encompassing everything we've come to know and love about the current generation of hardware. Even though we won't get to see any of the new features of DX10 and Shader Model 4.0, the performance of G80 will shine through due to its unified shader model. This will allow developers to do more with SM3.0 and DX9 while we all wait for the transition to DX10. In the mean time we will absolutely be able to talk about what the latest installment of Microsoft's pervasive graphics API will bring to the table. More Efficient State and Object Management One of the major performance improvements we will see from DX10 is a reduction in overhead. Under DX9, state change and draw calls are made quite often and can generate so much overhead that the API becomes the limiting factor in performance. With DX10, we will see the addition of state objects which hold all of the state information for a given pipeline stage. There are 5 state objects in DX10: InputLayout (vertex buffer layout), Sampler, Rasterizer, DepthStencil, and Blend. These objects can quickly change all state information without multiple calls to set the state per attribute. Constant buffers have also been added to hold data for use in shader programs. Each shader program has access to 16 buffers of 4096 constants. Each buffer can be updated in one function call. This hugely reduces the overhead of managing a lot of input for shader programs to use. Similar to constant buffers, texture arrays are also available in order to allow for much more data to be stored for use with a shader program. 512 equally sized textures can be stored in a texture array, and each shader is allowed 128 texture arrays (as opposed to 16 textures in DX9). The combination of 8Kx8K texture sizes with all this texture storage space will offer a huge boost in texturing ability to DX10 based games and hardware. A new construct called a "view" is being introduced in DX10 which will allow resources to be used as more than one type of thing at the same time. For instance, a pixel shader could render vertex data to a texture, and then a vertex shader could use a view to interpret the data as vertex buffer. Views will basically give developers the ability to share resources between pipeline stages more easily. There is also an DrawAuto call which can redraw an object without having to go back out to the CPU. This combined with predicated rendering should cut down on the overhead and performance impact of large numbers of draw calls currently being used in DX9.
November 10, 2006 12:00 PM - Intel's newest Quad Xeon MP versus HP's DL585 Quad Opteron.txt	 Introduction There has been a relentless assault on the Server CPU market. How else could you describe Intel's impressive amount of new server CPU launches, and aggressive pricing during the past several months? At the end of May 2006, Intel released the dual core Xeon DP 5080 "Dempsey" at 3.73 GHz, still based on the same architecture as the latest Pentium 4 ("Presler"). As shown by the Dell DVD store benchmark, Dempsey made the performance gap with the best AMD Opterons smaller, but it still wasn't very competitive in the performance/Watt league. Only one month later, we reviewed a new Xeon DP 5160 based on the Intel's brand spanking new Core architecture, codenamed Woodcrest. With the exception of SSL Encryption and the MySQL database tests, the new Xeon DP simply annihilated the competition. Our most recent data shows that the Xeon 5160 outperforms the best Opteron (2.8 GHz) by 10% (MySQL) to 60% (LAMP), while presenting 33% lower TDP numbers (80W versus 119W, 65W versus 95W). AMD launched the new Socket F in August, but the current Opterons are not capable of extracting higher performance out of the faster DDR2 DIMMs, leaving AMD no other option than severely reducing the price of their server CPU flagship in the dual socket market. The Xeon MP machine on top of the HP DL585 in our rack...but can it really overpower the quad Opteron? But Intel wasn't satisfied. The lucrative 4 socket market was and is still dominated by the 8xx Opteron, which managed to capture up to 50% of the market share in only a few years. In September the 3.4 GHz Xeon MP 7140M, codename Tulsa, was born. With up to 16 MB of L3-cache, can the new Xeon MP stop AMD's Quad Opteron from grabbing even more market share? Or do we have to wait for Tigerton to arrive? Let us find out....
December 27, 2006 5:00 AM - Quad Core Intel Xeon 53xx Clovertown.txt	  Introduction The Xeon 5160 (a.k.a. Woodcrest) is probably the best server chip Intel has made in the past decade. It delivers high IPC, high (3GHz) clock speed and a surprising low 80W TDP for a dual core design. But Intel's engineers saw that the fastest Woodcrest could score twice for Intel, in a slightly different form... or should we say package. Lower the voltage of a 3GHz from about 1.21 V to slightly less than 1.1V and you can only achieve 2.33GHz. However by running at 2.33GHz and 1.1V, Intel was able to cut the TDP in half. Now place two of these low voltage Xeons in one package and you get a 2.33GHz quad core Xeon and an 80W TDP. This is the essence of the CPU that bears the codename "Clovertown". The quad core Xeon runs at less than 1.1V The really weird thing about all this is that Intel sells those two Xeon 5160 chips with lower voltage for the same price as one of the higher clocked chips. A 2.33GHz quad core Xeon DP costs the same as a 3GHz dual core Xeon DP ($851), and a 1.86GHz quad core will cost as much as a 2.66GHz dual core. In other words, Intel is willing to give one chip "for free" just to "ignite the quad core era". Intel is also trading in (theoretically almost 30%) single threaded performance for more multi-threaded processing power while keeping the power envelope the same. It is not as radical as Sun's T1, but the philosophy behind it is more or less the same. Is it a good bet? Well, being first to market with a quad core x86 product will probably make a big impact. It should be especially interesting for HPC and rendering applications. If you need the power of four cores in a blade server, the 65nm Clovertown or Xeon 53xx is the ideal CPU for the very crammed, "hard-to-cool" housing. The Xeon 53xx - with the appropriate BIOS update - should have drop-in capability in 5000(x) based chipset boards. The availability of Xeon 53xx generates some very interesting choices for the server buyer. With 8 cores on a dual socket system, can it replace the more expensive quad socket systems such as those based on the Opteron 8 series or the recently launched Xeon 71xx CPUs? Should you go for a high clocked dual core Xeon or a lower clocked quad core Xeon? The "standard" answer is that it depends on the application, but that answer is boring and hardly informative. In this article we will try to give an answer to these questions as they apply to Rendering, OpenSSL, Java, SAP and MySQL applications. Jason and Ross are busy with the SQL Server benchmarks, so you can expect even more benchmarks soon.
February 11, 2007 5:44 PM - The Era of Tera Intel Reveals more about 80-core CPU.txt	 With no Spring Intel Developer Forum happening this year in the US, we turn to the International Solid-State Circuits Conference (ISSCC) for an update on Intel's ongoing R&D projects. Normally we'd hear about these sorts of research projects on the final day of IDF, these days presented by Justin Rattner, but this year things are a bit different. The main topic at hand today is one of Intel's Tera-scale computing projects, but before we get to the chip in particular we should revisit the pieces of the puzzle that led us here to begin with. Recapping Tera-Problems At the Spring 2005 Intel Developer Forum, Justin Rattner outlined a very serious problem for multi-core chips of the future: memory bandwidth. We're already seeing these problems today, as x86 single, dual and quad core CPUs currently all have the same amount of memory bandwidth. The problem becomes even more severe when you have 8, 16, 32 and more cores on a single chip. The obvious solution to this problem is to use wider front side and memory buses that run at higher frequencies, but that solution is only temporary. Intel's slide above shows that a 6-channel memory controller would require approximately 1800 pins, and at that point you get into serious routing and packaging constraints. Simply widening the memory bus and relying on faster memory to keep up with the scaling of cores on CPUs isn't sufficient for the future of microprocessors. So what do you do when a CPU's access to data gets slower and more constrained? You introduce another level in the memory hierarchy of course. Each level of the memory hierarchy (register file, L1/L2/L3 cache, main memory, hard disk) is designed to help mask the latency of accessing data at the level immediately below it. The clear solution to keeping massively multi-core systems fed with data then is to simply put more memory on die, maybe an L4 cache perhaps? The issue you run into here is that CPU die space is expensive, and the amount of memory we'd need to keep tens of cores fed is more than a few megabytes of cache can provide. Instead of making the CPU die wider, Intel proposed to stack multiple die on top of each other. A CPU die, composed of many cores, would simply be one layer in a chip that has integrated DRAM or Flash or both. Since the per-die area doesn't increase, the number of defects don't go up per die.  Memory bandwidth improves tremendously, as your DRAM die can have an extremely wide bus to connect directly to your CPU cores. Latency is also much improved as the CPU doesn't have to leave the package to get data stored in any of the memory layers. Obviously there will still be a need for main memory, as Intel is currently estimating that a single layer could house 256MB of memory. With a handful of layers, and a reasonably wide external memory bus, keeping a CPU with tens of cores fed with data now enters the realm of possibility. A year and a half later, Rattner was back but this time he was tackling another aspect of the era of tera - bus bandwidth. Although 3D die stacking will help keep many cores on a single die fed with data, the CPU still needs to communicate with the outside world. FSB technology, especially from Intel, has remained relatively stagnant over the past several years. If we're talking about building CPUs with tens of cores, not only will they need tons of memory bandwidth but they'll also need a very fast connection to the outside world. Intel's research into Silicon Photonics has produced a functional hybrid silicon laser demonstrated at the Intel Developer Forum late last year. The idea is that optical buses can offer much better signaling speed and power efficiency than their electrical equivalents, resulting in the ideal bus for future massively multi-core CPUs.  Justin Rattner's keynotes talked about some of Intel's Tera-scale projects, with 3D die stacking delivering terabytes of bandwidth needed for the next decade of CPUs and silicon photonics enabling terabits of I/O for connecting these CPUs to the rest of the system. The final vector that Rattner spoke about, was delivering a teraflop of performance. The CPU Rattner spoke of was a custom design by Intel that featured 80 cores on a single die, and today Intel revealed a lot more about its Teraflop CPU, the architecture behind it and where it fits in with the future of Intel CPUs.
July 26, 2007 2:00 PM - AMD Still in the Game.txt	  It always seems that the worse a company does, the more information it divulges. We saw this behavior with Intel at the end of the Pentium 4 era, and we're definitely seeing it now with AMD. Enjoy it while it lasts, because it sure makes the industry a lot more exciting to talk about. Today's disclosures are many of the things we alluded to in our last article about AMD's future, what we called The Road Ahead. If you were waiting for us to fill in the blanks, this article will do just that. Barcelona Update Before getting into the new stuff, AMD gave us a brief update on Barcelona, whose launch is now hopefully less than a month away. Barcelona is the second CPU to plug into what AMD is calling its 2nd generation Opteron platform, it will have one more socket-compatible successor before the platform is retired: The first Barcelona processors available will be the HE (Energy Efficient) and standard performance CPUs, running at speeds of 2.0GHz or lower at launch: In Q4 of this year AMD will introduce the SE (High Performance) Barcelona parts, running at 2.3GHz and above. AMD is doing its best to sugar coat the low clock speed launch by saying that it's addressing the majority of the market at these clocks, but the fact of the matter is that AMD would be singing a different tune if it was able to achieve higher clock speeds at launch.
October 30, 2007 3:00 AM - The Business of Technology Intel Q3 2007.txt	 In our first Business of Technology article, we took a look at the audio industry's falling star, Creative Labs. As the first in a new series of articles we were especially interested in feedback from you, the reader, and were not disappointed. Thank you for that feedback, it hasn't fallen on deaf ears and has gone into improving this and future Business of Technology articles. With that out of the way, for our second article in this series we'll jump right into the fray and take a look at one of the titans of the computer industry: Intel. Intel has a long and rich history; so long in fact that the company existed before most of the AnandTech staff, a bit of a rarity in the computer industry. It's the long history of Intel and the ramifications of such that make it such an interesting and unusual company to fully grasp. While we take a look at a number of Intel products here at AnandTech, few people actually realize the scope of the company - from our reviews you would think that Intel is a processor company, a chipset company, a motherboard company, and that's it. In reality they're also a major player in wireless communications, graphics (albeit integrated), flash memory, networking, embedded chips, and more. Processors are their biggest source of revenue and profit by far, but the other groups nevertheless still play a part. As is customary with this series, today we'll take a look at Intel from both sides - business and technology - to understand what the company has been doing the past quarter, and where they are going in the near future. Intel is of course a very powerful technology company, but they also do a solid job of managing their overall business interests. As we'll see, this has made Intel a very solid company over the past quarter, but also one that will be fighting against growth concerns.
November 27, 2007 6:00 AM - AMD's 3rd generation Opteron versus Intel's 45nm Xeon a closer look.txt	 Introduction The new Xeon 45nm (or Xeon 54xx series) arrived in the lab weeks ago, so a server CPU benchmark update is overdue. However, there is another reason we decided to write this article. When AMD launched their newest quad-core chips, we could only give you a preview of its performance - not a full review. We did not fully understand many of the performance aspects of the new AMD architecture, so we decided to delve a little deeper. We will only focus on performance in this article, as our primary goal is to get an idea where Barcelona (AMD's quad-core), Harpertown (Intel 45nm quad-core Xeon), and Clovertown (Intel quad-core 65nm Xeon) stand. To do so, we performed a minimal profiling of each of our benchmarks and we used several new micro benchmarks that will tell you a lot more than some real world benchmarks can. If you like understanding the benchmarks out there a bit better, dig in. The new 45 Xeon The new 45nm Xeon 54xx series, aka "Harpertown", is still based on the Core architecture, but it has been tweaked a bit. We have already discussed those improvements in detail here and here, so we won't discuss them in detail again, but here's a quick overview: Faster 4-bit divider (Radix-16) instead of 2-bit divider Up to 1600MHz FSB Shared 6MB (24-way set associative) instead 4MB L2 cache (per dual-core die) Super Shuffle engine (For SSE instructions) Split Load Cache Enhancement SSE4 The Radix 16 divider is the most interesting of these improvements. Dividing involves a repetition of subtractions, tests "if-it-fits" and shifts. If you can do this with four bits at a time instead of two, this means that you can cut the number of these iterations required to get your result in half. The square root calculation is similar and also benefits from these improvements. While divisions and square roots are rather rare in common software, they have a very significant performance impact. Contrary to the more "popular" instructions, they are not pipelined and the latency of these instructions is high. For example, a floating-point multiply takes five cycles and the "Clovertown Core" architecture can finish one every two cycles thanks to pipelining. However, a floating-point division takes no less than 32 cycles and cannot be pipelined at all. Nanotechnology is here: a core with 820 million transistors and you can fit at least two of them in one coin. (Photo by Tjerk Ameel) Besides being an improved "Clovertown", the new Xeon is also a marvel of nanotechnology with no less than 410 million transistors on a die of only 107 mm. Two die make one quad-core Xeon 54xx. Considering that this CPU is close to behemoth CPUs like Itanium and Power 6 in SPECint performance, the new Intel is a formidable adversary for AMD's newest quad-core. There is more than the CPU of course. The new CPU works on the old "Bensley/5000P chipset" platform, though we were not able to get it running on our P5000PSL Intel motherboard despite the fact that we applied the BIOS update that came out early this month. There is also a new HPC/workstation platform for the Intel Xeon thanks to the Seaburg chipset, which features an improved snoop filter. Besides reducing the snoop traffic, the new chipset should also be able to extract more bandwidth out of the same FBDIMMs. The support for DDR2-800 FBDIMMs should bring another performance boost but our current test platform is only stable with DDR2-667 FBDIMMs.
February 20, 2008 12:00 AM - Intel's Silverthorne Unveiled Detailing Baby Centrino.txt	 In a matter of months Intel will be officially releasing what may end up as its furthest reaching microprocessor architecture of the next decade, yet hardly anyone is talking about it and it's rarely characterized properly. Let's get the names straight first. Silverthorne is the processor, it's Merom-x86 based (no SSE4). Poulsbo is the chipset. The combination of the two is referred to as the Menlow platform. I've often referred to Silverthorne as the processor Apple wanted to use in the iPhone but couldn't. In spirit there's truth in that statement, but practically it couldn't happen. Silverthorne won't be able to fit in something the size of an iPhone, it's not cool enough, it's not integrated enough and it's just not ready for that market. Intel believes it will be ready in about 3 years, I tend to agree. But Silverthorne is launching this year, soon in fact, and there are still many unanswered questions. We know from CES about the types of devices we'll see it used in. Intel calls these Mobile Internet Devices, or MIDs, they are small devices that can be used to browse the web, check email, use chat clients, play music, view photos, etc... These MIDs will either run Vista or Linux, the majority being Linux due to lower system requirements and cost.  They will all be significantly larger than an iPhone, but their intended use is also a bit more strenuous (in theory) than an iPhone. Since Silverthorne is an x86 microprocessor, these MIDs will be able to run virtually any x86 application. Intel is quick to point out that Silverthorne based MIDs will be able to run the "full Internet" including Flash, whereas the iPhone can't. These MIDs won't double as cell phones, so you'll still need to carry one of those with you but the idea is that a MID will be with you instead of your laptop. It'll be a lighter, more functional device, with a much longer battery life. Think UMPC but actually useful. The problems with UMPCs (Ultra Mobile PCs) were numerous; you need a custom UI for a device that small, most of the devices failed miserably in this regard. The Vista UI just doesn't scale well down to that small of a size. The physical interface with the devices was also flawed, Apple showed us how it needed to be done with the iPhone and unfortunately no UMPC maker or software vendor could adapt in time. Then there's the hardware side, these things used CPUs that were far too slow for what we were asking of them. Full versions of Windows running on single-core CPUs running barely faster than 1GHz just wasn't enough for the sort of responsiveness we were hoping for. UMPCs, much like Portable Media Centers before them, were simply ahead of their time. In many ways I expect MIDs to be the same, but there's one major difference: this time we're starting with hardware designed for the task at hand. Instead of scaling down a Core based processor, we're starting from scratch with a brand new mobile CPU called Silverthorne. If you remember the months before Centrino was launched, in many ways you're in for a repeat.
February 27, 2008 12:00 AM - A Second Shot Windows Vista SP1.txt	 Its hard to say the last year has been anything but rough for Microsofts Windows division. Although we found Windows Vista favorable upon its launch last year after watching it go through an usually drawn-out development process, such a sentiment hasnt been shared by Windows users as a whole. Windows XP proved to be every bit the competition for Vista that Microsoft could ever fear it would be, at the time as when Vista was having its own post-launch pains. It was a bad combination, making for a bad year for Microsofts efforts in pushing its first new desktop OS in 5 years. Microsoft was looking to make a solid case for why Vista is a worthwhile successor to XP in a market notorious for a resistance to change, and they failed to do this thanks to a failure in immature technology and an inability to get a consistent and convincing message out. In the year since then you could make the argument that Microsofts marketing efforts still havent improved, but you would be hard pressed to make the same argument about Vista itself. Since its release an unfortunately large number of bugs and quirks have been discovered in Vista, which has kept Microsoft busy patching them over the year, while to their chagrin many consumers sit on the side watching. To Microsofts credit theyve done a lot with Vista well before the first service pack, various patches including the reliability & compatibility packs released over the last year have solved many of the earliest complaints about Vista; it already performs better and is less quirky across the board now than when it launched. But it goes without saying that this hasnt been enough to solve all of Vistas problems, putting a lot of watchful eyes on Service Pack 1. There is a saying among software development circles that businesses as a whole wont touch a Microsoft product until the first service pack; they would prefer to wait until a product has been widely used and the biggest problems identified & solved. Its cold but effective logic that also puts a great deal of pressure on Microsoft. No matter how good (or bad) a product is, half of their customers wont bat an eye until theres a service pack, making the first such pack just as important as the product launch itself in some ways. Complicating matters further with the Vista launch in particular is that Microsoft has tied Windows Server 2008 to the Vista kernel; getting Windows Server 2008 out the door means any and all Vista problems that would hinder server operation need to be eliminated. The result is that Service Pack 1 is a big deal for Microsoft, they need to show consumers that they can fix what still ails the OS, they need to show businesses that its now ready for them to use, and they need to show server administrators that the core technology is so good that a reliable server can be built off of it. Furthermore, with the progression of technology in the last year the timing couldnt be any more critical. The 4GB address space barrier for 32bit x86 is finally beginning to rear its head with more average computer uses; RAM prices have nosedived with 8GB of RAM going for as little as $160, resulting in a wide and very real need for a 64-bit operating system (and XP64 being a poor fit for consumers). Meanwhile PC OEMs are finally warming up to the Extensible Firmware Interface (EFI) and are ready to start building systems with it, meaning they too must move beyond XP. Even governments are finding they need to move to Vista as of late, as new encryption standards come in to play which only Vista supports. The result of this is that many different groups have been watching SP1 far more intently than past service packs. With the final version of SP1 in hand, today well be looking at what Microsoft is bringing to the table with Vistas first service pack. With a combination of new features, bug fixes, and performance improvements, theres a great deal to this service pack that well be covering so lets get started.
March 17, 2008 3:00 AM - Hardware Virtualization the Nuts and Bolts.txt	 Introduction First dual-core in 2005, then quad-core in 2007: the multi-core snowball is rolling. The desktop market is still trying to find out how to wield all this power; meanwhile, the server market is eagerly awaiting the octal-cores in 2009. The difference is that the server market has a real killer application, hungry for all that CPU power: virtualization. While a lot has been written about the opportunities that virtualization brings (consolidation, hosting legacy applications, resource balancing, faster provisioning...), most publications about virtualization are rather vague about the "nuts and bolts". We talked to several hypervisor architects at VMWorld 2008. In this article, we'll delve a bit deeper as we look to understand the impact of virtualization on performance. Performance? Isn't that a non-issue? Modern virtualization solutions surely do not lose more than a few percent in performance, right? We'll show you that the answer is quite a bit different from what some of the sponsored white papers want you to believe. We'll begin today with a look at the basics of virtualization, and we will continue to explore the subject in future articles over the coming months. In this first article we discuss "hardware virtualization", i.e. the technology that makes it possible to offer several virtualized server such as VMware's ESX, Xen, and Windows 2008's Hyper-V. We recently provided an introduction to application virtualization using Thinstall, SoftGrid, and others software packages at our new IT portal, it.anandtech.com. These articles are all about quantifying the performance of virtualized servers and understanding virtualization technologies a bit better. Hardware or Machine Virtualization versus "Everyday" Virtualization Every one of us has already used virtualization in some degree. In fact, most of us wouldn't be very productive without the virtualization that a modern OS offers us. A "natively running" server or workstation with a modern OS already virtualizes quite a few resources: memory, disks, and CPUs for example. For example, while there may only be only 4GB RAM in a Windows 2003 server, each of the tens of running application is given the illusion that they can use the full 2GB (or 3GB) user-mode address space. There might only be three disks in a RAID-5 array available, but as you have created 10 volumes (or LUNs), it appears as if there are 10 disks in the machine. Although there might only be two CPUs in the server, you get the impression that five actively running applications are all working in parallel at full speed. So why do we install a hypervisor (or VMM) to make fully virtualized servers possible if we already have some degree of virtualization in our modern operating systems? Operating systems isolate the applications weakly by giving each process a well-defined memory space, separating data from instructions. At the same time, processes share the same files, may have access to some shared memory, and share the same OS configuration. In many situations, this kind of isolation was and is not sufficient. One process that takes up 100% of the CPU time may slow the other applications to snail speed for example, despite the fact that modern OSes use preemptive multitasking. In case of pure hardware virtualization, you will have completely separate virtual servers with their own OS (guest OS), and communication is only possible via a virtual network.
March 17, 2008 5:00 PM - Opening the Kimono Intel Details Nehalem and Tempts with Larrabee.txt	 Prior to its Intel Developer Forum, Intel is revealing a bit more detail on some new products coming down the pipeline - including Nehalem and Larrabee. IDF is going to be all about scaling Intel Architecture from milli watts all the way up to Peta FLOPs. This is clearly a reference to the Intel Atom on the milli watts side and new high end quad-core Itanium and Larrabee products on the Peta FLOPs side. First up is Intel's quad-core Itanium product, codenamed Tukwila: Tukwila is Intel's first chip with a full 2 billion transistors and should be shipping by the end of this year, with full systems available next year. Tukwila, like Nehalem, will support Intel's QuickPath Interconnect (QPI), a point-to-point interconnect similar to AMD's Hyper Transport. Also like Nehalem, Tukwila will feature an integrated memory controller - two in this case. Next up was the Intel Dunnington processor, a 45nm 6-core Xeon part based on Penryn cores: With 6 cores (3 dual-core pairs on a single die) and a massive 16MB shared L3 cache, Dunnington is close to Tukwila in transistor count, weighing in at a whopping 1.9 billion transistors. Dunnington is the first shipping product to come out of Intel's India Design Team based out of Bangalore, India. Architecturally there's not much difference between Dunnington and current Penryn based Xeon parts, you simply get more cores and a very large L3 cache shared by all of the cores. Designing such a beast isn't an insignificant effort, but it's made easier because when Intel designs a core it designs everything up to but not including the L2 cache. The L2 and everything external to it is referred to as the "uncore" and is made somewhat modular, although not quite to the same degree as Nehalem. Dunnington is the first step in Intel implementing a very Phenom-like cache architecture with its future Core products, culminating in Nehalem.
April 2, 2008 12:05 AM - Intel's Atom Architecture The Journey Begins.txt	 The Atom processor's architecture is not about being the fastest, but being good enough for the tasks at hand. A product like ASUS' EeePC would not have existed 5 years ago, the base level of system performance simply wasn't great enough. These days, there's still a need for faster systems but there's also room for systems that aren't pushing the envelope but are fast enough for what they need to do. The complexity of tasks like composing emails, web browsing and viewing documents is increasing, but not at the rate that CPU performance is. The fact that our hardware is so greatly outpacing the demands of some of our software leaves room for a new class of "good enough" hardware. So far we've seen a few companies, such as ASUS, take advantage of this trend but inevitably Intel would join the race. One of my favorite movies as a kid was Back to the Future. I loved the first two movies, and naturally as a kid into video games, cars and technology my favorite was the second movie. In Back to the Future II our hero, Marty McFly, journeys to the future to stop his future son from getting thrown in jail and ruining the family. While in the future he foolishly purchases a sports almanac and attempts to take it back in time with him. The idea being that armed with knowledge from the future, he could make better (in this case, more profitable) decisions in the past. I'll stop the analogy there because it ends up turning out horribly for Marty, but the last sentence sums up Intel's approach with the Atom processor. Imagine if Intel could go back and remake the original Pentium processor, with everything its engineers have learned in the past 15 years and build it on a very small, very cool 45nm manufacturing process. We've spent the past two decades worrying about building the fastest microprocessors, it turns out that now we're able to build some very impressive fast enough microprocessors. The chart below tells an important story:  Manufacturing Process Transistor Count Die Size Intel Pentium (P5) 0.80m 3.1M 294 mm^2 Intel Pentium Pro (P6) 0.50m 5.5M* 306 mm^2* Intel Pentium 4 0.18m 42M 217 mm^2 Intel Core 2 Duo 65nm (0.065m) 291M 143 mm^2 Intel Core 2 Duo (Penryn) 45 nm 410M 107 mm^2  In 1993, it took a great deal of work for Intel to cram 3.1 million transistors onto a near 300 mm^2 die to make the original Pentium processor. These days, Intel manufacturers millions of Core 2 Duo processors each made up of 410 million transistors (over 130 times the transistor count of the original Pentium) in an area around 1/3 the size. Intel isn't stopping with Core 2, Nehalem will offer even greater performance and push transistor counts even further. By the end of the decade we'll be looking at over a billion transistors in desktop microprocessors. What's interesting however isn't just what Intel can do to push the envelope on the high end, but rather what Intel can now do with simpler designs on the low end. What's possible today on 45nm... With a 294 mm^2 die size, Intel could not manufacture the original Pentium for use in low cost devices however, today things are a bit different. Intel doesn't manufacture chips on a gigantic 0.80m process, we're at the beginnings of a transition to 45nm. If left unchanged, Intel could make the original Pentium on its latest 45nm process with a die size of less than 3 mm^2. Things get even more interesting if you consider that Intel has learned quite a bit in the past 15 years since the debut of the original Pentium. Imagine what it could do with a relatively simple x86 architecture now.
June 16, 2008 9:00 AM - NVIDIA's 14 Billion Transistor GPU GT200 Arrives as the GeForce GTX 280  260.txt	 One-Point-Four-Billion. That's transistors folks. The chip is codenamed GT200 and it's the successor to NVIDIA's G80 and G92 families. Why the change in naming? The GT stands for "Graphics Tesla" and this is the second generation Graphics Tesla architecture, the first being the G80. The GT200 is launching today in two flavors, the die of the larger one is pictured below: Um...yeah...we're reviewing that today (this die comparison is to scale and that's a dual-core Penryn) Let's put aside all the important considerations for a moment and bask in the glow of sheer geekdom. Intel's Montecito processor (their dual core Itanium 2) weighs in at over 1.7 billion transistors, but the vast majority of this is L3 cache (over 1.5 billion transistors for 24MB of on die memory). In contrast, the vast majority of the transistors on NVIDIA's GT200 chip are used for compute power. Whether or not NVIDIA has used these transistors well is certainly the most important consideration for consumers, but there's no reason we can't take a second to be in awe of the sheer magnitude of the hardware. This chip is packed full of logic and it is huge. At most, 94 NVIDIA GT200 die can be produced on a single 300mm 65nm wafer from TSMC. On the other end of the spectrum, Intel can fit around 2500 45nm Atom processors on a 300mm wafer. If the number of transistors wasn't enough to turn this thing into a dinner plate sized bit of hardware, the fact that it's fabbed on a 65nm process definitely puts it over the top. Current CPUs are at 45nm and NVIDIA's major competitor in the GPU market, AMD, has been building 55nm graphics chips for over 7 months now. With so many transistors, choosing not to shrink their manufacturing process doesn't seem to make much sense to us. Smaller fab processes offer not only the potential for faster, cooler chips, but also significantly reduce the cost of the GPU itself. Because manufacturing costs are (after ramping production) on a per wafer basis, the more dies that can be packed onto a single waffer, the less each die costs. It is likely that NVIDIA didn't want to risk any possible delays arising from manufacturing process changes on this cycle, but that seems like a risk that would have been worth taking in this case. Instead, GT200 is the largest die TSMC has ever fabbed for production. Quite a dubious honor, and I wouldn't expect NVIDIA to really see this as something of which to be proud. Of course, that doesn't mean we can't be impressed with the sheer massiveness of the beast. And what do we get from all these transistors? Moving up from 690M transistors of the original G80 and 754M transistors in G92 to the 1.4B transistors of GT200 is not a small tweak. One of the major new features is the ability to processes double precision floating point data in hardware (there are 30 64-bit FP units in GT200). The size of the register file for each SP array has been doubled. The promised ability of an SP to process a MAD and a MUL at the same time has been enhanced to work in more cases (G80 was supposedly able to do this, but the number of cases where it worked as advertised were extremely limited). And the number of SPs has increased from 128 on G80 to 240 with GT200. To better understand what all this means, we'll take a closer look at the differences between G80 and GT200, but first, the cards. Clock Speeds, Pricing and HDMI Information NVIDIA is introducing two GT200 parts today along with slightly tweaked branding. The new brand is called the GeForce GTX, and the first two cards are the GTX 280 and GTX 260. Gallery: NVIDIA GeForce GTX 280 & 260 Here's the card, say hello: The GeForce GTX 280 It's got a little sibling too: The GeForce GTX 260 The GeForce GTX 280 features 240 streaming processors running at 1.3GHz. It's got a 512-bit memory interface to 1GB of GDDR3 running at 2.2GHz (1107MHz core clock). The rest of the GPU runs at 602MHz. GTX 280 GTX 260 9800 GX2 9800 GTX 8800 GTS 512 8800 GT Stream Processors 240 192 256 128 128 112 Texture Address / Filtering 80 / 80 64 / 64 128 / 128 64 / 64 56 / 56 56 / 56 ROPs 32 28 32 16 16 16 Core Clock 602MHz 576MHz 600MHz 675MHz 650MHz 600MHz Shader Clock 1296MHz 1242MHz 1500MHz 1690MHz 1625MHz 1500MHz Memory Clock 1107MHz 999MHz 1000MHz 1100MHz 970MHz 900MHz Memory Bus Width 512-bit 448-bit 256-bit x 2 256-bit 256-bit 256-bit Frame Buffer 1GB 896MB 1GB 512MB 512MB 512MB Transistor Count 1.4B 1.4B 1.5B 754M 754M 754M Manufacturing Process TSMC 65nm TSMC 65nm TSMC 65nm TSMC 65nm TSMC 65nm TSMC 65nm Price Point $650 $400 $500 $300 $280 $170-$230 You'll need a power supply that can deliver up to 236W for the card itself and you'll need both a 6-pin and an 8-pin PCIe power connector (the board won't work with two 6-pin connectors). The GeForce GTX 280 will retail for $650 with availability planned for June 17th. The GTX 260 brings the total core count down to 192 by disabling two Texture/Processing Clusters on the GPU (but still maintaining more cores than any single-GPU G80/G92 based card). The SPs run slightly slower on the GTX 260 (1242MHz vs. 1296MHz, NVIDIA uses a 27MHz crystal so that's why we get these funny clock frequencies), while the memory runs at approximately 2GHz (999MHz memory clock, 1998MHz data rate). The GTX 260 also loses one of the 8 64-bit memory controllers, bringing the total memory bus down to 448-bits and total memory size down to 896MB. The GTX 260 consumes a maximum of 183W and only needs two 6-pin power connectors. The GeForce GTX 260 will set you back $400 and should be available on June 26th. The GTX 280's shroud, though it takes some cues from the 9800 GX2, is better engineered to allow for heat expulsion through a larger vent next to the IO ports. The 9800 GX2 didn't have a very wide opening to the outside world from which to remove heat. This resulted in the 9800 GX2 disproportionately increasing the heat of the system. Since we run our systems without cases we don't see problems like this as soon as our readers do, and we would like to thank you for pointing this out to us. Both the GTX 260 and 280 have two dual-link DVI outputs, which can be converted to HDMI using an adapter if you'd like. You can pass SPDIF using a connector at the top of the card, which will then be passed to the HDMI output when using the DVI-HDMI adapter. For the HTPC enthusiasts out there, the GTX 280 and 260 can output the following audio formats over HDMI: 2-channel LPCM 6-channel DD bitstream 6-channel DTS bitstream Unfortunately there's no support for 8-channel LPCM or bitstreaming of Dolby TrueHD or DTS HD-MA.
August 19, 2008 12:00 AM - Intel Developer Forum 2008 - Live Nehalem Gelsinger Keynote Coverage.txt	 Below you'll find live coverage, updated in real time, from the second and third keynotes at IDF. Refresh for the latest updates. All times are in EDT, the newest posts are at the top. 5:30PM - Time to go! 30 minutes until 3 hours of nothing but Nehalem, check back soon for more updates. 5:25PM - We're winding down here, Dadi just showed off some MIDs based on Menlow. Hopefully they're better than the early ones I played with back at CES. 5:22PM - ~1B IA processors shipping per year in the next decade. That makes much more sense. The plan is ~200M Atoms shipping per year by 2012. That's as many desktop CPUs as Intel is shipping today. Lofty goals Mr. Perlmutter, but we'd need a much better platform than what we have now - this is where Moorestown comes into play. 5:19PM - WiMAX notebooks from Toshiba, Lenovo, ASUS, Dell and Acer. Now it's time to talk about Netbooks, Mobile Internet Devices (MIDs) and Atom. 5:17PM - WiMAX demo: WiMAX video conference between John Sw, CTO of Clearwire Corporation, and all of us here in San Francisco. The video is smooth, but quality isn't all that great. 15Mbps WiMAX while driving 35mph. Nice. 5:16PM - WiMAX should be cheap, affordable and global. Ahem, no more per-SMS fees? I'd like to see the rest of the traditional cell phone providers jump ship to WiMAX... 5:14PM - Seven OEMs will be shipping notebooks with Intel WiMAX in Q4 '08. First WiMAX network in Baltimore, MD launching in the coming months. 5:13PM - Time to talk about wireless broadband. More WiMAX? Unfortunately, outside of IDF, WiMAX doesn't appear to be doing too well. Many of today's wireless demos were done using WiMAX at IDF. 5:11PM - A disable request is sent to the notebook wirelessly, since it has an integrated GPS the notebook can be tracked that way. You can also control the webcam remotely to get a look at who is using the notebook. Oh surprise, it's Pat Gelsinger. That sneaky Pat. 5:10PM - Anti-theft notebook demo; a ninja just stole Dadi's notebook with tons of secret roadmaps on it. Oh noes, what will he do? 5:07PM - Intel officially announced its SFF Centrino 2 chips. These are the smaller package CPUs and chipsets that were used in the MacBook Air, but now in Centrino 2 versions. I talked about them in our Centrino 2 launch article. 4:58PM - Calpella will be Nehalem for notebooks. This will happen next year with two and four cores. 4:57PM - What would a mobile demo be without also showing off the new SSDs we just talked about. Read our coverage from earlier today to find out more. 80GB MLC drives in September, 160GB drives in Q4. Intel is saying that these things are going to be fast. 4:55PM - They tried really hard to show off a good gaming experience on a notebook, but the gameplay of Assassin's Creed was choppy and horrible. They need a little work on picking apps for demos apparently. 4:54PM - Intel is also showing an HP notebook that can play a full 2 hour Blu-ray movie on one battery charge. 4:52PM - Intel just demonstrated switchable graphics on its Centrino 2 platform. By flipping a physical switch the demo system had its NVIDIA GPU turned off and switched to the integrated GM45 graphics. The actual switch took several seconds, much longer than the Hybrid Power tests I ran on the NVIDIA 780a chipset not too long ago. 4:45PM - Ok I'll stop counting chips, Dadi just announced Intel's first mobile Quad Core processor. It will run at 2.4GHz and be used for desktop replacement notebooks. Shipping this year, notebooks below: 4:44PM - Dadi put up some more numbers, ~400M notebook CPUs, ~200M desktop CPUs by 2012. Only 400M to go. 4:43PM - Ah, this makes more sense. ~200M desktop CPUs, ~200M notebook CPUs in 2008. If Atom + enterprise could make up the remaining 600M I can see this working. 4:42PM - Dadi: "1 billion IA (Intel Architecture) devices targeted by the end of the decade"? No way. That's a ton of Atom processors sold. 4:40PM - Dadi Perlmutter is on stage now, time to talk about mobility. Wow, the Banias/Centrino project was first started 10 years ago. I feel old. 4:37PM - Larry adapted a line from the major general's song from The Pirates of Penzance for Larrabee. I'll spare you the horror that was the butchering of lyrics. 4:35PM - Larry is giving a 50 second run down of Larrabee. Go read our article instead :-) 4:33PM - More about Larrabee. No need to sort for transparency demo. Sorry, that's as well as I can describe it. Look at the picture. 4:32PM - Dr. Larry Seiler is on stage now talking about Larrabee, he's going over some of the same stuff we've already seen in our Larrabee article. 4:30PM - Havendale motherboard pictured below. Also, Lost Planet performance of Nehalem is around 50% faster than Yorkfield at the same clock speed (a big hunk of this is due to Hyper Threading): 4:29PM - Okay ... Apparently Nehalem is Michael Phelps. I think I've heard it all now ... 4:26PM - It's time for Lynnfield and Havendale - mainstream Nehalem. 4:25PM - Oooh, the Nehalem server demos were done with DDR3 MetaRAM. This is Fred Weber's (old CTO of AMD) new company that builds high-density memory at lower cost. 4:22PM - Even though virtualization on the desktop hasn't taken off, it is still really important. Nehalem will refine Virtualization. An issue is in multiple high speed IO devices (like 10 GbE) end up incurring a lot of CPU overhead. Intel's VT-d (the d is for direct path) includes hardware resources for allowing virtualization software to share physical devices directly, reducing CPU overhead. 4:18PM - NASA is going to use petaflops of Nehalem processors to do cool things. Climate modeling and stuff. 4:15PM - Turbo mode will be available in i7. This will allow the CPU to actually increase single threaded performance by increasing frequency of cores dynamically. When not all cores are in use, there will be more headroom (meaning single threaded apps will see a benefit from having cores not in use). Note that this is a different turbo mode from what we've had in Penryn, Intel is actually promising a performance boost here. 4:13PM - Power management on Nehalem will be better than today. Integrated Power Gate technology will take voltage down to zero rather than just decreasing power. Actually building a perfect silicon "power switch" was tough and required some new fabrication technology. The tech is pretty cool, each core can be shut off independently without resorting to multiple power planes. Each core gets its own PLL like Phenom, but there is some clever stuff going on here to make the implementation a bit more elegant. 4:11PM - First showing of 8 core Nehalem ES wafer. This will be the server part. 4:10PM - Dunnington (45nm Xeon) will be the last Penryn based core delivered. After this ships (sometime soon) they'll be moving on to other things ... But the cool thing is that this one hits 1 million + on the TPC C benchmark. 4:09PM - It's time to talk about Nehalem! But first a recap of Tukwila (new Itanium) and Dunnington (6-core workstation/server Penryn). 4:05PM - There's a BMW on stage and they're talking about in vehicle intertainment (or is it infotainment?) or IVI. Yes, this is a very lame naming idea. But combining more functionality into the car is cool. Imagine a digital tour guide talking about the things you're driving by. 4:04PM - Pat played a video of OpenPeak running Atom, basically an iPhone-like IP phone for your home. It looks pretty sweet, like a large iPhone with tons of applications but more home focused rather than mobile focused. 3:59PM - Looking at getting everything on the internet (15 billion devices by 2015) is going to take a lot. Embedded processing is going to be more and more and a mix of low end CPUs and targeted designs like Atom and SoCs. Intel thinks they have the solution 3:57PM - Talking about embedded applications. Over 700 designs based around Atom since its inception. Too bad none of these are smart phones; we'll have to wait for Menlow for that. 3:54PM - Pat is on stage, he promised to talk about Nehalem. Awesome.
November 23, 2009 12:00 AM - AMD's 20102011 Roadmap from the IT Professionals Perspective.txt	 At its recent financial analyst day, AMD disclosed processor and platform roadmaps for 2010 and 2011. As the target public consisted mainly of financial analysts, the presentations focused more on AMDs strategy and competitiveness than on technical accuracy. We had a conference call with John Fruehe and Phil Hughes of AMD and we tried to find out what the new server CPU roadmap means for our readers, the IT professionals who actually configure and buy these servers. Compared to the mobile and desktop market, AMD is doing relatively well in the server and HPC market. The early delivery of the six-core Opteron (codenamed Istanbul) enabled Cray to build the fastest supercomputer in the world (at least for Q4 2009). It's called the the Cray XT5-HE Jaguar with 224162 cores, good for almost 1.76 million GFlops. The Opteron EE made heads turn in the low power cloud computing market, and the six-core Opteron is a good price/performance alternative in the rest of the server world. And last but not least, the 4-socket 84xx Opterons are the unchallenged champions in the quad socket world. Nevertheless, AMDs position in the server and HPC market is seriously threatened. An impressive 95 out of the top 500 supercomputers contain Intel's "Nehalem-EP" Xeon 5500 processors. Intels star has been rising fast in the HPC market since the introduction of the Intel Xeon 5500. Intels Nehalem EX is almost ready to attack the quad socket market. And there's more. AMD created a very cool niche market with the 40W ACP (60W TDP) Opteron EE. Large power limited datacenters bought these CPUs in quantities of a few (and more!) thousands at once. Just a few months ago, Intel also introduced a 45 Watt Xeon L3426 at 1.86 GHz based on their Lynfield core (LGA1156 socket). Considering that AMDs ACP numbers are rather optimistic and Intels TDPs are rather pessimistic, the 8-thread quadcore 1.86 GHz L3426 ($284) makes the six-core 1.8 GHz Opteron 2419EE look expensive ($989). The former can push its clock up to 3.2 GHz under single threaded loads, and is thus a really interesting option if your application has a significant part of non-parallel code. So far AMD has countered Intels higher per core performance with 50% more cores. Indeed, the six-core Opteron can keep up with the Xeon 5500 in quite a few applications. But Intel is readying a slightly improved six-core version of the Xeon 5500 series called Westmere-EP in the first half of 2010. Being a 32 nm high-K dielectric CPU, the six-core Westmere-EP wil offer about the same power consumption with six-cores under load as the quadcore Xeon 5500 (Nehalem EP). At idle, Westmere-EP will consume less (14 to 22% less leakage). Westmere-EPs architecture is identical to that of the Nehalem EP, with the exception of a 50% larger L3 cache (12 instead of 8 MB) and support for special AES instructions. AMD's Answer It was hardly noticeable but AMD made a historic step forward in September 2009 with the introduction of its own server chipsets. For the first time, AMD is a real server platform supplier, in control of both the CPU and chipset. The previous AMD server platform was mostly based on NVIDIA's nForce 3600 Pro. The nForce 3600 gave some system administrators quite a few headaches, especially in combination with VMwares ESX. VMwares ESX installed flawlessly on all Intel platforms we have tried so far, but it was unpredictable whether or not an nForce board would work with ESX. Of course, the added value of a tier one OEM is that they sort these things out and offer you a driver + hardware platform that is certified for ESX and others. So you could say that this was a non-issue for HP, SUN and Dell buyers (I have hardly seen any IBM Opteron based servers in the wild). Still, it is good to see that AMD is now completely responsible and in charge of its own server platform. Below you find the specs of AMDs northbridge server chipsets:   And next the southbridge chip.    At the moment, the impact of the Fiorano or SR56xx chipsets is negligible. Most server vendors are preparing the servers based on the C32 socket and G34 socket and dont feel like investing in the socket-F server platform which is at the end of its long road. Only Tyan and Supermicron, which focus mostly on the HPC market, offer servers based on the AMD SR5690 chipset right now.
May 30, 2008 12:00 AM - Interesting Information From NVIDIA's 2008 Stockholders Letter.txt	 Earlier today we got our hands on NVIDIA's 2008 stockholders letter, and have been thumbing over it. For those of you that aren't familiar with annual stockholder letters, every year ahead of their annual stockholders meeting and voting, NVIDIA sends out a package containing a voting sheet along with a fairly thick book which in turn contains their proxy statement for issues to be voted upon, their 10-K statement detailing the previous year's financials, and a letter to the stockholders written in plain English that summarizes what the company has been doing in the past year and will be doing in the next. All of these elements are interesting for different reasons, but for now we're going to focus on the letter to the stockholders, as that's where the buried treasure is today.  While the rest of the document is printed on standard newsprint, the letter itself is printed on heavy glossy paper so that NVIDIA can use pictures (and lots of them at that). Not very far in to the letter, we came upon a most interesting picture.   To date, NVIDIA's biggest GPU is G92, with around 750 million transistors. The 1.2 billion figure listed here would be wrong for a current GPU, but not for a next-generation GPU. Now NVIDIA's next-generation GPU is just about the worst kept secret in the industry, and to that extent there's been plenty of speculation on the features and size of such a GPU. 1.2 billion transistors is consistent (if not a tad higher) than what rumors have been floating about so the size doesn't come as much of a surprise, but we are surprised that NVIDIA would publish such a fact for a product that doesn't exist yet. None the less, of all the rumors we can at least lay those about the transistor count to rest: NVIDIA's next generation high-end GPU will have around 1.2 billion transistors, and that's right from the horse's mouth.  And just to put things in comparison, even Intel only has a single mainstream chip so far that is that big - the P4 based Tulsa Xeon with 16MB of on-die L3 cache, which came out to 1.3 billion. Beyond that, we'd have to start looking at exotic chips for supercomputers such as the latest Itanium (1.7 billion) to find something similarly large or larger.  We're still not sure what this die shot is of however. We don't have a die shot of the G80 or G92, which makes it hard to determine what this is a shot of. NVIDIA and AMD are both very protective of their parts in this regard, so it seems unlikely that the die shot matches the caption. If we had to take a bet, we'd guess that this is a G80 die shot although we certainly wouldn't complain if it was a shot of NVIDIA's next generation GPU.  Moving on, we have NVIDIA's financials for the quarter. NVIDIA's FY 2008 (February 2007 through February 2008) was positively monstrous, revenue was up nearly 33% and earnings were up 72%. If you're wondering why Jen-sun is driving the company so hard in a fight against Intel, there's a good part of the reason; Intel may be many times larger than NVIDIA, but NVIDIA has ample ammunition to stage a fight with.   Speaking of financials, we'll also touch quickly on their revenue breakdown for the year. Consumer GPU: 61%, Professional GPU: 14%, Chipset: 17%, Mobile/Console: 6%. Most of their growth for the year was in consumer GPUs followed by professional GPUs, while chipset and mobile/console sales were nearly flat by comparison.  Finally, there are legal matters. Back in November of 2006, the United States Department of Justice began an investigation in to illegal price fixing and other anti-competitive actions between AMD and NVIDIA. So far that investigation has not resulted in any charges, and it's not immediately looking like it ever will. However civil suits are another matter, since the start of the DOJ investigation some 55 civil suits have been filed against NVIDIA by consumers, alleging the same things as the DOJ investigation.  These have since been consolidated in to two suits, one for direct buyers of GPU products, and another for indirect purchases (i.e. those who purchased pre-built computers with NVIDIA GPUs). While we have known that some civil suits had been filed against NVIDIA, we hadn't previously seen a count; 55 was more than we were expecting however. It does not appear that these will end up being class action suits, the date for filing such a motion has passed and we are not aware of any such motion being filed or approved.  Discovery is ongoing, the trial will start on January 12th of 2009.  Update: If you're interested in seeing the investor materials, NVIDIA has the entire document posted to their site as a PDF. You can find it here.
May 27, 2009 12:00 AM - Intel talking about the 16-thread RISC killer .txt	 Take two Nehalem dies, turn them  90 degrees, add a lot of system interface logic and 8 MB extra of L3-cache and you get - very oversimplified - the impressive Nehalem EX, alias "Beckton". The new Xeon MP is an impressive monster, just like it's predecessor Dunnington. Dunnington consisted of 1.9 Billion transistors, the Xeon MP based on the "Nehalem" architecture will feature up to 2.3 Billion transistors.   Those 2.3 Bilion transistors are needed for Up to eight cores, 16 threads thanks to SMT Up to 24MB of shared L3 cache four QuickPath links four memory channels which support for up to 16 memory modules per socket Intel calls the chips to drive the DDR-3 modules "Scalable Memory Buffer" chips, which means that Intel figured out that it is best to move the power gobbling AMB chip from the FBDIMMs to the systemboard. As you need only one chip to drive several registered DDR-3 modules, it consumes a lot less power than placing an AMB chip on each DIMM.     In the second of half of this year, Intel will have a IBM Power 6 killer and a server platform to match. The irony is that when it comes to "Intel Scalable Memory Buffers", IBM has the right to say "what to took you so long to figure out that FB-DIMMs were a pretty bad idea?" Back in 2005, IBM's X3 chipset already featured a solution that allowed large memory capacities with lower latency and much lower power consumption than FBDIMMs.  It will be interesting to see what IBM's respons to the Nehalem EX will be, as Intel's first octal core is going to enter the last market where RISC CPUs still hold their ground: 8 sockets and more.There have been previous attempts, but this time it is for real:more than 15 8+ socket designs are being readied. More irony: IBM will probably design the servers with the highest socket counts which really give the Power servers a run for their money...  As Intel gave its octal core CPU RAS features (MCA) that once belonged to the RISC and Itanium families only, it seems that the last stronghold of the non-x86 servers is going to fall..."mainframe slowly" but steadily. Only the Ultrasparc T2 with its radically different architecture may survive this assault.  The Machine Check Architecture is of course ultra important for the future Xeon MP systems. Even a quad socket system will contain 32 cores and probably up to 512 GB of RAM. That kind of machine simply cries out for large databases and virtualization consolidation. In the latter case, MCA should allow hypervisors such as ESX to overcome critical errors in one of the VMs, instead of shutting down tens of VMs.  In a different note, Intel claims that by August 2009 50% of it's DP server processors sold will be "Nehalem" based. So even though AMD is executing very well and introducing the hex-core "Istanbul" soon, it is not a minute too soon as the Opterons are under heavy attack.  Update: Anand also talked about Nehalem EX in his lab update here.
April 12, 2010 6:00 PM - High-End x86 The Nehalem EX Xeon 7500 and Dell R810.txt	 The stellar performance of the Xeons based on the Nehalem and Westmere architectures had a dark side for Intel: it cast a big shadow on Intel's top of the line Xeon 7400 series. You can talk about RAS features all you want but when a dual-CPU configuration outperforms quad-CPU configurations of your top-of-the-line CPU, something is wrong. And if this happens in the applications the latter is supposed to excel in, something is very wrong. SAP, OLTP, and other high-end server workloads are the workloads that are supposed to run better on the most expensive Xeon, not on the "popular" Xeon. Even worse, the AMD six-core 8000 series outperforms Intel's Xeon X7460 by a large margin as of several months ago. Quad dodeca-CPU servers will start to pop up in the shops of several tier-one OEMs any moment now, so Intel's new Xeon EX has a serious challenge. Intel emphasizes that its Xeon X7500 series plays in a higher league than the competition from Austin. The mission of the X7560 is to beat the RISC chips. That's not a bad strategy, as the RISC server buyers are used to paying a lot more for their servers. For example, a very basic IBM Power 7 configuration startsat $34000. Intel created an octal-core 16-thread giant based on the successful Nehalem architecture. To fit in with the other RISC monsters the CPU also comes with a massive L3 cache (24MB) and a bucket load of RAS features. On the lower-end of the targeted high-end server market, the market where x86 traditionally did well, Intel is going to get fierce completion. AMD's latest 2.2GHz twelve-core 6174 comes with a price tag of $1165, regardless of whether the server features two or four sockets. Intel however expects the server manufacturers to cough up to $3692 for a 2.26GHz X7460. It's clear that both competitors are targeting a different market. AMD is going after the cost conscious HPC/virtualization market, offering the best price/performance and performance/watt. Intel has no intention to compete on price/performance. It targets the higher-end market where software license costs are more important than the hardware, where downtime is so costly that people are willing to pay a premium for extra reliability features, and/or where the performance demands are extremely high. Intel's objective is to offer better performance than RISC vendors with similar RAS features at a lower price point. Up to 64 cores (8x8) and 128 threads and 512GB RAM can be found in a single Xeon 7500 series machine, so scalability should be quite impressive. For those who need RAS features but have no need for high performance, Intel offers the Xeon 6000 series. In this article we take a closer look at one of the most affordable Xeon 7500/6500, the Dell R810.
August 23, 2010 12:39 AM - The World's First 3TB HDD Seagate GoFlex Desk 3TB Review.txt	 Ive spent so much of the past two years covering SSDs that youd think Id forgotten about traditional hard drives. All of my work machines have transitioned to SSDs, as have all of my testbeds for reliability and benchmark repeatability reasons Ive mentioned before. What I dont mention that often is the stack of 1TB hard drives I use to store all of my personal music/pictures/movies, AnandTech benchmark files that drive my lab and to power my home theater (yes, final update on that coming soon). Hard drives havent lost their importance in my mind, their role has simply shifted. My OS, applications, page file, documents and even frequently played games (ahem, Starcraft 2) all end up on my SSD. That doesnt leave a lot of room for anything else, and for that bulk data theres no cheaper or better alternative than mechanical storage. One and two terabyte drives are now commonplace, the former selling for $60 a pop. Recently Seagate announced the next logical step, a five platter three terabyte drive with a catch - its external only. The FreeAgent GoFlex Desk is a mouthful of branding that refers to Seagates line of external 3.5 drives. The drives themselves are standard 3.5 hard drives in a plastic enclosure designed to mate with GoFlex Desk adapters that add USB 2.0, USB 3.0, FireWire 800 or Ethernet connectivity to the drive. Currently the GoFlex Desk is available in 1TB, 2TB and 3TB capacities. Weve spent much of the past week testing the latter both as a look at 3TB hard drives as well as the external device itself. Not Just Another Upgrade The first thing I did with the GoFlex Desk was try to get access to the drive inside. Despite the fact that Seagate is shipping a 3TB GoFlex Desk, the internal drive (also made by Seagate) wont be available until the end of the year. Thats silly, I thought, so I went about pulling the drive out of its casing. The drive part of the GoFlex Desk is little more than two pieces of plastic snapped together. Start to separate them and pull as firmly (yet carefully) as you can and theyll pop off, hopefully without breaking any tabs in the process so you can snap it back together. Inside the GoFlex Desk 3TB was a standard 3.5 Seagate Barracuda XT drive. There are rubber squares installed where the mounting screwholes are and the drive is in a metal tray, but other than that this is a run of the mill SATA HDD. The 3TB Barracuda XT is a 7200RPM drive. The drive has a 32MB DRAM cache, which is half of what Seagate ships on its 2TB drive making it clear that the 3TB drive used in the GoFlex Desk isnt 100% performance optimized. Seagate reaches its 3TB capacity by using five 600GB platters. Internally the drive uses 4K sectors however it translates to 512-byte sectors before it reaches the SATA port. This means to a SATA interface the 3TB drive looks like a drive with 512-byte sectors. The GoFlex Desk docks then map the 512-byte sectors back to the 4K format. Theres obviously overhead associated with these translations but its not huge in most cases. The final 4K translation done by the GoFlex Desk dock means that you can partition the drive using MBR which ensures Windows XP compatibility. Update: Seagate offered some clarification to the paragraph above. Internally the 3TB drive uses 512-byte sectors, however the GoFlex dock emulates a 4K drive to allow for a single 3TB partition to be created in Windows. For those of you looking to buy a 3TB GoFlex Desk, crack the case open and use the drive inside your system there are some challenges that you should be aware of.
October 19, 2010 1:43 PM - Western Digital Caviar Green 3TB and My Book Essential 3TB Drives Reviewed.txt	 Two months ago I looked at the worlds first 3TB desktop hard drive. It was a 5 platter, 3TB Seagate Barracuda XT inside an external GoFlex Desk chassis. The performance of the drive wasnt anything out of the ordinary, but the poorly ventilated chassis seemed ill equipped to deal with the thermal load a 5-platter, 7200RPM 3TB drive would throw at it. I wasnt terribly pleased and I wondered if Western Digitals external enclosure might be better suited for heat dissipation. WDs 3.5 external drives fall under the My Book brand and they have visibly more ventilation than the GoFlex Desk I reviewed. As luck would have it, last week Western Digital announced its own 3TB external drive: the My Book Essential. The Essential suffix somehow implies USB 3.0 support. Today, Western Digital takes it one step further and announces availability of the internal drive as well. The Caviar Green line is now home to a 2.5TB and a 3.0TB model, priced at $189 and $239 respectively. Lets go ahead and review both shall we? The Caviar Green 3TB Seagates external 3TB offering used five 600GB platters to hit the magic capacity point. While increasing platter count is a quick way to get a larger drive, increasing areal density is the desired approach. Seagates quick time to market justified the former, while Western Digitals desire to deliver a low power drive demanded the latter. As a result the Caviar Green 3TB uses only four 750GB platters, giving this drive the highest platter density of any 3.5 hard drive. Western Digital Caviar Green 3TB  WD30EZRS Interface SATA 3Gbps Formatted Capacity (Base 10) 3,000,592 MB User sectors per drive 5,860,533,168 Cache Size 64MB Load/unload cycles 300,000 Limited Warranty 3 years Spindle speed is another differentiating factor between WDs drive and the earlier Seagate offering. While the GoFlex Desk houses a 7200RPM hard drive, WDs 3TB drive is sold under the Green label. Western Digital doesnt disclose actual spindle speed as it isnt consistent across all Green label drives. The 3TB specs simply list it as IntelliPower. I asked WD for more specifics and I got a reasonable explanation. In the Green line WD optimizes for power consumption. It attempts to make all drives consume roughly the same amount of max power, which happens to be 3 - 5W below a typical 7200RPM drive. The spindle speed isnt dynamic, its set at manufacturing and remains at that. All green drives will spin below 6000 RPM and the spec never drops below 5400RPM. What this means is that all 2.5TB drives will spin at one speed while all 3TB drives may spin at another, both between that 5400 RPM to 6000 RPM range. Like many modern drives, the 3TB Caviar Green uses 4KB sectors internally however it emulates 512-byte sectors for compatibility reasons. Unlike Seagate, Western Digitals 3TB drive ships with a 3Gbps SATA interface. This isnt a problem given that neither drive can push enough data to saturate the 3Gbps SATA interface. Without SSDs theres very little reason for 6Gbps SATA support on desktop storage these days. Where the 3TB Caviar Green really breaks the mold is that it is shipped with a HighPoint RocketRAID 620 PCIe SATA card. The purpose? Breaking the 2.19TB barrier of course.
May 19, 2011 1:30 PM - Westmere-EX Intel's Flagship Benchmarked.txt	 Intel's Best x86 Server CPU The launch of the Nehalem-EX a year ago was pretty spectacular. For the first time in Intel's history, the high-end Xeon did not have any real weakness. Before the Nehalem-EX, the best Xeons trailed behind the best RISC chips in either RAS, memory bandwidh, or raw processing power. The Nehalem-EX chip was well received in the market. In 2010, Intel's datacenter group reportedly brought in $8.57 billion, an increase of 35% over 2009. The RISC server vendors have lost a lot of ground to the x86 world. According to IDC's Server Tracker (Q4 2010), the RISC/mainframe market share has halved since 2002, while Intel x86 chips now command almost 60% of the market. Interestingly, AMD grew from a negligble 0.7% to a decent 5.5%. Only one year later, Intel is upgrading the top Xeon by introducing Westmere-EX. Shrinking Intel's largest Xeon to 32nm allows it to be clocked slightly higher, get two extra cores, and add 6MB L3 cache. At the same time the chip is quite a bit smaller, which makes it cheaper to produce. Unfortunately, the customer does not really benefit from that fact, as the top Xeon became more expensive. Anyway, the Nehalem-EX was a popular chip, so it is no surprise that the improved version has persuaded 19 vendors to produce 60 different designs, ranging from two up to 256 sockets. Of course, this isn't surprising as even mediocre chips like Intel Xeon 7100 series got a lot of system vendor support, a result of Intel's dominant position in the server market. With their latest chip, Intel promises up to 40% better performance at slightly lower power consumption. Considering that the Westmere-EX is the most expensive x86 CPU, it needs to deliver on these promises, on top of providing rich RAS features. We were able to test Intel's newest QSSC-S4R server, with both "normal" and new "low power" Samsung DIMMs. Some impressive numbers The new Xeon can boast some impressive numbers. Thanks to its massive 30MB L3 cache it has even more transistors than the Intel "Tukwilla" Itanium: 2.6 billion versus 2 billion transistors. Not that such items really matter without the performance and architecture to back it up, but the numbers ably demonstrate the complexity of these server CPUs. Processor Size and Technology Comparison CPU transistors count (million) Process Die Size (mm) Cores Intel Westmere-EX 2600 32 nm 513 10 Intel Nehalem-EX 2300 45 nm 684 8 Intel Dunnington 1900 45 nm 503 6 Intel Nehalem 731 45 nm 265 4 IBM Power 7 1200 45 nm 567 8 AMD Magny-cours 1808 (2x 904) 45 nm 692 (2x 346) 12 AMD Shanghai 705 45 nm 263 4
August 22, 2011 2:05 PM - Intel to Release New Itanium CPUs in 2012 New Architecture and Up to Eight Cores.txt	 Intel held a keynote at Hot Chips conference in Stanford University last week where it announced some details of upcoming Itanium CPUs. While originally meant to conquer the enterprise market, Itanium is mostly used for the ultra high end enterprise space. Itanium isn't backwards compatible with x86 and uses Intel's VLIW IA-64 architecture. Intel claims that Itanium is a four billion dollar business and more than 80% of world's top 100 companies utilize Itanium. The codename for the new CPUs is "Poulson" and it is the 10th Itanium CPU lineup. The biggest updates are a new architecture, twice as many cores (up to eight), twice the instruction throughput and a 32nm process. Comparison of Itanium CPUs  Poulson Tukwila Core/Thread Count Up to 8/16 Up to 4/8 Frequency TBA Up to 1.73GHz L3 Cache Up to 32MB (?) Up to 24MB Manufacturing Process 32nm 65nm Transistor Count 3.1 billion 2.046 billion Die Size 544mm^2 (?) 699mm^2 What's interesting is that Intel is skipping the 45nm process totally and going straight for 32nm. The process change alone would be huge but throw in a new architecture too and Poulson looks like a major upgrade from Tukwila. Doubling the cores is a very aggressive move as well, although not surprising due to the die shrink. Lets look at the new architecture and the features it provides. First, the new architecture will bring a new feature: Intel Instruction Replay Technology. By inserting instruction buffers between pipeline stages Poulson can more quickly recover from an error in the pipeline. Rather than having to completely flush the pipeline and start over from scratch, Poulson can simply begin execution at the last known good instruction buffer. Second, the Hyper-Threading Technology receives some improvements. Intel calls the new feature dual-domain multithreading, which Intel describes as allowing for independent front end and back end pipeline execution. Since the introduction of Tukwila in 2010, Itanium CPUs have shared the same chipset as Xeon MP CPUs (Becton and Westmere-EX) - i.e. the 7500 chipset. Poulson will continue this pattern and will use the same 7500 chipset as its predecessor. This was and still is a smart move from Intel as it allows clients to reuse many elements of existing Itanium designs. Source: Intel
May 8, 2015 8:00 AM - The Intel Xeon E7-8800 v3 Review The POWER8 Killer .txt	 The story behind the high-end Xeon E7 has been an uninterrupted triumphal marchfor thepast 5 years: Intel's most expensive Xeon beats Oracle servers - which cost a magnitudemore - silly, and offers much better performance perwatt/dollar than the massive IBM POWERservers.Each time a new generation of quad/octal socketXeons is born, Intel increases the core count, RAS features, and performance per core while charging more for the top SKUs. Each time that price increases is justified, as the total cost ofa similar RISC server is a factor more than an Xeon E7 server.From the Intel side, this new generation based upon the Haswell core is no different: more cores (18 vs 15), better RAS, slightly more performance per core and ... higher prices. However, before you close this tab of your browser, know that even this high-endmarket is getting (more)exciting.Yes, Intel is correct in that the market momentum is still very much in favor of themselves and thus x86. Noless than 98% of the server shipments have been "Intel inside". No less than 92-94% of the four socket and higher servers containIntel Xeons. From the revenue side, the RISC based systems are still good for slightly less than20% of the $49 Billion (per year)server market*. Oraclestill commands about 4% (+/- $2 Billion), but has been in a steady decline.IBM's POWER based servers aregood for about 12-15% (including mainframes) or $6-7 Billiondepending on who you ask (*).  It is however not game over (yet?) for IBM.The big news of the past months is that IBM has sold its x86 server division to Lenovo. As a result, Big Bluefinally throwits enormous weight behind the homegrown POWERchips. Instead of a confusingandhalfheartly"we will sell youx86 andItaniumtoo" message, we nowget the "time toswitchover toOpenPOWER" message.IBMspent $1 billionto encourageISVsto portx86-linuxapplications to the Power Linux platform. IBM also opened up its hardware: since late 2013, theOpenPowerFoundationhas been growing quickly withWistron(ODM),TyanandGoogle building hardware on top of the Power chips. TheOpenPOWERFoundation now has 113 members, and lots ofOpenPowerserversare being designed andbuild. Timothy Green of the Motley fool believesOpenPowerwill threatenIntel's serverhegemony in the largest server market, China.  But enough of that. This is Anandtech, and here we quantify claims instead of just rambling about changing markets. What has Intel cooked up and how does it stack up to the competion? Let's find out. (*)Source:IDC Worldwide Quarterly Server Tracker, 2014Q1, May 2014, Vendor Revenue Share
February 12, 2016 8:00 AM - Examining Soft Machines' Architecture An Element of VISC to Improving IPC.txt	 Last week, Soft Machines announced that their 'VISC' architecture was available for licensing, following the announcement of the original concepts over a year ago. VISC, in a nutshell, is designed as a solution to improving the number of instructions per clock a single thread can process in a given time, which potentially makes it a very interesting design in an era where IPC gains are harder and harder to realize. The concepts behind their new VISC architecture, which splits the workload of a single linear thread across multiple cores, are intriguing and exciting. But as with any new fundamental change in computer processing, subject to a large barrage of questions. We were invited to a presentation and call with the President and Chief Technical Officer Mohammed Abdallah and the VP Marketing and Business Mark Casey, and I put a number of questions on the lips of analysts to them. Identifying Single Thread Performance Bottlenecks Any discussion about processor performance over the last couple of decades has involved several factors, including getting better performance through an increased power budget, a higher frequency, extracting instruction level parallelism (ILP), getting better at minimizing delays through better branch prediction, or adding more cores and improving thread level parallelism (TLP). Each of these methods have varying degrees of success at increasing performance  long-time readers will remember the Pentium 4 days of hitting a frequency and power wall which then switched the focus to efficiency. Some tasks, like graphics, are inherently parallel and can take advantage of multiple hundreds or thousands of cores, or the software can be optimized. However, the nature of most software code and instructions is that they are single threaded by nature, and their performance relies on how fast the instructions can be processed within a single thread. The main way of increasing performance, or in this case the instructions per unit frequency (instructions per clock, or IPC), is to expand the CPU architecture to allow more commands to be processed at once. Moving from a 3-wide out-of-order architecture to a 5-wide out-of-order architecture theoretically allows for a 66% increase in instruction throughput if (and only if) the code is sufficiently dense enough to extract those operations, and the other features in the architecture can ensure all the operations are fed every clock cycle. The problem with moving to a wider architecture is typically power and design complexity. As shown by various chip designs over the years, the wider the architecture the more silicon has to be set aside for assets like buffers, re-order windows and caching. If there is a silicon budget and enough power headroom, we see designs like the six-wide Intel Skylake cores or the seven wide NVIDIA Denver cores able to extract peak performance when code is written that matches the hardware. However the potential downside of a wide architecture is that it remains inefficient for sets of instructions that only need a 2-wide or a 3-wide architecture. Alternatively, if multiple programs or threads want to use the hardware, then a single core is inaccessible to additional threads while the first thread is still in use (though this can be avoided somewhat by simultaneous multithreading or SMT which will let another thread have access when the first has encountered a stall such as waiting for L1/L2 memory). As a result, modern designs also include a number of cores to handle the multile thread/multiple program scenario. Generally speaking this works well, especially with high-performance cores, but it becomes a bit of an issue itself when much of the worlds hardware is actually composed of many cores that have poor single threaded performance. Older Core 2 / Conroe systems, basic Bulldozer, or ARM Cortex-A7 designs are (still) widely used and often ship with multiple cores to allow for multiple programs at once. And while they can scale up with additional threads to the number of cores they offer, if any single or lightly-threaded software needs more performance, those extra cores are not used or are only minimally beneficial overall. This brings us to Soft Machines, whose VISC architecture aims to change this. Meet VISC I should start by saying that despite the similarities to other architectural names, VISC is not an acronym. I asked directly and it is merely a noun for the purposes of trademarking. People can interpret it as a virtual instruction set computing or something similar, but the company doesnt apply any acronym to the letters. But a virtual instruction set is a good description here. For the most part, processor architectures were traditionally built around either CISC (complex) or RISC (reduced) instruction sets and execution models, while more modern designs (e.g. Intel Core) are increasingly a mix, or so-called CRISC design. The difference between CISC and RISC boils down to the fact that simpler designs can be more power efficient, but complex designs can do more complicated things in fewer cycles, all the while CRISC essentially meets the two paradigms in the middle in an attempt to gain the benefits of both, though not without inheriting some of the drawbacks as well. VISC, for lack of a better description, is a RISC design using a custom instruction set over a translation layer which allows a single thread of operations to be dispatched over multiple physical cores. The base diagram looks something like this: Here is an example of a VISC design with four physical cores. The design can handle four virtual cores or threads as well, but what makes the VISC design different is that when the virtual core has a thread of instructions, it can use the resources of any physical core. Thus, if each physical core is a 4-wide out-of-order design, if a thread running on a virtual core can utilize the resources of all four cores essentially making a giant 16-wide design, then under VISC can do so. This should instantly throw up a number of questions on What!? How?! Why?! Power? Frequency? Performance? Efficiency? Complexity? and as well as many others in the industry, we had the same questions.
May 11, 2017 11:10 AM - Intels Itanium Takes One Last Breath Itanium 9700 Series CPUs Released.txt	 One of Intels ventures into the historic mainframe space was Itanium: a 64-bit capable processor designed in conjunction with Hewlett Packard.The main reason for Itanium was to run HP-UX and compete against big names, such as Oracle, using a new IA-64 instruction set. The appeal for the original Itanium parts was support for RAS features, ECC, and cores focus on a wide, parallel architecture - the latest cores support 12-wide execution for example. For a short while, there was success: HPs systems based on Itanium are advertised as high-uptime mission critical servers, and a number of customers cling to these systems like a child clings to their favorite blanket due to the way they are integrated at the core of the company. The main purpose was to compete against other mission critical servers and mainframes based on SPARC and IBM Power. So when the processors were initially delivered to customers, there was potential. However the initial impression was not great - theyconsumed too much power, were noisy, and needed over the top cooling. Over the years and generations of Itanium, the march into the x86 enterprise space with x86-64 drew potential Itanium customers away, then followed the drop of Microsoft's support for Itanium in 2008, and Oracle's dropped support in 2011. Xeon offerings were becoming popular, with CPUs incorporating the RAS/ECC features required, and Intel decided to slow down Itanium development as a result. In the meantime, due to the way the market was moving, HP transitioned a good part ofits product stack to Xeons. Despite this, legal battles between HP and Oracle ensued given predicted support for HP-UX customers. At this point, there were fewer potential Itanium customers each quarter, although existing customers required support. Today marks the release of the final known variant of Itanium, the 9700 series, beyond assurance testing. Intel spoke to IDG, stating that this generation, code-named Kittson, would be the final member of the Itanium family. These chips are likely to only end up in HP-based Integrity i6 high-uptime servers running HP-UX, and start at $14500. Hewlett Packard Enterprise has stated previously that it will keep support for Itanium-based products until 2025, with the latest OS update (HP-UX 11i v3 2017) coming in June. As for the processors themselves, four 9700 processors form the stack, with quad-core and eight-core parts all with hyperthreading, differing in frequency, power, and L3 cache. Intel Itanium (Kittson) CPUs  Cores/ Threads Base Freq L3 TDP Cost* Itanium 9760 8/16 2.66 GHz 32 MB 170 W $4650 Itanium 9750 4/8 2.53 GHz 32MB 170W $3750 Itanium 9740 8/16 2.13 GHz 24 MB 170 W $2650 Itanium 9720 4/8 1.73 GHz 20 MB 130 W $1350 *Cost is listed for the equivalent Poulson CPUs. The base silicon comes in at 3.1 billion transistors, and are made on Intels 32nm process. Memory is supported up to DDR3-1067, with two memory controllers but support for scalable memory buffers is present. This is similar to the 9500 series, code-named Paulson. These chips are designed to be purely a drop into previous systems. Intel isnt announcing an official press release around this, and unlike other new architectures, there are next to zero improvements. According to the documents, the only change is that the top two SKUs get a clock bump: Theres probably something new under the hood, perhaps for a specific end-customer, but at this time Intel is directing anything 9700 related to equate to the 9500 series. Customers still interested in Itanium are directed to HPE resellers. Carousel Image from Konstantin Lanzet (Wikipedia) of Itanium 2 (Poulson) News Source: IDG
March 8, 2018 6:00 PM - Intel to Discontinue Itanium 9500 Poulson CPUs.txt	 Intel has begun its product discontinuance program for its "Poulson" Itanium 9500-series processors. Intels customers will have to make their final orders for these CPUs this fall and the last Poulson chips will be shipped in early 2021. Intels Itanium 9500-series lineup consists of four CPUs: the quad-core Itanium 9520 and 9550 as well as the eight-core Itanium 9540 and 9560. All of the these processors were released in Q4 2012, and were supplanted with the newer "Kittson" 9700 CPUs last year. Now Intel has set the entire Poulson family to discontinued in Q1 2021, a little more than eight years after their release. Intels customers are advised to place their orders on Itanium 9500-series processors by September 17, 2018. Orders will become non-cancelable on September 28, 2018. The final Poulson chips will be shipped by March 5, 2021. Keep in mind that HP Enterprise (the only company that uses Itanium) will cease selling servers based on the Itanium 9500-series on March 31, 2018, so demand for Poulson products is not going to be high in the coming years. Intels Poulson processor (pictured above on the right, image by Silicon.fr) was among the most significant microarchitectural and performance advancements of the Itanium products throughout their history: the CPU doubled issue width to 12 instructions per cycle, got 4-way Hyper-Threading, received higher frequencies, as well as up to eight cores. By contrast, Intels latest Itanium 9700-series processors run only slightly faster than the highest-end 9500-series chips. The retirement of the Poulson family will mean that Intel's 9700 processors will be the only Itanium parts on the market  and indeed they will be the last Itanium processors altogether as Intel has ceased further Itanium development. Meanwhile only a single vendor  long-time Itanium partner HP Enterprise  is still selling Itanium-based servers. But even so, expect Itanium machines to be around for years to come; HPEs Integrity machines are used primarily for mission- and business-critical applications, where customers are invested into the platform for the very long term. Related Reading: Intels Itanium Takes One Last Breath: Itanium 9700 Series CPUs Released Intel to Release New Itanium CPUs in 2012: New Architecture and Up to Eight Cores Image Source: Silicon.fr
August 21, 2018 5:55 PM - Hot Chips 2018 Tachyum Prodigy CPU Live Blog.txt	 05:55PM EDT - One of the more interesting talks is from Tachyum, who have a deep presentation about their new hyperscale Prodigy processors with up to 64 cores and eight channel memory. Tachyum is headed up by one of the original co-Founders of SandForce and Wave Computing. The talk is set to start at 3pm PT / 10pm UTC.06:01PM EDT - Trying to give the best of CPU GPU TPU06:01PM EDT - Prodigy is a server/AI/supercomputer chip for hyperscale datacenters06:01PM EDT - Is programmable and optimized06:02PM EDT - AI based on spiking model requires other hardware06:02PM EDT - Not all AI is neural networks06:02PM EDT - Tape out in 201906:02PM EDT - One die with three different versions06:03PM EDT - Different variants for different markets06:03PM EDT - T864 = 64 cores and 8 DDR4/5 controllers, 72 PCIe 5.0, 2x400G internet on single piece of silicon06:03PM EDT - Each core will be faster than Intel Xeon06:03PM EDT - Single threaded performance on Spec INT/FP better than Xeon06:04PM EDT - 2TF performance per core06:04PM EDT - Better perf than Volta chip wide06:04PM EDT - 180W at 4 GHz06:04PM EDT - 7nm, 12 metal layers, 0.8V, 290 mm206:05PM EDT - Data wires are small to reduce power06:05PM EDT - Smaller than ARM06:05PM EDT - Using standard cells and memories06:05PM EDT - In order design - out of order execution with compiler06:05PM EDT - Accepts C/C++/Fortran06:05PM EDT - Programming model is CPU-like06:06PM EDT - 32 x INT64 registers 06:06PM EDT - 32 vector registers 256/128 bits06:06PM EDT - Sorry pictures have stopped06:06PM EDT - maybe not06:06PM EDT - ILP based on bundling06:07PM EDT - 2 load + 2 multiply-add + 1 store + 1 address increment + 1 compre + 1 branch per clock06:07PM EDT - 1.72 instructions per cycle normal06:07PM EDT - based on bundle size from compiler06:07PM EDT - 2.6 instructions/bundle on average06:07PM EDT - 8-RISC-style micro-ops per cycle06:08PM EDT - Itanium was stalled 50% of the time due to dynamic scheduling. Normal OoO core is about 15%. Due to our simulator, we achieve less than 20% stall06:08PM EDT - Can do OoO and speculation in software without expensive hardware06:08PM EDT - Changing the perf/$ and perf/W equation06:09PM EDT - Linux and FreeBSD ported in 201906:09PM EDT - Device drivers, boot-loaded and Java JIT06:09PM EDT - Replace parts of GCC to enable Prodigy core06:09PM EDT - Working so critical applications work day one06:10PM EDT - Learning from Arm entering the hyperscale space - took over 12 months and $100m to compile and port to Arm06:10PM EDT - For a startup that would be problem06:10PM EDT - Allow customers to use x86 binary through QEMU emulator06:11PM EDT - allows for shorter deployment time06:11PM EDT - Existing binaries supported via emulators06:11PM EDT - Use service providers in Europe to port existing applications06:12PM EDT - The core - 8 micro-ops decode and dispatch per clock (16 bytes per op)06:12PM EDT - 1 store, 1 load, 1 load/store unit06:12PM EDT - 32 x 512-bit vector registers06:12PM EDT - Conventional front end06:12PM EDT - Simple branch predictor works sufficiently well with compiler06:13PM EDT - Machine can execute 1-4 words per clock, each word with 1-2 ops06:13PM EDT - Instruction Execution looks like in-order, but can slide instructions on cache misses06:14PM EDT - 9-stage pipeline due to no register renaming required06:14PM EDT - Supports 64B/clock per memory op into L106:14PM EDT - Private 512KB L2 cache06:14PM EDT - Spill and fill with data cache and L2 cache06:15PM EDT - L3 distributed cache is same latency as L2 06:15PM EDT - Supports FP16 and INT806:15PM EDT - 2x512-bit MAC or 3x512-bit int per clcok06:16PM EDT - Standard mesh with the cores, 8x806:16PM EDT - It also has an IO ring06:16PM EDT - for networking applications, lower latency for far memory access06:16PM EDT - important for no packet loss and reduces congestion06:16PM EDT - 32 bytes per clock per direction I/O to DRAM06:17PM EDT - 2 x HBM3 controllers and 8 x DDR controllers06:17PM EDT - Average utilization on Amazon EC is 30%. Facebook average utilization is 40%06:18PM EDT - 60-70% idle. With an AI chip, you can train / inference when idle with chips that are capable06:18PM EDT - Our chips you can run servers and then AI when idle06:18PM EDT - access to 20x AI 06:18PM EDT - don't need different hardware06:19PM EDT - In a Facebook 100MW datacenter, 442k servers. 40% idle means 265k idle servers per day06:19PM EDT - With our chip, those chips can run AI during downtime06:19PM EDT - e.g. 256k servers, each with 4x2x100 GbE with no oversubscription06:21PM EDT - Also saves power06:21PM EDT - New technology is needed - datacenters could consume 40% of planet energy by 204006:22PM EDT - Overall aims06:22PM EDT - Outperform Xeon E5 v4 using same GCC06:22PM EDT - 4.0 GHz on 7 nm06:22PM EDT - Tape out in 201906:22PM EDT - Multiple interested and engaged customers06:23PM EDT - Integer Datapath is in place - currently limited by speed of SRAMs06:23PM EDT - Q&A time06:26PM EDT - Q: I feel like deja vu - at Hot Chips, Intel introduced VLIW-concept Itanium that pushed complexity onto the compiler. I see traces of that here. What are you doing to avoid the Itanium traps? How will you avoid IP from Intel? A: Itanium was in-order VLIW, hope people will build compiler to get perf. We came from opposite direction - we use dynamic scheduling. We are not VLIW, every node defines sub-graphs and dependent instructions. We designed the compiler first. We build hardware around the compiler, Intel approach the opposite. 06:27PM EDT - Q: You mention emulation for x86. What kind of penalty in performance? A: About 40% performance loss. Significant because our customer didn't want us to invest in that, as 90% of their software will be natively compiled by next year. It's a temporary deployment. Binary 4.0 GHz emulated still outperforms 2.5 GHz Xeon06:28PM EDT - Q: How do you dodge the patents in place in this? A: We building a new beast. Our tech is significantly different. This is America.06:30PM EDT - That's a wrap. Next live blog in 30 mins on IBM.
December 27, 2018 3:30 PM - AnandTech Year in Review 2018 CPUs.txt	 When Ryan initially asked me to write a roundup of the years news on CPUs, I laughed. There has been a lot going on this year, from processor releases and reviews, to security issues, to discussions about the next few years of computing. However a couple of weeks ago I wrote a script to pull every AnandTech article out of our archive to filter into my own database for analysis. It turns out that the AT staff between us have written just shy of 200 news articles and longer format reviews about CPUs this year, and here are the highlights. When discussing CPUs, at least the desktop market, our attention focuses to two companies in particular: Intel and AMD. January: Security and CES The start of the year typically begins with the big CES trade show in Las Vegas, but before we event got to that point, news broke about two new classifications of vulnerabilities affecting most modern processors: Spectre and Meltdown. This was significant as these two names signified a new family of microarchitectural vulnerabilities derived from the base design of modern processors, exploiting some of the tricks used for how we get a lot of speedup in common day-to-day tasks. The news broke about a week earlier than the companies intended (part of the term responsible disclosure), but the big players having around six months notice to put forward fixes after Googles Project Zero first discovered them in 2017. These vulnerabilities were a common thread through 2018 (and still are today), as companies put forward a mix of software and firmware fixes for hardware in the wild, built security assurance teams, and put research towards hardware fixes for future products. Essentially every high-performance processor ever made Intel,AMD,ARM, andPOWER is thought to be vulnerable at some level, and every major company made official statements on the issue. Software and firmware fixes for several generations of processors came out through 2018, and new products that block some of these attacks came out in the latter stage of 2018, although 2019 is where it gets more serious. Some of these fixes cause performance regression in certain tasks, mostly enterprise based, however this is a topic that will also be at the forefront of every 2019-2020 CPU launch. Not to be outdone by security news, the CES trade show was one of our busiest ever. AMD kicked off proceedings with a full day of discussions, keynotes, and disclosures. The key to AMDs presentation is that the new Zen microarchitecture products, Ryzen and EPYC, were paving the way for the future. In the presentations, AMD presented roadmaps for product launches in 2018, including new APUs, a new range of desktop processors on 12nm with Zen+ updates, second generation Threadripper on the high end, and a push towards 7nm Vega GPUs at the end of the year. This was a very well put together presentation from AMD, showing that they have roadmaps and they are willing to commit. On top of this, AMD reiterated its long term roadmap on CPU and GPU technologies, going through 2020. One key highlight here was our interview with AMDs CEO, Dr. Lisa Su, on how AMD is set to approach its new era. Intel had its usual keynote at CES, hosted by now former CEO Brian Krzanich, which addressed the security issues briefly, spoke mostly about drones and connecting people, but made no mention of the companys progress on its next generation process technology at 10nm. Instead, the company mentioned 10nm very briefly at the end of a small 9 minute presentation at the Intel booth the next morning, at 8am, through Gregory Bryant, the SVP of the Client Computing Group. The news wasnt that great: Intel confirmed it had shipped 10nm in 2017 for revenue. It was literally that short of a sentence. No word on how the technology was progressing, or any future timelines. There was some upside for Intel in January however. The company launched its best performing integrated graphics solution ever: the Core with Radeon RX Vega Graphics set of SoCs. These processors took quad-core mobile chips and paired them with a custom Polaris graphics solution from AMD all on the same processor package. The graphics chip was connected with Intels EMIB technology to a stack of HBM2 memory, keeping the package small. The offerings ranged from 65W to 100W TDP, and the goal here was to provide something just below discrete graphics solutions in a thin and light form factor. We saw the HP Spectre x360 and Dell XPS 15 both updated with these options, as well as a Hades Canyon NUC, a Chuwi HiGame mini-PC, and a Dell Precision device later in the year. February: Visiting GlobalFoundries Transitioning from January to February is usually a slow time in the industry as Chinese New Year sets in. Depending on the company, this can be anything from a one week break to a three week break, so we dont tend to see many product launches at the end of January/beginning of February unless the factories have enough stock, or the fabs chain doesnt involve China. Intel kicked off the month with a reactionary measure to the Spectre and Meltdown issues by formally announcing the creation of the Product Assurance and Security Group (which had technically been reported in January, but wasnt formal until this point). The strategic goals of the group were unclear, except that the group will coalesce Intels cross company efforts to continuously improve product security. In the first week, Intel also launched its Xeon D-2100 platform, placing it in a spot above its previous Xeon D-1500 line-up. These processors were different to the older ones because instead of using mainstream Corecores, they were essentially the same as the Xeon Scalable line of processors, albeit with lower frequency, lower TDP, but focused on integrated networking and QuickAssist features. Parts are in the market up to 18 cores. A 'bunny suit' for inside the Fab. Unfortunately, no pictures allowed inside. Also in that early part of February, Nate and I visited GlobalFoundries Fab 8 in Malta, New York. At this time, GF was the talk of the town, having successfully executed AMDs strategy with Ryzen and EPYC, and were looking forward to both 7nm and EUV products. Having a look inside a modern fab is a rare thing indeed, and it opened our eyes to the fact that these are more than just black box factories and require leading edge technology and setups to execute as required. We also interviewed CTO Gary Patton about the fab and the future of GF through 2018. On AMDs side of the fence, the company launched its first desktop APUs, combining Zen cores and Vega graphics in a desktop form factor. The Ryzen 5 2400G and the Ryzen 3 2200G both offered compelling features: reasonable 720p gaming with four cores starting at $100. Along with the processors came a series of stock coolers that put everything else to shame. AMD also launched its EPYC embedded platform, offering a range of Ryzen-like products for integrated systems. March: Security Again The end of Q1 was our quietest month of the year in terms of absolute CPU news, although a couple of big elements came to the fore. Aside from our analysis into Ryzen Threadripper coolers with large base plates, such as the Noctua NH-U14S TR4-SP3, Intel also discontinued the n-1 family of Poulson Itanium processors, putting the penultimate nail into that family of products. If the security issues of Spectre and Meltdown were not enough, some other security issues came to light. A previously unknown security company, CTS-Labs, announced it had found several vulnerabilities within ASMedia chipsets and AMDs Secure Processor. We covered the issues in detail, however there were many questions about how this announcement came about: the security firm offered the companies no form of responsible disclosure (they even said it was a bad idea), no strict details on the vulnerabilities, produced a website in advance to promote the issues, hired a PR firm to handle it with professionally rendered videos about the issues, and some supposedly unrelated financial firms were ready with multi-page documents stating the issues meant that AMD was worth zero. In our interview with the company, it was clear that there were technical miscommunications, and when we asked about the backers for this project, the call was abruptly ended. I recorded a two-hour podcast/video on the issue with TechTeamGB, which is worth watching. AMD, after a week to analyse the findings, subsequently released a statement on the matter. The truth is, most of the exploits were secondary vulnerabilities  they required elevated permissions to begin with. However AMD confirmed that fixes would quickly be in place. The security company has not been heard from since, and the website they produced to promote the issues no longer exists.
January 31, 2019 6:30 PM - Intel to Discontinue Itanium 9700 Kittson Processor the Last of the Itaniums.txt	 Intel on Thursday notified its partners and customers that it would be discontinuing its Itanium 9700-series (codenamed Kittson) processors, the last Itanium chips on the market. Under their product discontinuance plan, Intel will cease shipments of Itanium CPUs in mid-2021, or a bit over two years from now. The impact to hardware vendors should be minimal  at this point HP Enterprise is the only company still buying the chips  but it nonetheless marks the end of an era for Intel and their interesting experiment into a non-x86 VLIW-style architecture. The current-generation octa and quad-core Itanium 9700-series processors were introduced by Intel in 2017, in the process becoming the final processors based on the IA-64 ISA. Kittson for its part was a clockspeed-enhanced version of the Itanium 9500-series Poulson microarchitecture launched in 2012, and featured a 12 instructions per cycle issue width, 4-way Hyper-Threading, and multiple RAS capabilities not found on Xeon processors back then. It goes without saying that the writing has been on the wall for Itanium for a while now, and Intel has been preparing for an orderly wind-down for quite some time. At this point, the only systems that actually use Itanium 9700-series CPUs are the HPE Integrity Superdome machines, which are running the HP-UX 11i v3 operating system and launched in mid-2017. So Intel's sole Itanium customer will have to submit their final Itanium orders  as well as orders for Intels C112/C114 scalable memory buffers by January 30, 2020. Intel will then ship its last Itanium CPUs by July 29, 2021. HPE for its part will support their systems throughat least December 31, 2025, but depending on how much stock HPE wants to keep on hand, they'll presumably stop selling them a few years sooner than that. With the EOL plan for the Itanium 9700-series CPUs in place, it certainly means that this is the end of the road for the whole Itanium project, both for HPE and Intel. The former has been offering Xeon-based NonStop and Integrity servers for years now, whereas the latter effectively ceased development of new CPUs featuring the IA-64 ISA earlier this decade. The machines running these CPUs will of course continue their operations through at least late 2025 (or until HPE drops HP-UX 11i v3) simply because mission-critical systems are bought for the long-haul, but Intel will cease shipments of Itaniums in 2.5 years from now. Related Reading: Intel to Discontinue Itanium 9500 Poulson CPUs Intels Itanium Takes One Last Breath: Itanium 9700 Series CPUs Released Intel to Release New Itanium CPUs in 2012: New Architecture and Up to Eight Cores Source: Intel
November 20, 2019 1:00 PM - AnandTech Exclusive An Interview with Intels Raja Koduri about Xe.txt	 Ever since Raja Koduri left AMD and joined Intel, I have been continuously asking for a 1-on-1 interview, as Im sure a number of my peers in the industry have also. For any event I went to that intersected with Raja, it became almost an amusing meme that Id ask for time with him. His (and his teams) response was pretty understandable, as he has had nothing more to add, officially for a while, given how deep his remit has been inside the company and how long these projects take to emerge. This week Raja gave the keynote at Intels HPC DevCon event, a precursor to Supercomputing, and I did my usual thing of asking for the interview, fully expecting the same not quite yet response. To my surprise, Intel agreed, and we spent the best part of an hour discussing his role at Intel, his work, and some of the finer details of the recent Xe-HPC, Ponte Vecchio, and Aurora announcements. Raja Koduri is a well-known figure in the semiconductor space, having previously held high positions at Apple, driving new architectures at AMD, and now at Intel in charge of everything to do with Architecture. His particular focus has been on Software and the new GPU initiative, and he is ultimately one prong of Intels charge into the wider compute arena, covering everything from integrated graphics to discrete graphics and then onto compute graphics. This means hardware, but also Raja is spending a lot of time with software, and digging deep with Intels new oneAPI initiative, to develop an all-encompassing SDK that developers can use to write code across any number of elements in Intels hardware stack. Raja is just coming shy of two years since joining Intel, over which time he has given a number of presentations and been part of a number of announcements, but truth be told the specifics of his role, beyond the few elements in this paragraph, are still unknown to the wider community. This week at Intels HPC Developer Conference (HPC DevCon, or just DevCon), the company lifted the lid on Intels first high-performance graphics implementation, called Ponte Vecchio (PVC for short). In the scale of Intels new Xe architecture for graphics, the company announced that it will have two microarchitectures for the wide graphics market (Xe--LP for low power/integrated solutions, and Xe-HP for high power/discrete solutions), and a single microarchitecture for the high performance compute market (Xe-HPC, which goes from discrete through to more complex packaging). Intel also showed diagrams of the chiplet design behind Ponte Vecchio, with EMIB, Foveros, a new Rambo cache, HBM, and a new Xe memory fabric (Xe-MF) that drives efficient scaling of all these elements. There are still plenty of points to pick from the slides of the presentation, which we are currently working on. Another element to the talk was Intels new oneAPI industry initiative, with the announcement that oneAPI is available in beta form today and is part of Intels DevCloud infrastructure too. We should point out that there were a few topics that Intel werent going to talk about, such as exact details about the new Ponte Vecchio design (finer details will be disclosed when Intel determines it is the right time, I was told)and we werent able to talk about Intels process technology. Intels recent disclosures or supply chain demands are not within Rajas wheelhouse, so were not covering them here. Id rather speak to the appropriate people directly about those topics. With that being said, wed like to thank Raja and his team for this opportunity.  Raja Koduri SVP, Chief Architect at Intel GM, Architecture, Graphics and Software Dr. Ian Cutress AnandTech, Senior Editor  The Koduri Factor Ian Cutress: Youve now been at Intel for coming up to two years, with a large remit of covering the companys Architecture. How would you characterize your role at Intel compared to what youve done at other companies? What makes Intel unique and special for you to be at the company at this phase of history? Raja Koduri: I think the biggest thing is to put my journey, to Apple, to AMD, and to Intel, into context. Its a cyclical thing that goes between wanting to make a difference at a scale, and wanting to be disruptive. At a scale company, like an Apple, you have the ability to reach hundreds of millions of users, so everything you do gets amplified. Theres a certain amount of satisfaction in that, in having an impact, and ultimately at a scale company you learn a lot. With enough time at such a company, you develop a fundamental set of disruptive ideas. Its hard to be disruptive at a scale company, and easier at a small company where they can be more agile and more open to taking risks. When I joined AMD 7 years ago, it was at an inflection point  they were hungry and ready to take risks and they provided me with a great opportunity to build new software and hardware architectures. They also provided me opportunities to learn about business and transforming company culture. The next cycle for me was basically where I saw the technology going, and that fundamental technologies are needed. As both Jim and I have said on stage at various times, we want to drive more data closer to where it is needed  this is in itself a very interesting challenge and goes beyond just GPU architecture or CPU architecture. This problem touches every layer of technology in the industry, and Intel is one of the few companies that have the scale of technology of investments to pursue this dream. The breadth and depth of technology scale is HUGE at Intel and I saw Intel as an incredible learning opportunity  enough to keep me busy and excited for the rest of my life. Its great having all these smart people in the room, with no barriers, talking through how were going to drive 10x, 100x, or more on this stuff. There are very few companies in which you can go across the entire stack, from transistor to software. IC: So far in your time at Intel, you and Jim Keller are often paired together as a team when it comes to disclosures and presenting to the media. In that context, Jim is often presented as the hardware person, and youre presented as the software person. Are your roles really that straight cut? RK: [chuckles] No, not really. You will have noticed that Im increasingly focusing on the software, but ultimately my role in Architecture at Intel covers both. Architecture is often a misused word today  people usually use it in the context of microarchitecture, when in reality architecture defines a contract between the system and the developer - how something is built is not architecture, but how something interfaces with outside world and how something functions is architecture. One of the most important aspects of architecture is the contract between hardware and software. I live very much in this hardware/software contract, the silicon platform contract, or in other words, the ecosystem  that is architecture. So my focus is on the what & why, and Jim focuses on how & when. Both of these points are connected with each other, so we work very closely to drive our collective teams forward. IC: Intel is well known for its process technology and manufacturing, and Dr. Murthy (Renduchintala, Rajas boss) announced a couple of years ago that Intel was disaggregating its product portfolio with its manufacturing process. When we interviewed Jim, he said (and has said repeatedly since) that hes not worried about the process, and in the past 30 years whenever people have had process problems they are always solved. Intels recent manufacturing issues are a known factor, but Ill ask the same question I posted to Jim  how much to you get involved at the process and manufacturing level? RK: Very much. Its also a really interesting topic. Like Ive said before, at Intel, we have such a strong connection between how we build our chips and how we manufacture them. Theres a historic way of how we set up the methodologies and then the processes and tools around that. At most companies you have to go with what the manufacturer tells you they can do  at Intel, we get to know everything. Were in a position where we can get involved in the manufacturing, and thats very different from the outside ecosystem. This method does also have its pros and cons. One of the big things that Jim and I are working on is how to amplify those positives in that model, as well as diminish the negatives. There are products and IPs, like particularly CPU for example, that get a lot of benefit from collaboration. By contrast, at a fundamental level, there are graphics and other things that due to the design dont need that intense level of customization  customizing at that level can actually get in the way of us executing fast relative to the outside industry.  Architecture and Microarchitecture IC: One of the big questions around Xe being an architecture and multiple micro-architectures being spun from it is to how many microarchitectures are being developed inside Intel? Before today we knew that Intel would have two, and today you put names to those, Xe-LP and Xe-HP, but also introduced Xe-HPC for the compute market. Is there a fourth? RK: It was good to mention the three on stage  it is three, and truth be told I dont have a fourth one. Maybe Ill have a fourth one if the need arises, but thats it  three covers the entire roadmap. IC: With the three variants of Xe youve mentioned today, Xe--L, Xe--HP, and Xe-HPC, are the products built on these microarchitectures fundamentally tied to specific Intel process nodes? RK: No. The IP can be ported to any process technology. IC: Of the three microarchitectures, I imagine you being knee deep in all three, defining the designs, managing the teams, and executing. Can you discuss a little bit actually how deep you personally go into this? RK: At Intel I say that from 7am to 7pm there is not a single moment where I can turn my neurons off. We have to get the architecture and the microarchitecture details right to have amazing products. Nobody inside or outside gave us a chance two years ago that we will get new things like GPUs done. They said we will take 5 years and we will lose interest two years in! Well, were two years in now and we have our first discrete GPUs powered on. Its not possible to drive these things without being hands-on. IC: One of the things about Xe and the graphics team is that Intel rehired a number of engineers that worked on the Larrabee product, the attempt at an x86 graphics architecture. Even though we got Xeon Phi from that project, the fact that Intel has rehired these engineers means that there are obviously things that Intel can relearn from them and that project. How does that filter into Xe? RK: Great question. You know, Larrabee, as well as Xeon Phi, taught us great learnings for lots of verticals. That experience helped us particularly with how Intel managed HPC at the time, and how we have built Xe-HPC and Ponte Vecchio today. There are a lot of problems that Xeon Phi solved particularly in relation to memory, coherency, virtual memory, reliability, and all that. Having access to that knowledge is helping us build a product like Ponte Vecchio very quickly. The Ponte Vecchio DNA incudes Gen, Xeon, Xeon Phi and even Itanium learnings.  The Goals of oneAPI IC: Intel is starting to open up about Xe, and it clearly wants to cover a large range of the market, all with the oneAPI stack. Can Xe and oneAPI be everything to everybody at the same time? RK: Thats a great question. First off, when we look at the scale and reach of graphics, whether its integrated graphics where we have hundreds of millions of users, or discrete graphics going into the cloud, one of the central elements is software. What is the software that runs all of this stuff? We have sets of APIs like DirectX, OpenGL, OpenCL, and other languages  and we also have middleware, like game engines that sit on top of the stack. What we want is to be everywhere where there is a software presence. I will summarize our strategy simply as Leverage, Optimize, Scale. We are leveraging our existing CPU and integrated software stack and integrated graphics IP. We have invested heavily to optimize our existing IP. The next step was to scale - for the high-end GPUs, we needed to scale over 1000x. As you saw from the Xe-HPC disclosures, thats our vision of scale. I think we have a very good strategy here, and that is not something by accident  Im taking my 20 years of industry experience, plus the two from being here at Intel, and applying it here. The way we approached the Xe design is to take measured steps  weve already proved with having silicon in hand. IC: One of the key things with high performance computing is to know your hardware. In order to extract every iota of performance, you have to know how big your caches are, where the latencies are, memory bandwidth, exact ALU structures, and ultimately build software that is rarely portable in order for it to be so performant. One of the features of oneAPI is to move away from this specifity by enabling software that can potentially work anywhere. How do you reconcile this desire for very specific optimizations and yet have a software package/SDK that is designed to help everybody? RK: Again, a great question. Our key goal with oneAPI was that no developer should be left behind. Within that, we have worked very hard on the interfaces for what we call ninja programmers  the low-level software developers that build the high-performance libraries that everyone else uses. We noticed that these ninja programmers have a strong non-linear impact on the ecosystem, so with our system programming layer inside oneAPI, and some of the abstractions available through oneAPI, will give these ninja programmers control of hardware resources at finer granularity. IC: Obviously the key operating systems and markets are going to be Windows, Linux, and to a certain extent, iOS. Weve seen software packages for HPC attack these operating systems very differently, so what is the oneAPI strategy here? RK: Great question. One of the things we agonised quite a bit over is how we make oneAPI support be very good on Windows. We recognize that the developer footprint with our PCs is a key strength, and we want to enable developers access to this stuff easily, whatever PC they pick up. So we put a lot of work in and youll see us supporting Windows and Linux in there. For operating systems beyond those, such as iOS, Android, and Chrome, its more whether you have on-device support or access to oneAPI service through cloud. This is where our DevCloud strategy, where developers can use oneAPI in the cloud, will come in. The other thing to say is that with oneAPI, the version we shipped today is the beginning of a long journey. Solving this problem and building the stacks, building the services, will take time. Many innovations are in the pipeline, and this is why God invented something called version 2 and 3! IC: Part of oneAPI, as you mentioned today on stage, is the ability to translate CUDA code to the oneAPI infrastructure. Youve been at one company in the past that previously attempted to provide translation tools for its hardware, with varying degrees of success. What can Intel do here differently to make it succeed at scale? RK: Great question. Portability of code between different parallel architectures has never been easy. There are often key fundamental differences between them, and a particular one is vector width. You cant take a program that is optimized for a smaller vector width and make it efficient on a machine that has a larger vector width without refactoring the code and all that. The Xe architecture is actually a narrower width machine - the variable vector width that we have and the ability to switch between SIMT mode and SIMD mode and combine them gives the software guys lots of tools to do more. Now having said that, the tools will take some time to mature. What we are seeing today is that were being more productive than prior attempts in the industry. We are also putting the software out ahead of the hardware for productive performance enablement.  Gaming on Xe IC: Turning to gaming solutions, because there is a lot of interest in how Intel is going to attack the gaming space: what weve seen today is a compute GPU based on chiplets. Moving from a monolithic graphics chip to a chiplet design is a tough paradigm to solve, so does working on chiplets help solve the multi-GPU issue on graphics? Is the future of graphics still consigned to single GPU, or should we expect multiple GPU scaling easier to manage? RK: Thats a great question. As you know, solving the multi-GPU problem is tough  it has been part of my pursuits for almost 15 years. Im excited, especially now, because multiple things are happening. As you know, the software aspect of multi-GPU was the biggest problem, and getting compatibility across applications was tough. So things like chiplets, and the amount of bandwidth now going on between GPUs, and other things makes it a more exciting task for the industry to take a second attempt. I think due to these continual advances, as well as new paradigms, we are getting closer to solving this problem. Chiplets and advancement of interconnect will be a great boost on the hardware side. The other big problem is software architecture. With many interesting cloud-based GPU efforts, I am optimistic that we will solve the software problems as well. IC: Narrowing the scope down to discrete gaming GPUs, how is Intel going to approach those driver stacks with Xe? RK: The system programming layer is a key difference between operating systems. The rest of the layers are largely OS independent. So we have a good development strategy here. IC: With the GPU team, particularly the GPU marketing team, weve seen Intel pull in industry talent from a wide variety of sources, such as competitors, analysts, and even some of my former media peers. Were seeing a strong commitment from Intel for this community, and building excitement for future Intel graphics solutions. To what degree, being in charge of graphics at Intel, are you pushing them ahead with that excitement, or are you telling them to reel it in? RK: We have incubated a discrete GPU business unit at Intel, run by Ari Rauch. There was a lot of excitement when we announced our graphics ambition, and that attracted a lot of GPU talent from the industry, including for our marketing efforts. They have been doing a good job building up connections with gaming community and leveraging their feedback. My guidance to them always is to reel it in until we have products! But we will be geared to enable developers and the wider community with our marketing outreach. IC: Have you discussed how the eventual discrete graphics launch is going to happen? RK: Not really. We are so much focused on execution right now. But I will tell you a funny story about Ponte Vecchio name. At Intel we have a policy for engineering code names to places or things you can find on a map. We have had too many lakes and I wanted to do bridges. Wanted to pick a place that I dont mind going to for a launch! Florence in Italy has some of best Gelato in the world. And I love Florence and the art and architecture there as well.  The Future of Gen Graphics IC: Is Xe anything like Gen at a fundamental level? RK: At the heart of Xe, you will find many Gen features. A big part of our decision making as we move forward is that the industry underestimates how long it takes to write a compiler for a new architecture. The Gen compiler has been with us, and has been continually improved, for years and years, so there is plenty of knowledge in there. It is impressive how much performance there is in Gen, especially in performance density. So we preserved lots of the good elements of Gen, but we had to get an order of magnitude increase in performance. The key for us is to leverage decades of our software investment  compilers, drivers, libraries etc. So, we maintained Gen features that help on software. IC: As Xe pushes on and products come out, will Intel continue to develop Gen as a separate architecture line? RK: All of our GPU teams are working on variants of the Xe architecture at the moment. We dont see a reason for Gen anymore  Xe-LP, our low powered variant, covers the market that Gen covered.  The Future and The Vision of Xe IC: You have been in the GPU space a long time. Is there anything definitive that you can say that Xe will bring to the table that hasnt been seen before? RK: In a word, vision. The exascale for everyone vision. Solving this requires fundamental disruptions in all layers of technology stack. And I think we have taken a big step towards that with Ponte Vecchio. When I look at our path ahead, I think about how we make that happen. That for me is the foundation of Xe, relative to how the rest of the industry is thinking about things, and all the problems weve discussed today: distributed memory problems, distributed computing problems, and computing at scale problems are all essential things in our vision. IC: Is there anything that the industry should know about Xe that it doesnt spend enough time thinking about? RK: Its a question of scale. I think the impact of 200 million PCs with integrated graphics, moving to Xe, with more performance and better efficiency, is something I dont see much of the industry appreciating. Intels reach and leverage means that a small change can make a big difference  here we are making a big change, and its going to have a knock-on effect. Its a big deal.  Many thanks to Raja and his team for their time.
December 24, 2019 9:30 AM - Analyzing Intels Discrete Xe-HPC Graphics Disclosure Ponte Vecchio Rambo Cache and Gelato.txt	 It has been a couple of weeks since Intel formally provided some high-level detail on its new discrete graphics strategy. The reason for the announcements and disclosures centered around Intels contract with the Department of Energy to build Aurora, an exascale supercomputer at the Argonne National Laboratory. The DoE and Argonne want developers clued into the hardware early, so when the supercomputer is deployed it can be used with as little learning time as possible. This means Intel had to flesh out some of its strategy, as well as lift the lid on its first announced discrete GPU product. Only time will tell if its a bridge too far, or over troubled water, but today we know it as Ponte Vecchio. Intel On Discrete Graphics: A Quick Recap While Intel has had a graphics portfolio for a couple of decades, those graphics solutions have been limited to embedded graphics and integrated graphics solutions. There was a slight attempt to move into the graphics space and play with the big boys, with the Intel i740, however that was a long time ago. Intels current graphics architecture, called Gen, is currently in use in hundreds of millions of mobile devices, and is present in a substantial number of desktop processors, even if a discrete GPU is being used instead. Intel has had high hopes for the graphics space before. Known as Larrabee, Intel attempted to engineer what was essentially x86 based graphics: using wide vector engines based on the same code path as Intel CPUs, the idea was to provide high-end graphics performance with the ease of programming in standard CPU code. While that product did actually run a number of graphics demos over the years, the hardware ended up being put to use in the high-performance computing market, where some developers saw the use of five-dozen 512-bit wide vector units absolutely fantastic for their simulations. This was the birth of AVX-512, which has lived on and now in Intels Xeon Scalable CPUs as well as consumer-grade Ice Lake laptop processors. The product that Larrabee ended up as, Xeon Phi, scored a number of supercomputer wins and originally the Xeon Phi Knights Hill product was destined to be put into Aurora in 2020. However the Xeon Phi program only lasted a few generations, with the final Knights Mill hardware not being widely deployed and subsequently put to pasture. Fast forward several years, and some management adjustments, and Intel has decided once again to enter the big graphics market. This time theyre going with something more conventional, something that looks more like a traditional graphics design. While the project started somewhere around three years ago, the big announcement that Intel was serious was when the company hired Raja Koduri, AMDs Chief Graphics Architect in December 2017, and then Jim Keller, renowned SoC Guru. Raja Koduris title, Chief Architect, and his two decade of experience in building graphics solutions at AMD and Apple showcased how serious Intel was with this. Since December 2017, Intel hasnt said much about its new graphics plans. Under Ari Rauch, notable marketing figures and analysts were hired to be part of the team. Intel disclosed at its Architecture Day in December 2018 that the graphics solutions it would offer would be a full top-to-bottom implementation, covering low power integrated graphics all the way to the high-end. At the time Intel stated there would be two main GPU microarchitectures, all building from the Xe architecture. Xe is meant to stand for eXascale for Everyone (rather than x^2.718), with the marketing message that Intel wants to put high-end performance and efficiency anywhere it can. As part of HPC DevCon, and Intels announcement with the DoE/Argonne, the veil was lifted, and we were told very slightly more than just the high level information. We were lucky enough to speak with Raja Koduri in a worldwide exclusive for the event, as his first official 1-on-1 interview since he joined Intel. It is worth a read and gives his perspective on a lot of ideas, as well as some of the decisions he has made. https://www.anandtech.com/show/15130/anandtech-exclusive-an-interview-with-intels-raja-koduri-about-xe This article is going to dive into Intels HPC DevCon disclosures about their graphics strategy. Here we are going to cover some of the blurb about Intels big plans, the new third microarchitecture in Xe called Xe-HPC, the new GPU product Ponte Vecchio, Intels new Memory Fabric, a breakdown of the oneAPI software stack as presented, and what all this means for the rest of Intels graphics platform. Exascale for Everyone Intel says that it is hard not to notice the insatiable demand for faster, more power efficient compute. Not only that, but certain people want that compute at scale, specifically at exascale. (It was disclosed at a high-performance supercomputing event, after all). For 2020 and beyond, Intel has designated this the Exascale era in computing, where no amount of compute is good enough for leading edge research. On top of this, Intel points to the number of connected devices in the market. A few years ago analysts were predicting 50 B IoT devices by 2020-2023, and in this presentation Intel is saying that by mid-2020 and beyond, there will be 100 billion devices that require some form of intelligent compute. The move to implementing AI, both in terms of training and inference, means that performance and computational ability have to be ubiquitous: beyond the network, beyond the mobile device, beyond the cloud. This is Intels vision of where the market is going to go. Intel splits this up into four specific categories of compute: Scalar, Vector, Matrix, and Spatial. This is certainly one blub part of the presentation I can say I agree with, having done high-performance programming in a previous career. Scalar compute, is the standard day-to-day compute that most systems run on. Vector compute is moving to parallel instructions, while Matrix compute is the talking point of the moment, with things like tensor cores and AI chips all working to optimize matrix throughput. The other part of the equation is spatial compute, which is derived from the FPGA market: for sparse compute that is complex and can be optimized with its own non-standard compute engine, then an FPGA solves it. Obviously Intels goal here is to cover each of these four corners with dedicated hardware: CPU for Scalar, GPU for Vector, AI for Matrix, and FPGA for Spatial. One of the issues with hardware, as you move from CPU to FPGA, is that it becomes more and more specialized. A CPU for example can do Scalar, Vector, Matrix, and Spatial, in a pinch. Its not going to be much good at some of those, and the power efficiency might be poor, but it can at least do them, as a launching point onto other things. With GPU, AI, and FPGA, these hardware specializations come with different amounts of complexity and a higher barrier to entry, but for those that can harness the hardware, large speed-ups are possible. In an effort to make compute more ubiquitous, Intel is pushing its oneAPI plan with a singular focal resource for all four types of hardware. More on this later. Intels Xe architecture will be the underpinning for all of its GPU hardware. It represents a new fundamental redesign from its current graphics architecture, called Gen, and pulls in what the company has learned from products such as Larrabee/Xeon Phi, Atom, Core, Gen, and even Itanium (!). Intel officially disclosed that it has its first Xe silicon back from the fabs, and has performed power cycling and basic functionality testing with it, keen to promote that it is an actual thing. So far the latest Gen graphics we have seen is the Gen11 graphics solution, which is on the newest Ice Lake consumer notebook processors. These are out in the market, ready to buy today, and feature performance 2x over the previous Gen9/Gen9.5 designs. (I should point out that Gen10 shipped in Cannon Lake but was disabled: this is the only graph ever where Ive seen Intel officially acknowledge the existence of Gen10 graphics.) We have seen diagrams, either potentially from Intel or elsewhere, showing Gen12. It would appear that Gen12 was just a holding name for Xe, and doesnt actually exist as an iteration of Gen. When we asked Raja Koduri about the future of Gen, he said that all the Gen developers are now working on Xe. There are still graphics updates to Gen, but the software developers that can be transferred to Xe have been already. If youre only going to read one thing today, then I want to skip ahead to Rajas final slide of what he presented at HPC DevCon. Putting a quite ambitious goal in front of the audience, it showed that Intel wants to be able to provide a 500x in performance per server node by the end of 2021 compared to the per-node performance in 2019. Now it is worth noting that this goal wasnt specifically nailed down: are we comparing vector code running in scalar mode on a single 6-core Xeon Bronze in 2019 to an optimized dual-socket with six Xe GPUs in 2021? 500x is a big bet to make, so I hope Intel is ready. In the next few pages, well cover Xe, Ponte Vecchio, oneAPI, and Aurora.
December 27, 2019 4:00 PM - AnandTech Year In Review 2019 Lots of CPUs.txt	 Throughout 2019, weve had quite the reverse of performance when it comes to the competitiveness of the modern performance-oriented desktop processor. This year weve seen AMD introduce its Zen 2 processor designs, offering up to 16 cores for mainstream use cases, and matching if not beating Intel in raw clock-for-clock performance. In a similar vein, AMDs Rome CPUs offer up to 64 cores per socket on the server side, where Intel can only offer 28 (or 56 in some configurations), while at the same time both consumer and server pushing PCIe 4.0 in preparation for upcoming GPUs and add-in accelerators. Intel has also launched a number of products, such as a couple of 5.0 GHz all-core CPUs, an unlocked 28-core CPU, Xeon 9200, and the next generation of server products. If youve been away from the CPU space this year, heres the Year in Review. January 2019 In early January we attended the annual CES trade show in Las Vegas. It was the start of many keynotes for AMDs CEO Dr. Lisa Su this year, which also included Computex, E3, Hot Chips, and various AMD partner events. The AMD Keynote at CES 2019: Looking Ahead Part of the keynote was that AMD was committed to delivering on its 7nm CPU platform later in the year, particuraly with its Zen 2 architecture, showing some very impressive benchmarks including an 8-core AMD vs 8-core Intel running a benchmark, scoring similarly, with the AMD at a quantifiably lower system power. We also got the first look at the 8-core Matisse package, along with predictions for a dual-die package offering up to 16-cores. At this point AMD was keeping quiet on core counts. Intels keynote by contrast was the first since the new interim CEO, Bob Swan, had taken over. It wouldnt be until later in the month that he would be confirmed as the full-time CEO, but unlike the previous CEO, he wasnt on stage. Instead, SVP Greg Bryant was up to talk about how Intel had launched a CPU offering a 2-core 5.0 GHz turbo (the i9-9900K, in October 2018), but also spoke about how Intel was committed to delivering 10nm in 2019 through its mobile Ice Lake platform, new 3D stacked Lakefield chip, and in 5G with its Snow Ridge design. Intels Keynote at CES 2019: 10nm, Ice Lake, Lakefield, Snow Ridge, Cascade Lake Most of this was based on Intels Architecture Day, which was only a few weeks prior in December 2018. At CES Intel also discussed its future for the server market with a brief coverage of Cascade Lake and also Intels AI options for that particular market. Also at hand was Intels Project Athena, which Intel had launched last year, with the idea to create a level of requirements for mobile devices to guarantee performance, battery life, and other features that Intel believes its high-end customers need. Also at CES, we learned of a new CPU that Intel didnt want many people to know about. The Core i9-9990XE was meant to be a special processor kept under wraps. With 14 cores all rated at 5.0 GHz, this super binned chip (according to partners) would only be available to Intels key partners through an auction only process. Not only that, but these CPUs would be sold to the partners without any warranty. End-users who would want these parts include high-frequency traders who want the best of everything and can amortize the costs. We ended up reviewing the hardware later in the year. Intel Core i9-9990XE : Up to 5.0 GHz, Auction Only As for CPU launches in January, Intel was the main player here. In order to boost the launch of its 9th Gen Core K-series processors from October 2018, the company launched its 9th Gen F-series hardware. F-series parts lacked integrated graphics, so we expected them to be cheaper, but they were listed on Intels pricing at the same price as the K parts that did have integrated graphics (later in the year the price did eventually change). Along with the F parts were KF parts that offered both overclocking functionality but no integrated graphics. What we also got to see in January 2019 was the Xeon W-3175X: Intels fully-unlocked 28-core beast of a processor. Intel shipped a number of press a full system to test this chip on, which included a rather beefy three-fan liquid cooler, an exhuberant $1500 motherboard, and a 1500W power supply. All in all, the test-kit for this CPU was $7000+, including the price of the processor, which was a cool $2999. The Intel Xeon W-3175X Review: 28 Unlocked Cores, $2999 As perhaps was to be expected, it knocked the socks off of any of the other processors weve ever tested, even putting some dual-socket configurations to shame with its high core count, high frequency, and perhaps not surprisingly, high power draw. Only two motherboard vendors even dared to build boards for it, ASUS and GIGABYTE, with only ASUS ready for launch day. EVGA eventually built a board too, when the product stack filled out a bit, but ultimately we were told by these companies that they expected the W-3175X to be sold mostly through system integrators. One of the final reviews to round off the month from us was our Cannon Lake review. Intel had officially shipped its first 10nm CPU products in December 2017, but they were no-where to be seen. Despite repeated requests to find out where this mythical CPU had gone, we found a few in China, with Lenovo producing a product for the education market. We pulled a few strings and got this Cannon Lake laptop in, and gave it a good teardown as well as a full architecture analysis. Intel's 10nm Cannon Lake and Core i3-8121U Deep Dive Review Cannon Lake, Intels first generation 10nm product, shipped in a dual core design with a disabled GPU (because it didnt work). It was more power hungry than the previous generation 15W laptop designs, and the parts that ended up in the market were undoubtedly salvaged to make it look like it was ready to go. Cannon Lake will forever remain a curio for CPU enthusiasts, and thats where it should stay. Right at the end of the month we had some good news. Intel had finally put the Itanic to bed. Intels Itanium product, initially launched in 2000, deployed the Itanium architecture for high-scale enterprise servers. The last generation of products, called Kittson, were released in 2017 but even then were only a frequency bump over the 2012 Poulson products. The only company still deploying Itanium was HP, and January 2019 marked the start of the End-of-Life cycle for this hardware. HP (and others) have until January 30th 2020 to make the last orders of CPUs, which will be shipped by July 29th 2021. But as it stands, with no roadmap and no more support, the Itanic has finally bubbled its last puff of air. Intel to Discontinue Itanium 9700 Kittson Processors, the Last of the Itaniums We also reviewed the AMD Athlon 200GE against the Intel Pentium G5400, in a battle of $60 hardware. The $60 CPU Question: AMD Athlon 200GE or Intel Pentium Gold G5400? A Review   February 2019 No CPU launches to report from February, although discussions about Intels ability to provide enough consumer CPUs into the market were slowly rising. Having only launched 9th Gen Core hardware for K/F/KF processors and not a full stack had people confused as to where Intel was going with the 9th Gen Core family. What we did at AnandTech was review a couple of special OEM-only parts from AMD. The Ryzen 5 2500X and Ryzen 3 2300X were technically announced in September, but due to their OEM-only status (and perhaps a bit of region-specificity going on), they could only be obtained in specific pre-built systems from HP, Lenovo, or other big vendors. We managed to get hold of the chips to compare their performance. The AMD Ryzen 5 2500X and Ryzen 3 2300X CPU Review It did strike us as odd as to why AMD doesnt widely distribute them. Potential reasons include that these cut down parts are limited in number, or AMD wanted to drive up the average selling price of its processors on the market  a tactic used by a lot of companies. At this point it was unclear, and as well read further, AMD did something a bit similar with the Ryzen 3000 family. March 2019 March was also a relatively quiet month, with nothing happening on the consumer processor front. There were lots of predictions about AMDs upcoming 7nm hardware, although nothing out of AMD itself. What AMD did finally put into retail was a high-speed version of its then current-generation Naples server hardware based on its Zen microarchitecture. The AMD EPYC 7371 was a 16-core part that pulled an extra 30 W (200 W vs. 170 W) but enabled a much higher base frequency (3.1 GHz vs. 2.3 GHz) and turbo frequency (3.8 GHz vs. 2.9 GHz) than any other EPYC on the market. Beyond that, AMD stated it had a 3.6 GHz all-core turbo. AMD Launches High-Frequency EPYC 7371 Processor While the product was announced in November 2018, it actually was made available in February 2019. While we didnt get to review the product directly, our friend Patrick Kennedy at ServeTheHome did do a review and said that it offered AMDs most competitive EPYC server product to date, easily replacing dual 8-core systems from Intel (a common configuration) with a single chip.  April 2019 While the leaks and rumors are still flying for AMDs Zen 2, April was all about Intel. Intels big announcement for the month was the launch of its new enterprise processors. The second generation of the Xeon Scalable line is called Cascade Lake, and the key updates here are not only the first round of hardened security updates for Spectre/Meltdown, but Intel also adjusted the core counts at specific price points to offer 25% more cores for the same price. The new processors also support Intels Optane DCPMM, which allows for up to 4.5 TB of memory per socket with specific SKUs. Intels Enterprise Extravaganza 2019: Launching Cascade Lake The socketed Cascade Lake hardware matched the previous Skylake in terms of peak core counts (up to 28), and IPC was almost equal as well, with Intel leveraging increases in core counts and small frequency lifts to give overall frequency improvements. We also saw the Xeon 9200 family being launched. This new line, sitting above the Cascade Lake Platinum 8200, essentially puts two 28-core silicon dies onto a single package. These chips arent socketed, and require to be soldered on board, but essentially act as a dual-socket processor in a single socket (albeit, at 1/3 of the link speed). Intel made these chips in order to offer high-density systems (224 cores in a 1U), but also to counter the upcoming the 64-core option from AMD. Hands on with the 56-core Xeon Platinum 9200 CPU: Intels Biggest CPU Package Ever Also on the books first was the Xeon D-1600 series, a network focused lower core count embedded chip for the edge market - a sort of baby brother to the D-2100 series. Intel Launches the Xeon D-1600 Family: Upgrades to Xeon D-1500 For consumers, the release cycle meant that Intel started putting 45W TDP versions of the 9th Gen Core family into the market, for both desktop and mobile. Finding them on desktop since the launch has been rather tricky (and seemingly location specific). Intel 9th Gen Core Processors: All the Desktop and Mobile 45W CPUs Announced  May 2019 At the beginning of May, we started to bid farewell to one of Intels most ambitious projects. Originally derived from Intels attempt to make an x86 graphics chip called Larrabee, Xeon Phi had been on the run as a many-core design focused in high-performance computing, namely due to its 72 AVX-512 units. The last product announcement for Xeon Phi had the 10nm product going into the Aurora supercomputer, but Aurora is now getting Intels new XeHPC GPU instead, called Ponte Vecchio. For Xeon Phi, the last generation that was given life was Knights Mill, which was almost identical to Knights Landing except for an optimization to go to AI. The Larrabee Chapter Closes: Intel's Final Xeon Phi Processors Now in EOL Xeon Phi was an interesting product that slowly became obsolete as Intel introduced AVX-512 on its main Xeon Scalable family of processors with dedicated AI instructions. With Knights Mill now being ushered into its End-of-Life program, the chapter on Larrabee has finally closed, with the company now set to launch a range of GPUs up and down the stack with its new Xe architecture. A side note on other Intel news: technically the company released its Xeon E-2200 family of CPUs in May as well. These are the Xeon analogues to the Core 9th Gen hardware, except with enterprise features like ECC and vPro by default. Intel Launches Xeon-E 2200 Series for Servers: 8 Cores, up to 5.0 GHz  The end of the May/beginning of June is our annual trip to Computex, and Dr. Lisa Su took to the stage for AMDs keynote. AMD for the first time gave everyone details about the next generation of Ryzen: Ryzen 3000 built on 7nm chiplets using Zen 2 microarchitecture. In the disclosures, AMD covered its new 8-core products, as well as its new flagship Ryzen 9 3900X with 12 cores using two chiplets, as had been theorized by the previous CES disclosure. Dr. Lisa Su at Computex 2019: AMD Keynote Live Blog Of course, everyone was asking two things: (a) whats new in Zen 2, and (b) wheres the 16-core ? More on that later in the year. Intel also had its usual keynote for Computex, with Greg Bryant taking the stage. Last year the company had an upset where it showed a 28-core 5.0 GHz CPU without mentioning it was overclocked (and seemed very cagy about it at the time). This year there were no such hangups, and the company announced the Core i9-9900KS. This 8-core 5.0 GHz CPU actually does 5.0 GHz on all cores out of the box, without the need for exotic cooling. Presented by Jon Carvill, now ex-Intel Intel Announces 8 Core i9-9900KS: Every Core at 5.0 GHz, All The Time Intel didnt state a release date at the time (it ended up being October), nor a price, with the price expected to be high. It sounded like obvious competition for AMDs 12-core and 16-core, so it did create quite a lot of traffic. At Computex, Intel also mentioned that its next generation of X-series processors (Cascade Lake-X) would be coming to the high-end desktop market later that year, as would its 10nm Ice Lake laptop processors. This would coincide with the official release of the Project Athena 1.0 laptop specifications. Computex 2019: Intel Keynote with Gregory Bryant The full SKU list for Ice Lake was released, covering 9W, 15W, and 28W hardware. To date (as of Dec 2019), we still have only seen the 15W hardware come to market, so we are expecting to see 28W and 9W parts in Q1 2020. The big uplift for the high-powered parts is in the graphics, offering up to 64 EUs of the new Gen11 graphics. We got a chance to benchmark an early preview in August. One other thing we found at Computex. We know from the previous year that AMD had done a deal to create a joint venture in China with a Chinese company called THATIC. This joint venture had a license to request changes to the Zen 1 architecture, which AMD would approve, and then they could be manufactured by the joint venture. These Hygon processors were built under license specifically for the Chinese market, and even today at the end of 2019 its rare to even see one, let alone touch it or test it. At Computex, we found one to look at. Spotted at Computex: Let Bygones be Bygons, with a Sugon Hygon Interestingly enough, we got the bottom right of the CPU translated. A literal translation is 'Using Cores to Calculate The Future' However, the second character used in the slogan is a homonym, which could be translated as: 'Using Passion to Calculate The Future' For a Chinese CPU, the synergistic phrasing is very poetic. Sugon, despite never having interacted with AnandTech before, asked us to take down our short article, and set us some odd documents that had nothing to do with AnandTech. The partner who had the CPU at the booth no longer speak to us. Thats how sensitive a Hygon is. As part of our Computex coverage, we asked AMD about these parts, about what is different  we were told if you test one and find something out, let us know and we might confirm it. Perhaps this is a good time as any to mention that we have some Hygon for testing, and found some very distinct differences to Zen 1. Stay tuned in early 2020 for our review. As part of ourComputex coverage, we were also given the opportunity to participate in a roundtable with AMD's CEO, Dr. Lisa Su, and quiz her on the shows announcements. The questions in this interview are varied, due to the nature of a roundtable discussion rarely staying on the same topic, but Lisa offered some good gems of info. All Ryzen: Q&A with AMD CEO Dr. Lisa Su  June 2019 Technically Computex rolled into June, but also at the beginning of June is E3. Normally AnandTech doesnt cover E3, however AMD decided to have a big 2-day NDA tech event just before the show started. This is usual before a big launch, and usually its a struggle for the company that does the tech event to keep everyone quiet in advance. Nonetheless, AMD took the time to go through the new Zen 2 microarchitecture in-depth, with key architects on hand to answer questions. So after the first full day of presentations, we only had 10 hours between the end of the presentations and embargo lift. I swore to some $deity that I was destined to have it written up in time. Heres my 7000-word analysis of AMDs Zen 2 microarchitecture. AMD Zen 2 Microarchitecture Analysis: Ryzen 3000 and EPYC Rome Aside from making the core wider, AMD had increased the micro-op cache size, introduced a new style of branch predictor, fully enabled dual AVX2 pipes, improved the load/store balancing, and a better efficiency by using 7nm for the cores but 14nm for the IO die. AMD claimed a direct +15% performance uplift from Zen1 to Zen2, which would put it directly in the firing line with Intels Skylake (now Coffee Lake) 14nm++ processors for raw clock-for-clock performance. Promising a release date of 7/7 (because 7nm), everything was heating up, to the point where people were now starting to ask about Threadripper. During the E3 event, AMD also announced that alongside the 8-core and 12-core hardware, it would be launching a 16-core counterpart, the Ryzen 9 3950X. This didnt really surprise anyone, except for its low TDP (only 105 W, 10 more than the 12-core), and the fact it was listed as having the highest single-thread frequency of any Ryzen 3000 chip. AMD listed it as coming September, but it ultimately came out at the end of November due to unprecedented demand of 7nm chiplets. AMD 16-Core Ryzen 9 3950X: Up to 4.7 GHz, 105W, Coming September To cap off the Computex/E3 announcements, Wendell from Level1Techs and I sat in our hotel in Los Angeles to talk about the announcements and our opinions of them, for his channel. Video is embedded below. Fireside Chat with Ian and Wendell: Ryzen 3000, Zen 2, Navi, Xeon W  The rest of the month of June was all for Intel. The company finally launched the rest of its 9th Gen Core processor stack, filled with non-K and non-F processors, as well as Pentiums and Celerons. There were no real surprises in frequencies or properties of these CPUs, almost mirroring what we saw on the 8th Generation hardware with small frequency adjustments and identical pricing. We also saw Intel launch the Xeon W-3200 series on the back of the new Mac Pro launch. The W-3200 family uses Intels LGA-3647 socket, and becomes a cheaper way to enable a 28-core workstation without going down the Xeon Scalable route. Intel also started to offer the Xeon W family with M versions for middle-tier memory support, allowing the maximum amount of memory to be supported go up from 1 TB to 2 TB (for an extra $3000). Intel Cascade Lake Xeon W-3200 Launched: Server Socket, 64 PCIe 3.0 lanes
April 19, 2021 7:00 AM - AI Funding Spree 300m for Groq 676m for SambaNova.txt	 The growth of AI has seen a resurgence in venture capital funding for silicon start-ups. Designing AI silicon for machine learning, both for training and inference, has become hot property in Silicon Valley, especially as machine learning compute and memory requirements are coalesced into tangible targets for this silicon to go after. A number of these companies are already shipping high performance processors to customers, and are looking for further funding to help support customers, expand the customer base, and develop next generation products until profitability happens, or the company is acquired. The two latest funding rounds for AI silicon were announced in this past week. Groq (Series C, $300m, Tensor Streaming Processor) When Groqs first product came onto the scene, detailed by the Microprocessor Report back in January 2020, it was described as the first PetaOP processor that eschewed traditional many-core designs and instead implemented a single VLIW-like core with hundreds of functional units. In this method, the data is subject to instruction flow, rather than instructions being reliant on data flow, saving time on synchronicity and decode overhead that many-core processors require. The end result is a product that implements 400,000 multiply-accumulate units, but the key marketing metric is the deterministic performance. Using this single core methodology, the Groq Chip 1 will take the same time to inference workload without any quality-of-service requirements. In speaking with CEO Jonathan Ross, Groqs TSP enables workloads that were previously unusable due to long tail quality of service performance degradation (i.e. worst case results take too long). This is especially important in analysis that requires batch size 1, such as video. The Groq ecosystem also means that distribution across many TSPs simply scales out inferences per second, with multiple Groq Chip 1 parts under the same algorithm all implementing the same deterministic performance. Jonathan stated to us, as the company has stated in the past, that Groq as a company was built on a compiler-first approach. Historically this sort of approach puts a lot of pressure on the compiler doing the optimization (such as Itanium and other VLIW processors), and often leads to concerns about the product as a whole. However, we were told that the team never touched any silicon design until six months into the software and compiler work, allowing the company to lock down the key aspects of the major ML frameworks before even designing the silicon. As part of its funding efforts, Groq reached out to us for a company update. All of Groqs hardware and software work to date has been achieved through two rounds of VC funding, totaling $67.3m, with about $50m being used so far. In that capital they have designed, built, and deployed the Groq Chip 1 TSP to almost a dozen customers, including the audio/visual industry, datacenter, and government labs. The second generation product is also well underway. This latest Series C funding round of $300m, led by Tiger Global Management and D1 Capital, will allow the company to expand from 120 people to 250 by the end of the year, support current and future customers with bigger teams, and enable a progressive roadmap. Groq stated in our briefing that its second generation product will build on its unique design points, offering alternatives for customers that were interested in the Groq Chip 1 but have other requirements for their workloads. Each generation of Groqs TSP, according to the company, will have half a dozen unique selling points in the market (some public, some not), with one goal at least to displace as many GPUs as possible with a single TSP in order to give customers the best TCO. SambaNova (Series D, $676m, Cardinal AI) The second company this week is SambaNova, whose Series D funding is a staggering $676 million, led by SoftBanks Vision Fund 2, with new investors Temasek and GIC, joining existing backers such as BlackRock, Intel Capital, GV (formerly Google Ventures) and others. To date SambaNova has generated over $1.1 billion in investment, enabling a $5 billion valuation. SambaNovas entry into the AI silicon space is with its Cardinal AI processor. Rather than focusing on machine learning inference workloads, such as trying to identify animals with a known algorithm, the Cardinal AI processor is one of the few dedicated implementations to provide peak training performance. Training is a substantially harder problem than inference, especially as training algorithms are constantly changing and requirements for the biggest datasets are seemingly ever increasing. The Cardinal AI processor has already featured on AnandTech, when SambaNova announced its eight-socket solution known as the DataScale SN10-8R. In a quarter rack design, an EPYC Rome x86 system is paired with eight Cardinal processors backed by 12 terabytes of DDR4-3200 memory, and SambaNova can scale this to a half-rack or full-rack solution. Each Cardinal AI processor has 1.5 TB of DDR4, with six memory channels for 153 GB/s bandwidth per processor. Within each eight socket configuration, the chips are connected in an all-to-all fashion with 64x PCIe 4.0 lanes to dedicated switching network silicon (like an NVSwitch) for 128 GB/s in each direction to all other processors. The protocol being used over PCIe is custom to SambaNova. The switches also enable system-to-system connectivity that allows SambaNova to scale as required. SambaNova is quoting that a dual-rack solution will outperform an equivalent DGX-A100 deployment by 40% and will be at a much lower power, or enable companies to coalesce a 16-rack 1024 V100 deployment into a single quarter-rack DataScale system. SambaNovas customers are looking for a mix of private and public cloud options, and as a result the flagship offering is a Dataflow-as-a-Service product line allowing customers a subscription model for AI initiatives without purchasing the hardware outright. These subscription systems can be deployed internally to the company with the subscription, and be managed remotely by SambaNova. The company cites that TensorFlow or PyTorch workloads can be rebuilt using SambaNovas compiler in less than an hour. SambaNova has not given many more details on its architecture as yet, however they do state that SambaNova can enable AI training that requires large image datasets (50000x50000 pixel images, for example) for astronomy, oil-and-gas, or medical imaging that often require losing resolution/accuracy for other platforms. The Cardinal AI processor can also perform in-the-loop training allowing for model reclassification and optimization of inference-with-training workloads on the fly by enabling a heterogeneous zerocopy-style solution  GPUs instead have to memory dump and/or kernel switch, which can be a significant part of any utilization analysis. The company has now been through four rounds of funding: Series A, $56m, led by Walden International and Google Ventures Series B, $150m, led by Intel Capital Series C, $250m, led by BlackRock Series D, $676m, led by SoftBank This puts SambaNova almost at the top of AI chip funding with $1132m, just behind Horizon Robotics ($1600m), but ahead of GraphCore($460m), Groq ($367m), Nuvia($293m, acquired by Qualcomm),Cambricon($200m), andCerebras($112m). Related Reading SambaNova Breaks Cover: $450M AI Startup with 8-Socket AI Training Solutions (and more) Cerebras Wafer Scale Engine News: DoE Supercomputer Gets 400,000 AI Cores NVIDIA Unveils Grace: A High-Performance Arm Server CPU For Use In Big AI Systems Intels New eASIC N5X Series: Hardened Security for 5G and AI Through Structured ASICs Qualcomm's Cloud AI 100 Now Sampling: Up to 400TOPs at 75W TSMC and Graphcore Prepare for AI Acceleration on 3nm Intel Whittles Down AI Portfolio, Folds Nervana in Favor of Habana Samsung Kicks Off Mass Production of AI Chip for Baidu: 260 TOPS at 150 W
August 20, 2021 9:00 AM - An AnandTech Interview with Jim Anderson CEO of Lattice Semiconductor.txt	 In our coverage of the semiconductor space, we typically think of two main vectors of hardware  the CPU and the GPU. Beyond that, we look at FPGAs, microcontrollers, and this decade is bringing the advent of the dedicated AI processor. What ties all of these products together is actually the FPGA - a field programmable gate array that allows a skilled technician to essentially build a custom circuit out of configurable gates. This means an FPGA can be used to design and simulate a full CPU or GPU, but also an FPGA offers a reconfigurable way to offer optimized compute power that adapts to the needs of its users without the cost of millions or tens of millions to design dedicated silicon. One of the first FPGA companies on the market was Lattice Semiconductor, which now focuses on small power efficient FPGA designs that end up in everything from consumer devices to servers. Weve been loosely following Lattice for a number of years, however three years ago the company went through a bit of a change. It hired Jim Anderson, the then AMD SVP of Computing and Graphics who had overseen the launch of Ryzen, the brand of processors that has re-energized the company from near bankruptcy to a number of years of extended market share growth and profitability. Jim and I met frequently at AMD events and we spoke at depth at the state of the consumer product landscape as well as how the semiconductor space was evolving. When it was announced he took the role of CEO at Lattice, I was a little taken aback, but glad that he had found a new challenge the complemented his background in semiconductor design and expertise. Over those three years at Lattice, Jim has initiated a cultural shift that is playing out in the company roadmaps  new products, a more agile approach, and a need to focus on enabling machine learning at every part of its product stack. The recent financial disclosures at Lattice show an increasing demand for its hardware, as well as the company making strides to double its addressable market over the next five years. I thought this would be a good time to reconnect with Jim to find out exactly what hes doing at Lattice to earmark the next generation of growth at this foundational FPGA company. Jim Anderson CEO, Lattice Semiconductor Dr. Ian Cutress AnandTech From a career standpoint, Jim has a lot of experience. Academically he holds a MS in EE and CS from MIT and an MBA from MIT, and in his career he spent 8 years at Intel as a CPU architect on Xeon and Itanium along with strategic planning, 8 years at LSI in strategic planning and marketing of network components, a year at Axxia/Intel as the GM of Networking, and then 3 years at AMD leading up Ryzen. Now he is three years into the role of CEO at Lattice.  Ian Cutress: Youve now been CEO of Lattice for three years, and coming from the position of SVP of AMDs Computing and Graphics business group, what made you make the jump from consumer hardware to power-efficient FPGAs? Jim Anderson: Well I loved it at AMD. I was super happy there - I love the people at AMD. Great people, really innovative, very determined. I'm a product guy - I always have passion about the product, and in particular that product we were launching that day back in Italy, I was always super excited about. I love the products. But when Lattice reached out to me, it was kind of hard to pass up the opportunity, because Lattice is a company that's one of the original founding FPGA companies from the early 80s. It's a company that's been around for about 40 years, it's got a great history of innovation, and its kind of a unique place thatinnovates and innovates around small and really power efficient FPGAs. Those devices go into all sorts of applications across many different markets. It is a great company. I felt like if I joined Lattice that I could help it get on a stronger path moving forward, help it build better products, and build products more quickly. So that's kind of what attracted me to Lattice and the last three years have been a lot of fun. We've completely rebuilt the product line at Lattice, we're on a great trajectory, and actually, I'm even more excited about the years to come. We have some really great products on the roadmap.  IC: Most users who know about Lattice understand that it's an FPGA company, but when most of us speak about FPGAs, we refer to the big names in the industry: Xilinx and Altera (which is now part of Intel). Can you talk about how Lattice positions itself in the market compared to the big names, and why that matters? JA: Yeah, absolutely. First of all let me point out that if you look at unit volume size, actually Lattice is the highest unit volume manufacturer of FPGAs. We're the biggest by unit volume. Where we specialize is in small size, power efficient, and very, very easy to use FPGAs. Because they're really power efficient, and they're small size, they can go into all sorts of different applications that really large FPGA just can't go in. So either from a physical space constraint or a power constraint, our devices can reach all sorts of applications that our competitors can't, and so you'll find us in industrial, all sorts of industrial IoT applications, industrial automation, or robotics. You'll find us in communications, computing, consumer devices, and automotive electronics as well. So we really specialise in the small power efficient part of the market. Those two competitors that you mentioned, which are traditional competitors, they really focus on the very large, very high power, complex FPGAs. We focus on the small size and power efficient. You would be amazed at the number of applications we go into. Whenever I meet with a new customer, I'm always amazed at some of the new innovative crazy applications they're using our chips in. To give you a sense of the size, this is one of our smallest FPGAs on the end of tweezers here. I'm showing it on the end of the tweezers, and this is about 1.4 millimeters square. Obviously we make some bigger devices as well, but devices this size can go into applications that other chips cant, where there are either size or power constraints. Since you mentioned the last time we met three years ago, if we compare our FPGA to this Threadripper chip from AMD, [this is what it looks like]. So I've now moved to the total other end of the spectrum! So myself and the Lattice team are really innovating at the other end of the spectrum: small, power efficient, easy to use, and like I said, important to lots of different applications. IC: Your PR team sent a strip of those tiny FPGAs to me. When I opened it, I wondered what they were! I couldnt see them - I asked myself why have they sent me a bit of plastic? I had to get the zoom macro on my smartphone to kind of see them, and even then I still can't read what's printed on it. JA: When I first started looking more closely at Lattice before I joined, that was one of the things that I found really fascinating. I spent a lot of my career working at the other end of the spectrum on big high performance CPUs and things like that, so to work at the other end of the spectrum on the smaller size, power efficient FPGAs I thought was pretty fascinating. Well, I'm glad you got the samples!  These @latticesemi FPGAs that just arrived in the mail are smaller than 1mm x 1mm. I can hardly see them without a magnifying glass of some sort. If I try and bite into these, they'll get consumed. Maybe I can use them as sprinkles. pic.twitter.com/BHbxrH0HbQ  .   (@IanCutress) July 28, 2021   IC: You've been CEO at Lattice for three years now, and I think most of our audience may recognize you from the three years you spent at AMD in charge of the client division as Ryzen was first launched. I've looked through your history and you've got experience as a CPU architect at Intel, strategic planning and marketing on networking silicon for LSI, and your academic background is a Masters in EE from MIT. What ends up drawing you from architecture, to planning, to networking, to processors and graphics, and now to Lattice? JA: The way that you said that, it makes it sound like I can't hold down a job, right?! [laughs] I would say the common thread through my career is always that I've worked in the semiconductor industry - but you're right, and I've worked in a number of different functions. I started as a CPU architect at Intel working on Xeon and Itanium chips. But Ive worked on multicore DSP chips, really complicated network processors used in communications, lots of ASICs, CPUs for client devices, graphics, and now FPGAs. So the common thread is always within the semiconductor industry. I think that, for me, the semi industry is really exciting, because it's basically the foundational layer of the entire tech industry. It is the fundamental substrate of the rest of the industry and it's pervasive in our lives. Anytime you touch an electronic device, you're touching the semiconductor industry in some way. So I'm just fascinated by the whole spectrum of devices built in the semi industry, and so now the opportunity to work on FPGAs at Lattice. The semi industry is great, and I like all parts of it.  IC: Every time I seem to mention FPGAs in my work, comments always arise as to 'why not just build an ASIC?'. Can you explain why Lattice's customers, or anyone that uses an FPGA, chooses to do so over dedicated hardware or a software solution? JA: There are both technical reasons and there are also economic reasons. On the technical side, a lot of times our customers are trying to innovate on their system designs. They're trying to figure out how they can add new features or new capabilities that differentiate them in the marketplace. A lot of times [the solution is] an FPGA, which is incredibly customizable and adaptable. It can be a key part of the system that allows the customer to really customize and adapt their system. Then one of the benefits of the FPGA is not just that you can customize it for exactly what you need, versus say a standard product, but you can reprogram it over the lifetime of the system. So let's say, as your market changes or as you want to incorporate new features, [it can be updated]. Actually, we have a lot of customers that run artificial intelligence algorithms on our FPGAs. Those algorithms are constantly evolving, and so the fact that they could just reprogram the FPGA as the evolution of the AI algorithm changes, that's a big benefit. It provides future proofing for the platform, and so that's a big reason why customers design in FPGAs. Another reason [to use an FPGA] that you mentioned is ASICs, or rather the difficulty in scaling them. If you're going to build a truly custom chip for your application, that takes 18 to 24 months minimum to go build that chip, right? From the initial architectural concept, to when you have something that's production ready. Your needs could have easily changed over those two years that it takes to develop that chip - whereas with an FPGA, you customise it right away for exactly what you need. If the needs change over those one to two years, no problem - you just reprogram the thing. Our chips are really power efficient and size optimized. A lot of times there's not a big advantage to a custom chip, in terms of power or cost. Then kind of on the economic side, look at the expense of doing a fully custom chip today. You know that expense has escalated incredibly over the last 20+ years I've been in the industry. It used to be inexpensive to create your own custom chip, you know  it is way more expensive now, both in terms of development effort, mask costs, etc. So it's really seldom that it actually makes economic sense, especially with the type of FPGAs we have, and the cost points that we can hit, that there's really no economic benefit as well. So for all those reasons, if you look at the history of the FPGA industry, it's grown as fast as the semiconductor industry in total, or actually faster in many years and in many cases. So that's why you see, over the past 40 years, FPGAs have continued to grow, and that the market is very healthy as customers continue to adopt that and in all sorts of applications.  IC: Where can most end-users expect to find a Lattice-based product in their lives today? JA: I think if you're using any electronic device, [youre probably interacting] with a Lattice device somehow during your daily life, either directly, or maybe indirectly. For instance, a data centre or some of the industrial stuff that we do, for instance, in servers. Now, if you look in servers, either enterprise class servers or servers that are in big hyperscale data centres, obviously end users are accessing data and data centres such as big hyperscale data centres all the time. In those you would find a Lattice chip. In almost every new server today, way over 80% of servers have at least one Lattice chip, if not more. Those chips are doing control management of the platform and security as well. You're now starting to find Lattice FPGAs in client computing devices, where we're providing a number of kinds of new functionalities. Also in communications infrastructure, so either wireline or wireless infrastructure like the 5G infrastructure, lots of devices use Lattice. Then all sorts of consumer electronics, high-end audio systems, home automation systems, automotive electronics - I'm trying to make sure I run through the whole list here! But I think that gives you a sense of it all. Ultimately we have over 9000 customers, and if you look over the last four years, we've shipped about a billion devices. If you think about that, about a billion lattice chips in all sorts of applications, youll find us all over the place.  IC: When I spoke to Esam Elashmawi a few weeks ago, he explained to me that Lattice is having an ever present increase in the server market, having silicon in around 20% of servers a few years back to around 80% today. When I review the hardware, Ive always noticed the Lattice logo there, but I didn't realize how expansive Lattice's growth in that market has been. Why exactly are we seeing Lattice silicon becoming a vital part of the enterprise motherboard market? JA: Yeah, great question! It's definitely been a big growth area for us. Our position in servers has grown considerably over the last few years, and we expect it to continue to grow. As Elam said, if you go back two or three years, about 20% of servers shipped with a Lattice piece of silicon. A few years ago they were doing more basic tasks, sort of power management or basic control functions on the server platform. Now, if you zoom forward to today, over 80% of servers today in the latest generation that's in volume, all ship with at least one piece of Lattice silicon doing more control and management of the server platform. Moving forward, we're doing more security functionality as well. It used to be that, years ago, people were just worried about security more at the software layer. But now, there's concern of security all the way down to the hardware platform. So we have specific Lattice devices that are designed to provide what's called platform security or platform resilience. So what our devices do is they go to and check to make sure for instance, as your servers booting up, that the hardware itself hasn't been corrupted, or also the firmware hasn't been corrupted. So it will look at the firmware version before the system starts up, verify that it's the correct firmware version, and that nobody's corrupted or loaded the wrong piece of firmware. Then if it detects a wrong piece of firmware, because we have a golden copy stored within the solid state memory within the Lattice chip, it will actually swap and repair that firmware. That's just one example of the kind of new capabilities that we're bringing to a server. But if you look at today's servers, over 80% ship with Lattice silicon. In the next generation that's just starting to ramp, our attach rate will actually start to exceed 1x, meaning that on average server platforms will have multiple Lattice chips used. So when that happens we'll actually be shipping more chips into the server market than the total number of servers that are shipped! Also, our ASPs (average selling price) continues to grow upwards, because we continue to bring more value to each server generation. So this has been a great growth area for us. Based on the multi-generational discussions we're having with our customers, we expect it to continue to be a good growth area. By the way, I should mention, because I figured you might ask based on my history, is that we are CPU agnostic. Lattice supports both Intel and AMD platforms, and in fact, we support ARM based servers today. That's actually an advantage for our customers, and that's important that we're able to service all architectures, so we're totally agnostic.  IC: I was going to ask then - do you make very specific versions of your FPGAs for AMD, for Intel, or even for Ampere? Or do they just take the ones off the shelf and optimize it how they want? JA: That's exactly right, [the can take the ones off the shelf] and that's the beauty of it. That one FPGA design can then be customized by server OEM or even each server customer, because they want it to have their unique value addition. Our server customers are putting their own custom customization into the FPGA, and that helps them customize or differentiate their platform, but the beauty is that we can use a single device to service that.  IC: One of the things I keep noticing on Lattice financial calls is a mention of a culture shift internally around the 2018 timeframe, which just happens to be about the time you took the role of CEO! Can you explain what was in place at that time, and how you've adjusted Lattice to be to what looks like path to sustained growth / what that sort of culture shift looks like? I think you alluded to aligning the product roadmap, but does it go beyond that? JA: I think it goes beyond that. I think there has been a pretty significant cultural change at Lattice over the last few years. When I joined in September of 2018, shortly after that, I brought on a new leadership team. We went and recruited for all the kind of key functions like engineering, sales, marketing, and supply chain. [We recruited] deep industry veterans, people that have been in the FPGA industry not just for a few years, but for decades and have multiple decades of experience. So we rebuilt the leadership team with industry experts, and then we did make a pretty big cultural shift at the company. I will say there were a couple of cultural attributes that Lattice has always had, that we definitely have encouraged and we've maintained. For example, Lattice has always been very customer centric, and in fact if you ask our customers they'll tell you one of the reasons they love working with Lattice is because we've always been attuned to our customers. So we're definitely continuing to encourage that, but also Lattice is a really collaborative place. We collaborative internally, and the groups work really well together, but we also collaborate really well with our customers. We're encouraging those cultural attributes, but we did encourage or bring some new attributes - one of them is around speed and agility, which I feel like in the tech industry is really important. It can become a really important competitive advantage. I think that speed and agility matter a lot more than the size of the company, or [compared to] the size of the resources. I think the ability to go fast, get products to market fast, and to adapt quickly is absolutely a competitive advantage. That's something we've really encouraged since day one, since I've joined. I think you can see that play out in our product roadmap - if you look at the number of new products we brought out over the last two years versus previous Lattice history, we've tripled the rate of new products that we're bringing to market. Our cadence is three times faster than it used to be. That's great for our customers - they love it because they've got new and fresh products coming out all the time from Lattice. So I think that's a marker of that cultural shift towards more speed and agility. Then the other one I would say that we really encouraged was innovation. We always say innovation, and everybody says that, but I would say it's about being bold about the innovative steps that youre willing to take. We are being much more bold about the future innovation that we're driving. So I think that's something we've really been encouraging the team to do - be much more bold in terms of their thoughts about where we drive the technology and the product roadmap. The cultural shift has been a big part of it when I think about the progress that weve made. But the other piece that I would say, that we sort of mentioned early on like I said, is that you know I'm a product guy. At the end of the day our customers, the one thing they care most about is our products. So right away, in the first six months after I joined, we totally rebuilt the product roadmap and rebuilt it for not just the next year or two, but the next 5+ years. Its completely rebuilt, really jacked up where we were headed in terms of the performance and the capabilities and the features. We're now really starting to see the benefits of that in this year, and in the years to come. IC: I did want to talk about product roadmaps because you've kind of been slowly announcing how you've realigned your products over the last few months. Going from one or two products a year, a very linear cadence, there are now multiple derivatives in a parallel design flow. You've also said this helps double the potential addressable markets from $3 billion to $6 billion. Aside from just raw revenue dollars, what's the goal here? How does an FPGA company innovate? Is it more about explicit customer demand, or is it more pathfinding? JA: It's always customer centric, our innovation. We always say this inside of Lattice - our innovation is always customer centric and market application centric. All the innovation that we're doing is to try to solve a customer problem or enable a new application - that's really what our innovation is directed to. We did make a big change over the last two to three years in how we do that innovation. From a product roadmap perspective, we used to be very serial in how we build products. We would build one productwith one architecture, and then the next product would have a new architecture. We were very sequential. What we've done now is take a platform approach, both from the hardware and the software perspective, and design each FPGA platform from the very beginning with the mind that we would build multiple derivatives. We would also be building versions of that FPGA platform that were say optimized around particular applications or customer needs. So that platform approach has allowed us to dramatically speed up the number of new products that we can bring to market, and then it has allowed us to do new products that are more optimized for particular applications. One example of that would be security. Were now developing FPGAs that are optimized for doing platform security. The other examples would be FPGAs that are more optimized for doing artificial intelligence processing at the edge of the network - inference processing in edge applications. So that platform approach has helped us to increase the number of products, but also helped us really tune our products for particular applications, especially the really big growth applications, or the applications that meet the needs of our big customers. IC: Does optimization in that sense always mean taking some of the logic gates away and building, say, a hardened crypto accelerator, or a hardened AI accelerator? Does it go beyond that? JA: It can be that, but we may also tune [the frequency and bandwidth of] the FPGA fabric architecture, or the implementation itself, to be better aligned to a particular application or needs. We use a variety of different techniques. IC: That's through profiling customer workflows? You get together with customers to understand what they're doing, and what they're trying to solve? JA: Exactly! We spend a lot of time with our customers making sure we understand their needs, not just for the next 12 months, but for the next 3+ years. We then map those customer needs, or application needs, back to the particular FPGA architectures [that they are interested in].  IC: You mentioned earlier about client computing - you're very much in the sense of promoting FPGAs to help with AI accelerated camera analysis, people's webcams on devices, that sort of thing. Where exactly are we expecting to see Lattice in client computing in that regard? JA: Client computing has been a growth area for us over the last few years - our client computing falls within our computing and communications segment, and that segment has been growing really well. It grew double digits last year, and it grew another 15% year over year in the most recent quarter so. Client computing has been a big contributor to that. There were a couple of new significant customer platforms that began ramping into production last year, and then are ramping into full production this year. And then we're engaged with a number of OEMs, and new client computing platforms. When we say client computing, we're talking about basically PCs and tablets, those types of devices. But some of the interesting applications that we're getting used in are things like artificial intelligence. For instance, let's say I'm looking at my laptop, I'm doing work, but I turn away and I look away - maybe I'm having a conversation with somebody for a few minutes and I'm not looking at the laptop. Our device will be analyzing the video signal, and it will detect that you're still in front of the laptop, but that you've looked away. It will detect that using AI algorithms, and then the laptop screen will dim to save the battery power. Then as soon as you look back, it'll bring the screen back up. That may seem like not a big deal, but when you start to add up the power savings, the screen on the laptop burns a tremendous amount of power. If you save all of that battery life over the course of a full day, you can save quite a bit, and drive quite a bit of battery efficiency. The other thing that our devices or FPGAs can do is watch the video input, and let's say somebody comes up behind you and they are shoulder surfing by looking over your shoulder at what you're working on. It can detect that there is somebody else in the frame, and then put a little red dot on screen or whatever to notify you that somebody is behind you, watching what you're doing. There's a bunch of other examples as well, but we can do this because our devices are small and really power efficient - we can do this at incredibly low levels of power. Then another example would be security, some of that same security that we talked about on server devices is also applicable to client devices. So that's another potential application - but you know, one of the things we really like about this market is that if you look at the server space, which we already talked about, we've done really well and really proliferated Lattice across the server space. But the great thing about the client computing market is it's 20 times larger in terms of unit size. There are 20 clients for every one server, and so to us that's just a huge TAM opportunity to bring Lattice devices into that market and enable all new sorts of functionality and capability for end users. IC: So the demonstration about looking away and having people over your shoulder, there is one company who has demonstrated that today, and that would be Intel. Are you working with Intel on that? Can I make that connection? JA: [laughs] Well, what I would say is that we would view Intel in the PC segment as a strategic partner. We would certainly work with them on enabling those types of experiences. What I will point out is that our devices can do that functionality at ridiculously low power, so we have a big power efficiency advantage, and on a laptop device as you know power efficiency is a premium. So our devices can do that at a level that I don't think anyone else in the industry can. We would view Intel as a potential platform partner there along with everybody else in the ecosystem. IC: That's a very crafty way of saying no. I love it! IC: It makes me question with that sort of topic whether you would actually be working with the platform developer, or perhaps that would be more sort of an OEM enablement strategy - maybe it's something that the OEM wants to do, on top of a platform provided by a higher level partner? JA: We would work with both - we do two things. We'd work directly with the OEM that's building the platform - we'd certainly have engaged with them. But then we'd also be working with the ecosystem as well to make sure that our parts are interoperable and work with the rest of the ecosystem partners, so we'd been doing both in parallel.  IC: One of the things with FPGAs - I get a lot of feedback for is that they're just hard to develop for. You need to know how to use them before you use them, which sounds like the wrong way to learn how to code! What exactly is Lattice doing to kind of ease that transition for people who may understand software, but are kind of new to the hardware? JA: This is really one of our main mantras at Lattice. [We want] to make the use of our devices as easy as possible. So this is something we've driven a tremendous amount of improvement, just over the last few years, to make the devices very easy to use. This is not just for people that are familiar with FPGAs, but for developers that may have never used an FPGA in their entire career. So one of the big things that we've been doing over the last two to three years is developing what we call application specific software solution stacks. So think about these as pre-built tools and libraries that a customer can take off the shelf and use that to abstract out the complexity of an FPGA. It allows the customer to use our devices at a level of abstraction that they're comfortable with. This is something we've been investing a tremendous amount of effort into, and we built out a pretty significant portfolio of these applications solutions and they make it very easy for the customer to adopt our devices into their systems so they can get the innovation. They can get access to the innovation that we're driving and get that easily, but also get to market much more quickly. Also, even if a customer is familiar with FPGAs, it can help them switch from a competitor's FPGA to our FPGA, so it can ease that switching or transition. We're building out a portfolio of these stacks, and we brought four of them to market to date. The first one that we developed was called SenseAI. And that's a software stack that's specifically for artificial intelligence, for doing inference processing especially in edge applications. Whether those are consumer applications or industrial applications, it really helps enable inference processing on our devices at the edge of the network. That was the first one. The second one was around embedded vision processing, and enabling embedded vision processing on our devices. The third one and we kind of touched on this earlier was actually around security and platform hardware security, making it really easy to use our devices for doing hardware security. Then the fourth one, which we just launched in May, was around factory automation. This is making it easy to design Lattice devices for doing all sorts of industrial automation tasks, as well as robotics. So each one of these are, like I said, a pre-built solution that the customer can take and use as is, orif they want to customize it, they can do that. But it has really lowered the barrier, or reduced the effort necessary, to design devices into systems. We're going to continue to build out, we've got additional software stacks on the roadmap, and we'll continue to build out a wider portfolio here. IC: To explain it my way, that would be like having precompiled libraries that manage the FPGA, and all you do is you call the library and the functions they're in and it does it automatically? JA: That would be a good analogy!  IC: So we spoke about the communications and compute segment earlier - Lattice's biggest market now is communications and compute, and it's not often we get a chance to direct users to think about what telecommunication companies (telcos) need out of their silicon. What makes Lattice FPGAs the right fit for telcos, and how are they using them? JA: It's not just Lattice FPGAs, but the communications industry has a long history of using FPGAs in all sorts of applications. Its not just wireless applications, which you're asking about, but wireline as well. You will find FPGAs used in all sorts of different places in communications. In wireless applications, like the new 5G infrastructure that's being built out, you find Lattice devices in the control plane. If you look at a base station, which is basically processing at the bottom of the tower, and then the antenna at the top of the tower, you find Lattice devices in the baseband unit at the bottom of the tower, but also in the radio heads that are at the top of the tower, and the towers which are transmitting the signals. What we're doing there is controlling the management of the system, the power management, and then some security functionality moving forward as well. A lot of times the reason is that in communications, especially wireless infrastructure, that FPGAs get used as we kind of think of 5G simplistically as one monolithic standard worldwide. That's not the case - there are umbrellas of standards that exist, and each region has different frequencies, different frequency bands, different local customizations, and then also the 5G or any wireless standard is evolving over time as well. A wireless infrastructure OEM may start to design a system before the 5G standard is fully completed, so they build FPGAs in because they need the ability to customize for, let's say, different geographies. They can reprogram the FPGA to adapt the system for different geographies with different unique requirements, rather than building a dedicated hardware system for each geography. Or if the standard evolves over time, there are new capabilities that they have to integrate into the system. They can reprogram the FPGA to adapt and include that new functionality in the system. So it's a combination of the flexibility, as well as the adaptability, and some future proofing. That's why you see FPGAs get used. The 5G infrastructure has been a great growth area for us - we were really early in the 5G build out worldwide. You know, if you try to get a 5G signal, a true 5G signal, you know that there's a lot of build out that needs to happen, especially in North America and Europe. We're still early in that, and it is a great growth area for Lattice. IC: You mentioned the control plane in 5G, and there's also the data plane - the telcos use FPGAs initially in the data plane to do all the data compute, and then replace it with their own ASIC over time. In the control plane, FPGAs tend not to be replaced - that kind of sustains your growth in that market? Is that fair to say? JA: Yeah, that's exactly right. We're usually designed into the control plane. In the data plane, where the data is flowing through, that's where the primary data stream is flowing through the system. When a system is initially launched, the large FPGAs designed by our two traditional competitors may get used initially in those systems, and they may get replaced by ASICs because those very large FPGAs are extremely high power, and they're truly expensive too. So they may get replaced by ASICs. Now our FPGAs, which are on the control plane, are smaller, power efficient, and generally don't get replaced by ASICs because, for all the reasons we talked about earlier, there's just not a big economic reason to change them, or from a technical standpoint, there's not a lot of power efficiency savings either.  IC: With those large pieces of silicon from the main competitors, we see them be aggressive on packaging and using the latest process technology node. I assume these small iCE FPGAs aren't made on some 7 nm process are they? Otherwise, I wouldn't be able to see them, I think! JA: [laughs] Well, I'm not sure the size would be that terribly different!A lot of times on a device like the iCE, the size of the silicon may be IO limited. The IO has both an analog and a digital component. The analog doesn't scale very well with technology, but the digital part does. So when you get to a certain size, being on the latest bleeding technology node actually isn't that big of a benefit, and in a lot of cases, a bigger technology node is perfectly fine. In some cases actually, it has some advantages. What we do with our devices is that we're always picking the technology node that's really optimized for the size of device that we're developing but also the customer needs that we're trying to get - and the time to market as well. For a lot of cases, it just doesn't make any sense for us to be on the bleeding edge of the technology.  Jim with a Threadripper IC: I'd be remiss if I didn't get a chance to ask you about your time at AMD. I remember you and me talking over dinner in Maranello about the vision of the company at the time. You had the upper hand because you knew what was coming down the pipe! But are there any fond memories, or any special moments? JA: It's been three years now, the memory starts to get fuzzy! IC: But now you're not under NDA! JA: That's true, actually! [laughs] First of all, when I think back about my time at AMD, my really fond memories are about the people. I keep in contact with a lot of people that I worked with at AMD who I consider really good friends. It was just a great group of people - you know, innovative, creative. AMD was always a really scrappy team. I loved that about AMD, and the people that are really determined. So that's always a fond memory. If I had to think about what events or what things that AMD did that I have great memories about, launching the first Ryzen chips was absolutely a great highlight at the time that I was at AMD. It was the client business unit that launched the first Zen based devices, and of course, you were there for that launch! We launched it first into desktop, followed by mobile later. The amount of work that the team put into that, how hard we worked, and then the excitement and anticipation of just being able to finally bring Ryzen to market was really exciting. Then the event that you mentioned at the very beginning, which was kind of the last event that I was at AMD, which was the Threadripper 2 launch. I loved that just because, I kind of always had a special place in my heart for Threadripper, because on that product line we just would sort of throw out the rulebook and just do whatever we can. Whatever the most extreme thing we could do, we would do on Threadripper, and I always have fun with that product line. But yeah, it was a great time. The last three years at Lattice look fantastic; it feels like we've made a lot of progress over the last three years. Weve rebuilt a product portfolio. But when I look forward, I'm much more excited about where we're headed over the next 3 to 5+years. I think the next few years are much more exciting for the company than even the past three years have been. So Im really excited about where we're headed. IC: Do you have any project inside Lattice that is also sort of throws the rulebook out? JA: Well, it's possible! It's possible. Stay tuned. Many thanks to Jim Anderson and his team for their time. Also thanks to Gavin Bonshor for transcription.